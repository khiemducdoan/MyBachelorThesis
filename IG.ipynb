{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from underthesea import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "class DynamicClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=768, \n",
    "                num_classes=4, \n",
    "                dropout_rate=0.25, \n",
    "                num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            output_dim = input_dim // 2\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if i < num_layers - 1:  # No dropout after last layer before classification\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, feature):\n",
    "        return self.model(feature)\n",
    "\n",
    "class ViTBERTClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                pretrained_model_name, \n",
    "                num_classes=4, \n",
    "                dropout_rate=0.25, \n",
    "                num_layers=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = DynamicClassifier(input_dim=self.bert.config.hidden_size,\n",
    "                                            num_classes=num_classes,\n",
    "                                            dropout_rate=dropout_rate,\n",
    "                                            num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state)\n",
    "        return logits\n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 16:08:14.518141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749712094.536882 1439086 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749712094.542608 1439086 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749712094.556290 1439086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749712094.556319 1439086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749712094.556321 1439086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749712094.556323 1439086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 16:08:14.561232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model= torch.load(\"/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/notebooks/best_model/best_model.pt\",weights_only= False, map_location='cpu')\n",
    "\n",
    "model.eval()\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBERT(Dataset):\n",
    "    def __init__(self, data_path, tokenizer=None):\n",
    "        self.dataset = pd.read_csv(data_path)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.iloc[idx, 0]  # Extract text from dataframe\n",
    "        label = self.dataset.iloc[idx, 1]-1.0  # Extract label\n",
    "\n",
    "        # Tokenization\n",
    "        tokenized_text = self.tokenizer(text, add_special_tokens=True, max_length=100, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Extract fields from tokenized text\n",
    "        input_ids = tokenized_text['input_ids'].squeeze().long()\n",
    "        attention_mask = tokenized_text['attention_mask'].squeeze().long()\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Changed dtype to torch.long\n",
    "\n",
    "        # Return tensors\n",
    "        return tokenized_text ,input_ids, attention_mask, label, text  # Return attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ViTBERT(data_path=\"/media/data3/home/khiemdd/ViTBERT/dataset/dataset_chi_ha_hieu/dataset_final_train_after.csv\", \n",
    "#                   tokenizer=\"demdecuong/vihealthbert-base-word\")\n",
    "# dataloader = DataLoader(dataset,batch_size= 1)\n",
    "# tokenized_text,input_ids, attention_mask, labels, text = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tokenized Text:\", tokenized_text)\n",
    "\n",
    "# print(\"\\nInput IDs:\", input_ids)\n",
    "# print(\"Shape of Input IDs:\", input_ids.shape)\n",
    "# print(\"Type of Input IDs:\", type(input_ids))\n",
    "\n",
    "# print(\"\\nAttention Mask:\", attention_mask)\n",
    "# print(\"Shape of Attention Mask:\", attention_mask.shape)\n",
    "# print(\"Type of Attention Mask:\", type(attention_mask))\n",
    "\n",
    "# print(\"\\nLabels:\", labels)\n",
    "# print(\"Shape of Labels:\", labels.shape)\n",
    "\n",
    "# print(\"\\nText:\", text)\n",
    "# print(\"Shape of Text:\", len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\"rất nhẹ\", \"nhẹ\", \"vừa\", \"nặng\"]\n",
    "def predict(X_batch):\n",
    "    preds = model(X_batch)\n",
    "    return F.softmax(preds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_ids.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0\n",
      "Cached: 0\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    print(f'Allocated: {torch.cuda.memory_allocated()}')\n",
    "    print(f'Cached: {torch.cuda.memory_reserved()}')\n",
    "\n",
    "# Clear cache and print memory usage\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/notebooks/best_model/best_model@.pt\",weights_only= False, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  62628,    168,  23886,  13891,  10380,  10132,  10133,  43024,\n",
      "          13055,  23000,  10116,  23912,    168,  28930,  10985,  10240, 101379,\n",
      "            168,  23000,  10376,  16895,  16317,  99142,  11542,  10143,  10410,\n",
      "          10132,  10133,    192,  48439,    168,  33901,  10308,  18323,  10504,\n",
      "          99142,  11542,  56975,    168,  23886,  13891,  99142,  10206,    168,\n",
      "          10911,  10147,  10985,  10240, 101379,    168,  23000,  10376,  16895,\n",
      "          16317,  99142,  11542,  29435,  37403, 110175,  23000,  10116,    123,\n",
      "            192,  21187,  10376,  10944,  10240,    168,  62628,  34101,  29435,\n",
      "          37403,  98696,    192,  21187,  10376, 105829,  10123,  11915,  10743,\n",
      "          10263,    177,  59139,    124,    125,  62628,  34101,  10149,  12815,\n",
      "            168,  10799,  25216,    168,  77586,  13891,  15176,  10240,  33901,\n",
      "          10308,    168,  26219,  51410,    168,  89499,    168,    299,  21187,\n",
      "          10376,    102,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"chấn_thương sọ não máu tụ dưới màng_cứng vùng thái_dương đỉnh phải dập não xuất_huyết bao trong phải vết_thương phần_mềm vùng thái_dương đỉnh phải gãy kín đầu dưới 2 xương cẳng_chân trái gãy kín chỏm xương đốt bàn ngón i ii 3 4 chân trái do tai_nạn giao_thông tăng huyết_áp đái_tháo_đường\"\n",
    "\n",
    "\n",
    "model_name = \"medicalai/ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_text = tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "print(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "input_ids = tokenizer_text[\"input_ids\"]\n",
    "attention_mask = tokenizer_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6579, -0.9015,  0.0828,  2.5161]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = input_ids.to(torch.device(\"cuda:0\"))\n",
    "attention_mask = attention_mask.to(torch.device(\"cuda\"))\n",
    "model(input_ids, attention_mask).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the input text and get input IDs and attention mask\n",
    "    encoding = tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    return encoding['input_ids'], encoding['attention_mask']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_func(input_ids, attention_mask):\n",
    "#     # Get the output of the model\n",
    "#     input_ids = input_ids.to(torch.device(\"cuda:0\"))\n",
    "#     attention_mask = attention_mask.to(torch.device(\"cuda:0\"))\n",
    "#     input_ids = torch.tensor(input_ids,dtype= \"float\",requires_grad=True)\n",
    "#     attention_mask = torch.tensor(attention_mask,dtype=\"float\",requires_grad=True)\n",
    "#     outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#     # Get the classification logits\n",
    "#     logits = outputs\n",
    "#     return logits\n",
    "def forward_func(embeddings, attention_mask):\n",
    "    attention_mask = attention_mask.to(embeddings.device)\n",
    "    outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "    return outputs.logits\n",
    "\n",
    "\n",
    "def explain(model, text, target_class):\n",
    "    input_ids, attention_mask = preprocess(text)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    # Lấy embedding đầu vào\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    embeddings = embedding_layer(input_ids)\n",
    "    embeddings.requires_grad_()\n",
    "\n",
    "    # Tính attribution\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=embeddings,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        target=target_class,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "    return attributions, tokens, delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of labels:\", model.config.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [CLS]: 0.0000\n",
      "      trauma: 0.0002\n",
      "       ##tic: -0.0000\n",
      "           _: 0.0001\n",
      "       brain: -0.0003\n",
      "      injury: -0.0005\n",
      "           _: 0.0002\n",
      "       trade: -0.0008\n",
      "      ##mark: -0.0000\n",
      "           _: 0.0001\n",
      "compensation: -0.0007\n",
      "           _: 0.0002\n",
      "        soft: 0.0004\n",
      "        left: 0.0005\n",
      "      summit: -0.0003\n",
      "         due: -0.0001\n",
      "          to: -0.0001\n",
      "        fall: -0.0001\n",
      "       [SEP]: -0.0006\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n",
      "       [PAD]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = \"traumatic_brain injury_trademark_compensation_soft left summit due to fall\"\n",
    "target_class = 1\n",
    "\n",
    "# Run explanation\n",
    "attributions, tokens, delta = explain(model, text, target_class)\n",
    "\n",
    "# Display results (mean of attribution vector per token)\n",
    "for token, attribution in zip(tokens, attributions[0].tolist()):\n",
    "    score = np.mean(attribution)\n",
    "    print(f\"{token:>12}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "\n",
    "def visualize_token_attributions(tokens, attributions, normalize=True):\n",
    "    html_output = \"\"\n",
    "\n",
    "    # Compute 1D scores per token (mean across embedding dim)\n",
    "    scores = [np.mean(attr) for attr in attributions]\n",
    "\n",
    "    # Optional: normalize scores to [0, 1] for coloring\n",
    "    if normalize:\n",
    "        max_score = max(abs(np.min(scores)), abs(np.max(scores))) + 1e-10\n",
    "        scores = [score / max_score for score in scores]\n",
    "\n",
    "    for token, score in zip(tokens, scores):\n",
    "        red = f\"rgba(255, 0, 0, {score})\" if score > 0 else f\"rgba(0, 0, 255, {abs(score)})\"\n",
    "        html_output += f\"<span style='background-color: {red}; padding:2px; margin:1px; border-radius:3px'>{token}</span> \"\n",
    "\n",
    "    display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>-0.49</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sub                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##peri                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##tone                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##al                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ato                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ma                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chronic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> secondary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> due                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 4th                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> week                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surgery                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> conduct                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ive                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> irrigation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> closed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ato                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##mas                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> both                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sides                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> first                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> uri                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##nary                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ob                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##stru                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ction                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disco                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##nne                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ction                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sugar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>-0.49</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sub                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##peri                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##tone                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##al                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ato                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ma                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chronic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> secondary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> due                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 4th                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> week                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surgery                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> conduct                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ive                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> irrigation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> closed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ato                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##mas                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> both                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sides                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> first                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ha                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> uri                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##nary                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ob                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##stru                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ction                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disco                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##nne                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ction                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> _                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sugar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import visualization\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records, tokenizer):\n",
    "    # Reduce attribution vector (embedding dim) -> 1D per token\n",
    "    attributions_sum = attributions.sum(dim=2).squeeze(0)  # shape: (seq_len,)\n",
    "    attributions_norm = attributions_sum / torch.norm(attributions_sum)\n",
    "    attributions_np = attributions_norm.cpu().detach().numpy()\n",
    "\n",
    "    # Tokenize again to match attribution positions\n",
    "    input_ids = tokenizer(text, return_tensors='pt', truncation=True)['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Ensure same length\n",
    "    if len(attributions_np) > len(tokens):\n",
    "        attributions_np = attributions_np[:len(tokens)]\n",
    "    elif len(tokens) > len(attributions_np):\n",
    "        tokens = tokens[:len(attributions_np)]\n",
    "\n",
    "    # Add visualization record\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "        attributions_np,\n",
    "        pred,\n",
    "        str(pred_ind),\n",
    "        str(label),\n",
    "        str(pred_ind),  # e.g., predicted class description\n",
    "        attributions_np.sum(),  # total attribution score\n",
    "        tokens,\n",
    "        delta\n",
    "    ))\n",
    "from captum.attr import visualization\n",
    "\n",
    "# Your input\n",
    "text = \"Subperitoneal hematoma chronic secondary hemorrhage due 4th week surgery_conductive irrigation_closed hematomas on both sides hemorrhage_first day hemorrhage_urinary obstruction_disconnection_sugar\"\n",
    "target_class = 1\n",
    "pred = 0.95  # mock prediction confidence\n",
    "pred_ind = 1  # predicted class index\n",
    "label = 1     # ground truth label (or something else)\n",
    "delta = 0.01  # convergence delta returned by IG\n",
    "\n",
    "# Compute attribution\n",
    "attributions, tokens, delta = explain(model, text, target_class)\n",
    "\n",
    "# Visualization list\n",
    "vis_data_records = []\n",
    "\n",
    "# Add to Captum visualizer\n",
    "add_attributions_to_visualizer(\n",
    "    attributions,\n",
    "    text,\n",
    "    pred,\n",
    "    pred_ind,\n",
    "    label,\n",
    "    delta,\n",
    "    vis_data_records,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Display (inside Jupyter)\n",
    "visualization.visualize_text(vis_data_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-family:monospace; line-height:1.8em'><span style='background-color:rgba(0,0,255,0.021441575443050417);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>[CLS]</span> <span style='background-color:rgba(0,0,255,0.0414385987707563);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>subperitoneal</span> <span style='background-color:rgba(0,0,255,0.04513352445616076);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>hematoma</span> <span style='background-color:rgba(255,0,0,0.13122023212137005);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>chronic</span> <span style='background-color:rgba(255,0,0,0.0898603649776698);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>secondary</span> <span style='background-color:rgba(0,0,255,0.020982242540994813);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>hemorrhage</span> <span style='background-color:rgba(255,0,0,0.1135696583517701);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>due</span> <span style='background-color:rgba(255,0,0,0.008251506109826254);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>4th</span> <span style='background-color:rgba(255,0,0,0.04870265044751644);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>week</span> <span style='background-color:rgba(0,0,255,0.05375901959576121);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>surgery</span> <span style='background-color:rgba(255,0,0,0.0010919167073392907);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(255,0,0,0.03988290754577799);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>conductive</span> <span style='background-color:rgba(0,0,255,0.01460490248946091);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>irrigation</span> <span style='background-color:rgba(0,0,255,0.014753365990591055);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(0,0,255,0.006000391652405647);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>closed</span> <span style='background-color:rgba(255,0,0,0.050578388833114864);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>hematomas</span> <span style='background-color:rgba(255,0,0,0.07267144667058155);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>on</span> <span style='background-color:rgba(255,0,0,0.010061526605269263);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>both</span> <span style='background-color:rgba(255,0,0,0.1556516685836026);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>sides</span> <span style='background-color:rgba(255,0,0,0.029154376843336816);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>hemorrhage</span> <span style='background-color:rgba(255,0,0,0.022012711370763608);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(0,0,255,0.06781088186162915);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>first</span> <span style='background-color:rgba(0,0,255,0.06677932676145733);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>day</span> <span style='background-color:rgba(255,0,0,0.035519433894835986);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>hemorrhage</span> <span style='background-color:rgba(255,0,0,0.01562364357982264);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(255,0,0,0.0014859463223960505);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>urinary</span> <span style='background-color:rgba(0,0,255,0.012685915451492644);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>obstruction</span> <span style='background-color:rgba(255,0,0,0.004223911822417847);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(255,0,0,0.0122281746891302);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>disconnection</span> <span style='background-color:rgba(255,0,0,0.0019847348759747083);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>_</span> <span style='background-color:rgba(0,0,255,0.11825267960390158);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>sugar</span> <span style='background-color:rgba(0,0,255,0.0);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>[PAD]</span> <span style='background-color:rgba(0,0,255,0.9999908115752123);padding:3px; margin:2px; border-radius:4px; display:inline-block;'>[SEP]</span> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import numpy as np\n",
    "\n",
    "def average_attribution_for_words(tokens, attributions):\n",
    "    words = []\n",
    "    word_attributions = []\n",
    "    current_word = \"\"\n",
    "    current_attr = []\n",
    "\n",
    "    for token, attr in zip(tokens, attributions):\n",
    "        if token == \"[PAD]\":\n",
    "            if \"[PAD]\" not in words:  # Chỉ thêm 1 lần\n",
    "                words.append(\"[PAD]\")\n",
    "                word_attributions.append(0.0)\n",
    "            continue  # Bỏ qua các PAD tiếp theo\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]\n",
    "            current_attr.append(np.mean(attr))\n",
    "        else:\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                word_attributions.append(np.mean(current_attr))\n",
    "            current_word = token\n",
    "            current_attr = [np.mean(attr)]\n",
    "\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        word_attributions.append(np.mean(current_attr))\n",
    "\n",
    "    return words, word_attributions\n",
    "\n",
    "def visualize_text_attributions(tokens, attributions):\n",
    "    words, word_attrs = average_attribution_for_words(tokens, attributions)\n",
    "\n",
    "    max_score = max(abs(np.min(word_attrs)), abs(np.max(word_attrs))) + 1e-8\n",
    "    html = \"<div style='font-family:monospace; line-height:1.8em'>\"\n",
    "\n",
    "    for word, score in zip(words, word_attrs):\n",
    "        normalized = score / max_score\n",
    "        r, g, b, alpha = (255, 0, 0, abs(normalized)) if normalized > 0 else (0, 0, 255, abs(normalized))\n",
    "        html += (\n",
    "            f\"<span style='background-color:rgba({r},{g},{b},{alpha});\"\n",
    "            f\"padding:3px; margin:2px; border-radius:4px; display:inline-block;'>\"\n",
    "            f\"{word}</span> \"\n",
    "        )\n",
    "\n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "visualize_text_attributions(tokens, attributions[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khiem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
