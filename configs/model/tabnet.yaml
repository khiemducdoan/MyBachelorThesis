name: tabnet
model:
  _target_: src.models.tabnet.TabNetClassifier
  params:
    n_d: 8  # Width of decision prediction layer
    n_a: 8  # Width of attention
    n_steps: 3  # Number of steps
    gamma: 1.3  # Feature selection regularization
    n_independent: 2
    n_shared: 2
    virtual_batch_size: 128
    momentum: 0.02
    mask_type: sparsemax

optimizer:
  _target_: torch.optim.Adam
  lr: ${training.learning_rate}
  weight_decay: 0.0001

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 5 