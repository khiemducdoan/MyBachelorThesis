wandb: Currently logged in as: vinakhiem120 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py:304: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="./configs", config_name="main")
/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. learning_rate uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
Create sweep with ID: kxuhcx7m
Sweep URL: https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
[2025-06-02 23:54:50,393][wandb.agents.pyagent][INFO] - Starting sweep agent: entity=None, project=None, count=100
wandb: Agent Starting Run: 5izjzx6k with config:
wandb: 	Fdropout_rate: 0.1544347996047699
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.3897846298908923
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.1278074945777689
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250602_235451-5izjzx6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5izjzx6k
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
Training started...
Epoch 0 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:54:58,554][__main__][INFO] - Train Epoch: 0 [0/51 (0%)]	Loss: 1.342984
Epoch 0 [Train]:   2%|‚ñè         | 1/51 [00:00<00:36,  1.37it/s]Epoch 0 [Train]:   4%|‚ñç         | 2/51 [00:01<00:22,  2.14it/s]Epoch 0 [Train]:   6%|‚ñå         | 3/51 [00:01<00:18,  2.63it/s]Epoch 0 [Train]:   8%|‚ñä         | 4/51 [00:01<00:15,  2.94it/s]Epoch 0 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:14,  3.15it/s]Epoch 0 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:02<00:13,  3.30it/s]Epoch 0 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:13,  3.38it/s]Epoch 0 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.43it/s]Epoch 0 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:11,  3.50it/s]Epoch 0 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:03<00:11,  3.54it/s]Epoch 0 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.56it/s]Epoch 0 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:10,  3.57it/s]Epoch 0 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:04<00:10,  3.58it/s]Epoch 0 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.58it/s]Epoch 0 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.58it/s]Epoch 0 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:09,  3.58it/s]Epoch 0 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:05<00:09,  3.58it/s]Epoch 0 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.59it/s]Epoch 0 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:08,  3.59it/s]Epoch 0 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:06<00:08,  3.58it/s]Epoch 0 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.58it/s]Epoch 0 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.59it/s]Epoch 0 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:07,  3.59it/s]Epoch 0 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:07<00:07,  3.59it/s]Epoch 0 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.59it/s]Epoch 0 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:06,  3.58it/s]Epoch 0 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.58it/s]Epoch 0 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.57it/s]Epoch 0 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.57it/s]Epoch 0 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:05,  3.57it/s]Epoch 0 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:09<00:05,  3.58it/s]Epoch 0 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.58it/s]Epoch 0 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.59it/s]Epoch 0 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.58it/s]Epoch 0 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.58it/s]Epoch 0 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.59it/s]Epoch 0 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:03,  3.58it/s]Epoch 0 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.58it/s]Epoch 0 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.57it/s]Epoch 0 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.57it/s]Epoch 0 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.55it/s]Epoch 0 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.56it/s]Epoch 0 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.55it/s]Epoch 0 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:01,  3.55it/s]Epoch 0 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.52it/s]Epoch 0 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.49it/s]Epoch 0 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.51it/s]Epoch 0 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.51it/s]Epoch 0 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 0 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.50it/s]Epoch 0 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.20it/s]                                                                Epoch 0 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:01, 10.33it/s]Epoch 0 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.92it/s]Epoch 0 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 11.14it/s]Epoch 0 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 11.25it/s]Epoch 0 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 11.29it/s]Epoch 0 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 11.33it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:55:13,555][__main__][INFO] - Epoch 0: Val Loss: 1.3042, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7f3808042270>
<numpy.flatiter object at 0x7f3808042270>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7f3808045580>
<numpy.flatiter object at 0x7f3808045580>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f3808045580>
<numpy.flatiter object at 0x7f3808045580>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7f3808045580>
<numpy.flatiter object at 0x7f3808045580>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f3808045580>
<numpy.flatiter object at 0x7f3808045580>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-02 23:55:13,654][__main__][INFO] - Saved best model at epoch 0 with accuracy: 0.5842
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-02 23:55:13,784][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
[2025-06-02 23:55:14,296][__main__][INFO] - Saved model at epoch 0
Epoch 1 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:55:14,590][__main__][INFO] - Train Epoch: 1 [0/51 (0%)]	Loss: 1.570009
Epoch 1 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.43it/s]Epoch 1 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.42it/s]Epoch 1 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.48it/s]Epoch 1 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.49it/s]Epoch 1 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.51it/s]Epoch 1 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:12,  3.53it/s]Epoch 1 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:01<00:12,  3.54it/s]Epoch 1 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.54it/s]Epoch 1 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:11,  3.54it/s]Epoch 1 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.55it/s]Epoch 1 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.55it/s]Epoch 1 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:10,  3.55it/s]Epoch 1 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.55it/s]Epoch 1 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:03<00:10,  3.54it/s]Epoch 1 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.55it/s]Epoch 1 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:09,  3.55it/s]Epoch 1 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.55it/s]Epoch 1 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.54it/s]Epoch 1 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.54it/s]Epoch 1 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.55it/s]Epoch 1 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:05<00:08,  3.55it/s]Epoch 1 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.55it/s]Epoch 1 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:07,  3.55it/s]Epoch 1 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.55it/s]Epoch 1 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.54it/s]Epoch 1 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.54it/s]Epoch 1 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.54it/s]Epoch 1 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:07<00:06,  3.54it/s]Epoch 1 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.53it/s]Epoch 1 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:05,  3.54it/s]Epoch 1 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.55it/s]Epoch 1 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.54it/s]Epoch 1 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.54it/s]Epoch 1 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.54it/s]Epoch 1 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:09<00:04,  3.54it/s]Epoch 1 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.54it/s]Epoch 1 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:03,  3.53it/s]Epoch 1 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.54it/s]Epoch 1 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.54it/s]Epoch 1 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.54it/s]Epoch 1 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.53it/s]Epoch 1 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:11<00:02,  3.53it/s]Epoch 1 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.53it/s]Epoch 1 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:01,  3.54it/s]Epoch 1 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:12<00:01,  3.54it/s]Epoch 1 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.54it/s]Epoch 1 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.54it/s]Epoch 1 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.53it/s]Epoch 1 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:13<00:00,  3.53it/s]Epoch 1 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.53it/s]Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.22it/s]                                                                Epoch 1 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 1 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.29it/s]Epoch 1 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.22it/s]Epoch 1 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 11.20it/s]Epoch 1 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 11.20it/s]Epoch 1 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 11.18it/s]Epoch 1 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 11.17it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:55:29,712][__main__][INFO] - Epoch 1: Val Loss: 1.3118, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
Epoch 2 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:55:29,998][__main__][INFO] - Train Epoch: 2 [0/51 (0%)]	Loss: 1.285672
Epoch 2 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.53it/s]Epoch 2 [Train]:   4%|‚ñç         | 2/51 [00:00<00:13,  3.54it/s]Epoch 2 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.53it/s]Epoch 2 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.52it/s]Epoch 2 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.52it/s]Epoch 2 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:12,  3.52it/s]Epoch 2 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:01<00:12,  3.52it/s]Epoch 2 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.51it/s]Epoch 2 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:11,  3.51it/s]Epoch 2 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.51it/s]Epoch 2 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.51it/s]Epoch 2 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.50it/s]Epoch 2 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.51it/s]Epoch 2 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:03<00:10,  3.51it/s]Epoch 2 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.51it/s]Epoch 2 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:09,  3.52it/s]Epoch 2 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.52it/s]Epoch 2 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.53it/s]Epoch 2 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.53it/s]Epoch 2 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.52it/s]Epoch 2 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:05<00:08,  3.52it/s]Epoch 2 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.52it/s]Epoch 2 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:07,  3.55it/s]Epoch 2 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.53it/s]Epoch 2 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.53it/s]Epoch 2 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.53it/s]Epoch 2 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.53it/s]Epoch 2 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:07<00:06,  3.53it/s]Epoch 2 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.52it/s]Epoch 2 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:05,  3.52it/s]Epoch 2 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.51it/s]Epoch 2 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.51it/s]Epoch 2 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.51it/s]Epoch 2 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.52it/s]Epoch 2 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:09<00:04,  3.52it/s]Epoch 2 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.52it/s]Epoch 2 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:03,  3.51it/s]Epoch 2 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.50it/s]Epoch 2 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.49it/s]Epoch 2 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.49it/s]Epoch 2 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.49it/s]Epoch 2 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:11<00:02,  3.49it/s]Epoch 2 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.49it/s]Epoch 2 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.49it/s]Epoch 2 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:12<00:01,  3.49it/s]Epoch 2 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.50it/s]Epoch 2 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.50it/s]Epoch 2 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.47it/s]Epoch 2 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:13<00:00,  3.48it/s]Epoch 2 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.48it/s]Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.16it/s]                                                                Epoch 2 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 2 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.23it/s]Epoch 2 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.18it/s]Epoch 2 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 11.16it/s]Epoch 2 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 11.15it/s]Epoch 2 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 11.14it/s]Epoch 2 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 11.13it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:55:45,237][__main__][INFO] - Epoch 2: Val Loss: 1.3510, Accuracy: 0.1980, Metrics: {'accuracy': 0.19801980198019803, 'f1_macro': 0.08264462809917356, 'f1_weighted': 0.06546109156370182, 'precision_macro': 0.04950495049504951, 'recall_macro': 0.25}
Epoch 3 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:55:45,527][__main__][INFO] - Train Epoch: 3 [0/51 (0%)]	Loss: 1.292142
Epoch 3 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.46it/s]Epoch 3 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.43it/s]Epoch 3 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.47it/s]Epoch 3 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.49it/s]Epoch 3 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.50it/s]Epoch 3 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:12,  3.51it/s]Epoch 3 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.51it/s]Epoch 3 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.50it/s]Epoch 3 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.49it/s]Epoch 3 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.49it/s]Epoch 3 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.49it/s]Epoch 3 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.49it/s]Epoch 3 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.49it/s]Epoch 3 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.49it/s]Epoch 3 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.48it/s]Epoch 3 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.48it/s]Epoch 3 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.48it/s]Epoch 3 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.48it/s]Epoch 3 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.48it/s]Epoch 3 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.48it/s]Epoch 3 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.48it/s]Epoch 3 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.49it/s]Epoch 3 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.48it/s]Epoch 3 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.49it/s]Epoch 3 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.49it/s]Epoch 3 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.49it/s]Epoch 3 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.48it/s]Epoch 3 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.47it/s]Epoch 3 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.47it/s]Epoch 3 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.47it/s]Epoch 3 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.48it/s]Epoch 3 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.47it/s]Epoch 3 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.48it/s]Epoch 3 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.48it/s]Epoch 3 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.48it/s]Epoch 3 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.48it/s]Epoch 3 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.47it/s]Epoch 3 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.47it/s]Epoch 3 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.47it/s]Epoch 3 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.48it/s]Epoch 3 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.48it/s]Epoch 3 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.47it/s]Epoch 3 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.47it/s]Epoch 3 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.47it/s]Epoch 3 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:12<00:01,  3.46it/s]Epoch 3 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 3 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.47it/s]Epoch 3 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.47it/s]Epoch 3 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.47it/s]Epoch 3 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.47it/s]Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.15it/s]                                                                Epoch 3 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 3 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.21it/s]Epoch 3 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.12it/s]Epoch 3 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 11.10it/s]Epoch 3 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 11.09it/s]Epoch 3 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 11.08it/s]Epoch 3 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 11.08it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:56:00,890][__main__][INFO] - Epoch 3: Val Loss: 1.0789, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
Epoch 4 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:56:01,193][__main__][INFO] - Train Epoch: 4 [0/51 (0%)]	Loss: 0.792636
Epoch 4 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.36it/s]Epoch 4 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.41it/s]Epoch 4 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.44it/s]Epoch 4 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 4 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 4 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.46it/s]Epoch 4 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.46it/s]Epoch 4 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.46it/s]Epoch 4 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.46it/s]Epoch 4 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 4 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 4 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 4 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 4 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 4 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 4 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 4 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 4 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.46it/s]Epoch 4 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.47it/s]Epoch 4 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.47it/s]Epoch 4 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.47it/s]Epoch 4 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.46it/s]Epoch 4 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 4 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 4 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.45it/s]Epoch 4 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.44it/s]Epoch 4 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.44it/s]Epoch 4 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.44it/s]Epoch 4 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.44it/s]Epoch 4 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.44it/s]Epoch 4 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.44it/s]Epoch 4 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 4 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.45it/s]Epoch 4 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.45it/s]Epoch 4 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 4 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 4 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 4 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.45it/s]Epoch 4 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 4 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 4 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 4 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.45it/s]Epoch 4 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 4 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.44it/s]Epoch 4 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 4 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 4 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.45it/s]Epoch 4 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.44it/s]Epoch 4 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.45it/s]Epoch 4 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.44it/s]Epoch 4 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.12it/s]                                                                Epoch 4 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 4 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.07it/s]Epoch 4 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.03it/s]Epoch 4 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.99it/s]Epoch 4 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 11.00it/s]Epoch 4 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.99it/s]Epoch 4 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 11.00it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:56:16,679][__main__][INFO] - Epoch 4: Val Loss: 1.0563, Accuracy: 0.6436, Metrics: {'accuracy': 0.6435643564356436, 'f1_macro': 0.4276315789473684, 'f1_weighted': 0.5961066031415171, 'precision_macro': 0.47794840294840296, 'recall_macro': 0.4237480739599384}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7f3808985130>
<numpy.flatiter object at 0x7f3808985130>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7f38089d0cd0>
<numpy.flatiter object at 0x7f38089d0cd0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f38089d0cd0>
<numpy.flatiter object at 0x7f38089d0cd0>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7f38089d12d0>
<numpy.flatiter object at 0x7f38089d12d0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f38089d12d0>
<numpy.flatiter object at 0x7f38089d12d0>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-02 23:56:16,745][__main__][INFO] - Saved best model at epoch 4 with accuracy: 0.6436
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-02 23:56:16,845][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 5 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:56:17,169][__main__][INFO] - Train Epoch: 5 [0/51 (0%)]	Loss: 0.816173
Epoch 5 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.42it/s]Epoch 5 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.44it/s]Epoch 5 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.45it/s]Epoch 5 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 5 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 5 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:12,  3.46it/s]Epoch 5 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.46it/s]Epoch 5 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.46it/s]Epoch 5 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.44it/s]Epoch 5 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.45it/s]Epoch 5 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.44it/s]Epoch 5 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.44it/s]Epoch 5 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:11,  3.45it/s]Epoch 5 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 5 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 5 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.45it/s]Epoch 5 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.45it/s]Epoch 5 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 5 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 5 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 5 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 5 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.43it/s]Epoch 5 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.44it/s]Epoch 5 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.44it/s]Epoch 5 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.44it/s]Epoch 5 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.45it/s]Epoch 5 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.44it/s]Epoch 5 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.44it/s]Epoch 5 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.44it/s]Epoch 5 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.44it/s]Epoch 5 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.44it/s]Epoch 5 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.44it/s]Epoch 5 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.44it/s]Epoch 5 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.45it/s]Epoch 5 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 5 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 5 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 5 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.45it/s]Epoch 5 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 5 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.44it/s]Epoch 5 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 5 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.45it/s]Epoch 5 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 5 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 5 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 5 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.44it/s]Epoch 5 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.44it/s]Epoch 5 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.44it/s]Epoch 5 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.44it/s]Epoch 5 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.45it/s]Epoch 5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.12it/s]                                                                Epoch 5 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 5 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.03it/s]Epoch 5 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.94it/s]Epoch 5 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.94it/s]Epoch 5 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.94it/s]Epoch 5 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.93it/s]Epoch 5 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.93it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:56:32,688][__main__][INFO] - Epoch 5: Val Loss: 0.9697, Accuracy: 0.6733, Metrics: {'accuracy': 0.6732673267326733, 'f1_macro': 0.49994687407029625, 'f1_weighted': 0.6115886483459004, 'precision_macro': 0.5086306098964327, 'recall_macro': 0.5143489984591679}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7f38089a1170>
<numpy.flatiter object at 0x7f38089a1170>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7f38089a1e00>
<numpy.flatiter object at 0x7f38089a1e00>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f38089a1e00>
<numpy.flatiter object at 0x7f38089a1e00>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7f38089a1e00>
<numpy.flatiter object at 0x7f38089a1e00>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f38089a1e00>
<numpy.flatiter object at 0x7f38089a1e00>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-02 23:56:32,752][__main__][INFO] - Saved best model at epoch 5 with accuracy: 0.6733
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-02 23:56:32,878][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 6 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:56:33,201][__main__][INFO] - Train Epoch: 6 [0/51 (0%)]	Loss: 0.761072
Epoch 6 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.43it/s]Epoch 6 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.44it/s]Epoch 6 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.44it/s]Epoch 6 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 6 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.44it/s]Epoch 6 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.44it/s]Epoch 6 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 6 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.44it/s]Epoch 6 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 6 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.44it/s]Epoch 6 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.44it/s]Epoch 6 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.44it/s]Epoch 6 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:11,  3.45it/s]Epoch 6 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.44it/s]Epoch 6 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.44it/s]Epoch 6 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.44it/s]Epoch 6 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.45it/s]Epoch 6 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 6 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 6 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 6 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 6 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.45it/s]Epoch 6 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 6 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 6 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.46it/s]Epoch 6 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.46it/s]Epoch 6 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.46it/s]Epoch 6 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.46it/s]Epoch 6 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 6 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 6 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.45it/s]Epoch 6 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 6 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.46it/s]Epoch 6 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 6 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 6 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 6 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.46it/s]Epoch 6 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.45it/s]Epoch 6 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 6 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.46it/s]Epoch 6 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 6 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.45it/s]Epoch 6 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 6 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 6 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 6 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.45it/s]Epoch 6 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 6 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 6 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 6 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 6 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                Epoch 6 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 6 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.05it/s]Epoch 6 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.03it/s]Epoch 6 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 11.00it/s]Epoch 6 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.98it/s]Epoch 6 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.98it/s]Epoch 6 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.98it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:56:48,690][__main__][INFO] - Epoch 6: Val Loss: 1.3267, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
Epoch 7 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:56:48,990][__main__][INFO] - Train Epoch: 7 [0/51 (0%)]	Loss: 1.464551
Epoch 7 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.35it/s]Epoch 7 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.40it/s]Epoch 7 [Train]:   6%|‚ñå         | 3/51 [00:00<00:14,  3.42it/s]Epoch 7 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.44it/s]Epoch 7 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.44it/s]Epoch 7 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 7 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 7 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 7 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 7 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 7 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 7 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.45it/s]Epoch 7 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 7 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 7 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.46it/s]Epoch 7 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.45it/s]Epoch 7 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 7 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.46it/s]Epoch 7 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 7 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.46it/s]Epoch 7 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 7 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.44it/s]Epoch 7 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.44it/s]Epoch 7 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 7 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.44it/s]Epoch 7 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.45it/s]Epoch 7 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.45it/s]Epoch 7 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.45it/s]Epoch 7 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.45it/s]Epoch 7 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 7 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.46it/s]Epoch 7 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 7 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.46it/s]Epoch 7 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 7 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.46it/s]Epoch 7 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 7 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.46it/s]Epoch 7 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.46it/s]Epoch 7 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 7 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 7 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 7 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 7 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 7 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 7 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 7 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.45it/s]Epoch 7 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.45it/s]Epoch 7 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 7 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.45it/s]Epoch 7 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.45it/s]Epoch 7 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.13it/s]                                                                Epoch 7 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 7 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:01, 11.00it/s]Epoch 7 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.96it/s]Epoch 7 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.98it/s]Epoch 7 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.98it/s]Epoch 7 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.97it/s]Epoch 7 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.97it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:57:04,473][__main__][INFO] - Epoch 7: Val Loss: 1.3839, Accuracy: 0.6436, Metrics: {'accuracy': 0.6435643564356436, 'f1_macro': 0.3664806110458284, 'f1_weighted': 0.5257239589998952, 'precision_macro': 0.32677902621722843, 'recall_macro': 0.42334360554699535}
Epoch 8 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:57:04,773][__main__][INFO] - Train Epoch: 8 [0/51 (0%)]	Loss: 1.331001
Epoch 8 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.35it/s]Epoch 8 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.41it/s]Epoch 8 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.43it/s]Epoch 8 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 8 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 8 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 8 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 8 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 8 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.46it/s]Epoch 8 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.45it/s]Epoch 8 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.45it/s]Epoch 8 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.45it/s]Epoch 8 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:11,  3.45it/s]Epoch 8 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 8 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 8 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.45it/s]Epoch 8 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.45it/s]Epoch 8 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 8 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 8 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 8 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 8 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.46it/s]Epoch 8 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.46it/s]Epoch 8 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 8 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.46it/s]Epoch 8 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.46it/s]Epoch 8 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.46it/s]Epoch 8 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.46it/s]Epoch 8 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 8 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 8 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.46it/s]Epoch 8 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.44it/s]Epoch 8 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.44it/s]Epoch 8 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.45it/s]Epoch 8 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 8 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 8 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 8 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.46it/s]Epoch 8 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 8 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 8 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 8 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 8 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.46it/s]Epoch 8 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.46it/s]Epoch 8 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.46it/s]Epoch 8 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 8 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.45it/s]Epoch 8 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 8 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 8 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 8 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                Epoch 8 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 8 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.08it/s]Epoch 8 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.01it/s]Epoch 8 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.99it/s]Epoch 8 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.98it/s]Epoch 8 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.97it/s]Epoch 8 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.96it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:57:20,250][__main__][INFO] - Epoch 8: Val Loss: 0.9930, Accuracy: 0.6535, Metrics: {'accuracy': 0.6534653465346535, 'f1_macro': 0.3206586318112375, 'f1_weighted': 0.5696623982547636, 'precision_macro': 0.28818283166109254, 'recall_macro': 0.3622881355932204}
Epoch 9 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:57:20,548][__main__][INFO] - Train Epoch: 9 [0/51 (0%)]	Loss: 0.319138
Epoch 9 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.38it/s]Epoch 9 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.41it/s]Epoch 9 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.43it/s]Epoch 9 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.44it/s]Epoch 9 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 9 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 9 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 9 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 9 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 9 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.45it/s]Epoch 9 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 9 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 9 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.45it/s]Epoch 9 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.46it/s]Epoch 9 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 9 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.45it/s]Epoch 9 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 9 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.47it/s]Epoch 9 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.47it/s]Epoch 9 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.47it/s]Epoch 9 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.46it/s]Epoch 9 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.45it/s]Epoch 9 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 9 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 9 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.45it/s]Epoch 9 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.45it/s]Epoch 9 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.45it/s]Epoch 9 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.45it/s]Epoch 9 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 9 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 9 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.45it/s]Epoch 9 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 9 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.46it/s]Epoch 9 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 9 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.46it/s]Epoch 9 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 9 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.46it/s]Epoch 9 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.46it/s]Epoch 9 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 9 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.46it/s]Epoch 9 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 9 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 9 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.46it/s]Epoch 9 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.46it/s]Epoch 9 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.46it/s]Epoch 9 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 9 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 9 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.45it/s]Epoch 9 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 9 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 9 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                Epoch 9 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 9 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.09it/s]Epoch 9 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.00it/s]Epoch 9 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.99it/s]Epoch 9 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.99it/s]Epoch 9 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.98it/s]Epoch 9 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.99it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:57:36,019][__main__][INFO] - Epoch 9: Val Loss: 1.0031, Accuracy: 0.5644, Metrics: {'accuracy': 0.5643564356435643, 'f1_macro': 0.30859375, 'f1_weighted': 0.48344678217821785, 'precision_macro': 0.25649675162418795, 'recall_macro': 0.4264252696456086}
Epoch 10 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:57:36,312][__main__][INFO] - Train Epoch: 10 [0/51 (0%)]	Loss: 1.141868
Epoch 10 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.44it/s]Epoch 10 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.45it/s]Epoch 10 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.45it/s]Epoch 10 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 10 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 10 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 10 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.46it/s]Epoch 10 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 10 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 10 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.45it/s]Epoch 10 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 10 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 10 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:11,  3.45it/s]Epoch 10 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 10 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.46it/s]Epoch 10 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 10 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.45it/s]Epoch 10 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 10 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 10 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 10 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 10 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.45it/s]Epoch 10 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 10 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.46it/s]Epoch 10 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.46it/s]Epoch 10 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.46it/s]Epoch 10 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.46it/s]Epoch 10 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.46it/s]Epoch 10 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 10 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 10 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.45it/s]Epoch 10 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.46it/s]Epoch 10 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.46it/s]Epoch 10 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.45it/s]Epoch 10 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 10 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 10 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 10 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.45it/s]Epoch 10 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.46it/s]Epoch 10 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.46it/s]Epoch 10 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 10 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 10 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.46it/s]Epoch 10 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.46it/s]Epoch 10 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.46it/s]Epoch 10 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 10 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 10 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 10 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.45it/s]Epoch 10 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.45it/s]Epoch 10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                 Epoch 10 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 10 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.01it/s]Epoch 10 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.96it/s]Epoch 10 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.97it/s]Epoch 10 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.98it/s]Epoch 10 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.97it/s]Epoch 10 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.97it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:57:51,788][__main__][INFO] - Epoch 10: Val Loss: 1.2110, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
Epoch 11 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:57:52,080][__main__][INFO] - Train Epoch: 11 [0/51 (0%)]	Loss: 0.743037
Epoch 11 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.44it/s]Epoch 11 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.45it/s]Epoch 11 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.45it/s]Epoch 11 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 11 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.45it/s]Epoch 11 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 11 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.46it/s]Epoch 11 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.46it/s]Epoch 11 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.46it/s]Epoch 11 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 11 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 11 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 11 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 11 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.46it/s]Epoch 11 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.46it/s]Epoch 11 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 11 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 11 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 11 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 11 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 11 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 11 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.46it/s]Epoch 11 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.46it/s]Epoch 11 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.42it/s]Epoch 11 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.42it/s]Epoch 11 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.43it/s]Epoch 11 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.45it/s]Epoch 11 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.45it/s]Epoch 11 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.45it/s]Epoch 11 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.45it/s]Epoch 11 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.45it/s]Epoch 11 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.46it/s]Epoch 11 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.46it/s]Epoch 11 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 11 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.46it/s]Epoch 11 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 11 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 11 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.46it/s]Epoch 11 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.46it/s]Epoch 11 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.46it/s]Epoch 11 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 11 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 11 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.46it/s]Epoch 11 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.46it/s]Epoch 11 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.46it/s]Epoch 11 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 11 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 11 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 11 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.45it/s]Epoch 11 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 11 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                 Epoch 11 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 11 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.08it/s]Epoch 11 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.01it/s]Epoch 11 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.98it/s]Epoch 11 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.97it/s]Epoch 11 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.97it/s]Epoch 11 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.96it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:58:07,560][__main__][INFO] - Epoch 11: Val Loss: 1.4836, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.3010269576379974, 'f1_weighted': 0.5272563199837313, 'precision_macro': 0.26752533783783783, 'recall_macro': 0.35741525423728815}
Epoch 12 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:58:07,859][__main__][INFO] - Train Epoch: 12 [0/51 (0%)]	Loss: 0.592720
Epoch 12 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.36it/s]Epoch 12 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.38it/s]Epoch 12 [Train]:   6%|‚ñå         | 3/51 [00:00<00:14,  3.42it/s]Epoch 12 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.43it/s]Epoch 12 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.44it/s]Epoch 12 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.45it/s]Epoch 12 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 12 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 12 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 12 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 12 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 12 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 12 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 12 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.46it/s]Epoch 12 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 12 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 12 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 12 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 12 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 12 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 12 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 12 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.45it/s]Epoch 12 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 12 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.45it/s]Epoch 12 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.46it/s]Epoch 12 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.46it/s]Epoch 12 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.46it/s]Epoch 12 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.46it/s]Epoch 12 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 12 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 12 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.46it/s]Epoch 12 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.46it/s]Epoch 12 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.45it/s]Epoch 12 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 12 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 12 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 12 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.46it/s]Epoch 12 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.46it/s]Epoch 12 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.46it/s]Epoch 12 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 12 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 12 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.45it/s]Epoch 12 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 12 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 12 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 12 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.45it/s]Epoch 12 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 12 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 12 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 12 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                 Epoch 12 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 12 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.08it/s]Epoch 12 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.96it/s]Epoch 12 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.95it/s]Epoch 12 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.96it/s]Epoch 12 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.95it/s]Epoch 12 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.97it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:58:23,337][__main__][INFO] - Epoch 12: Val Loss: 1.1732, Accuracy: 0.6238, Metrics: {'accuracy': 0.6237623762376238, 'f1_macro': 0.27083333333333337, 'f1_weighted': 0.5255775577557756, 'precision_macro': 0.24871323529411765, 'recall_macro': 0.3082627118644068}
Epoch 13 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:58:23,630][__main__][INFO] - Train Epoch: 13 [0/51 (0%)]	Loss: 1.236901
Epoch 13 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.44it/s]Epoch 13 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.46it/s]Epoch 13 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.45it/s]Epoch 13 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 13 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.46it/s]Epoch 13 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.46it/s]Epoch 13 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.45it/s]Epoch 13 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.46it/s]Epoch 13 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.46it/s]Epoch 13 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 13 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 13 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 13 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 13 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.45it/s]Epoch 13 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 13 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 13 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 13 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 13 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 13 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 13 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 13 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.46it/s]Epoch 13 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.46it/s]Epoch 13 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.46it/s]Epoch 13 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.45it/s]Epoch 13 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.46it/s]Epoch 13 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.46it/s]Epoch 13 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.46it/s]Epoch 13 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.46it/s]Epoch 13 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.46it/s]Epoch 13 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.46it/s]Epoch 13 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 13 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.45it/s]Epoch 13 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 13 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.45it/s]Epoch 13 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.46it/s]Epoch 13 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.46it/s]Epoch 13 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.46it/s]Epoch 13 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 13 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 13 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 13 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 13 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 13 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.46it/s]Epoch 13 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.46it/s]Epoch 13 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 13 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.46it/s]Epoch 13 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.46it/s]Epoch 13 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 13 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 13 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                 Epoch 13 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 13 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.07it/s]Epoch 13 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 11.02it/s]Epoch 13 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.98it/s]Epoch 13 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.97it/s]Epoch 13 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.96it/s]Epoch 13 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.95it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:58:39,105][__main__][INFO] - Epoch 13: Val Loss: 1.3988, Accuracy: 0.6139, Metrics: {'accuracy': 0.6138613861386139, 'f1_macro': 0.31144550843593255, 'f1_weighted': 0.5460217704396979, 'precision_macro': 0.27534562211981567, 'recall_macro': 0.36186440677966103}
Epoch 14 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:58:39,397][__main__][INFO] - Train Epoch: 14 [0/51 (0%)]	Loss: 1.308831
Epoch 14 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.45it/s]Epoch 14 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.46it/s]Epoch 14 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.46it/s]Epoch 14 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.46it/s]Epoch 14 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.46it/s]Epoch 14 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.46it/s]Epoch 14 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.46it/s]Epoch 14 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.46it/s]Epoch 14 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.46it/s]Epoch 14 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.46it/s]Epoch 14 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.46it/s]Epoch 14 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 14 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 14 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.46it/s]Epoch 14 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.46it/s]Epoch 14 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.46it/s]Epoch 14 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.46it/s]Epoch 14 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.46it/s]Epoch 14 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 14 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.46it/s]Epoch 14 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.46it/s]Epoch 14 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.46it/s]Epoch 14 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.45it/s]Epoch 14 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.46it/s]Epoch 14 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.46it/s]Epoch 14 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.45it/s]Epoch 14 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.45it/s]Epoch 14 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.45it/s]Epoch 14 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.45it/s]Epoch 14 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.45it/s]Epoch 14 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:08<00:05,  3.45it/s]Epoch 14 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.45it/s]Epoch 14 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.45it/s]Epoch 14 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.46it/s]Epoch 14 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.46it/s]Epoch 14 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 14 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 14 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:10<00:03,  3.46it/s]Epoch 14 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.46it/s]Epoch 14 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 14 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.46it/s]Epoch 14 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.46it/s]Epoch 14 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.46it/s]Epoch 14 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 14 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 14 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.46it/s]Epoch 14 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.45it/s]Epoch 14 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.45it/s]Epoch 14 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 14 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.46it/s]Epoch 14 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.14it/s]                                                                 Epoch 14 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 14 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:00, 11.08it/s]Epoch 14 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.99it/s]Epoch 14 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.95it/s]Epoch 14 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.94it/s]Epoch 14 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.94it/s]Epoch 14 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.93it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:58:54,874][__main__][INFO] - Epoch 14: Val Loss: 1.2414, Accuracy: 0.2871, Metrics: {'accuracy': 0.2871287128712871, 'f1_macro': 0.27536932581251683, 'f1_weighted': 0.23714719578323668, 'precision_macro': 0.3084724378881988, 'recall_macro': 0.49268104776579347}
Epoch 15 [Train]:   0%|          | 0/51 [00:00<?, ?it/s][2025-06-02 23:58:55,174][__main__][INFO] - Train Epoch: 15 [0/51 (0%)]	Loss: 0.982478
Epoch 15 [Train]:   2%|‚ñè         | 1/51 [00:00<00:14,  3.39it/s]Epoch 15 [Train]:   4%|‚ñç         | 2/51 [00:00<00:14,  3.43it/s]Epoch 15 [Train]:   6%|‚ñå         | 3/51 [00:00<00:13,  3.44it/s]Epoch 15 [Train]:   8%|‚ñä         | 4/51 [00:01<00:13,  3.45it/s]Epoch 15 [Train]:  10%|‚ñâ         | 5/51 [00:01<00:13,  3.44it/s]Epoch 15 [Train]:  12%|‚ñà‚ñè        | 6/51 [00:01<00:13,  3.44it/s]Epoch 15 [Train]:  14%|‚ñà‚ñé        | 7/51 [00:02<00:12,  3.44it/s]Epoch 15 [Train]:  16%|‚ñà‚ñå        | 8/51 [00:02<00:12,  3.45it/s]Epoch 15 [Train]:  18%|‚ñà‚ñä        | 9/51 [00:02<00:12,  3.45it/s]Epoch 15 [Train]:  20%|‚ñà‚ñâ        | 10/51 [00:02<00:11,  3.45it/s]Epoch 15 [Train]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:03<00:11,  3.45it/s]Epoch 15 [Train]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:03<00:11,  3.46it/s]Epoch 15 [Train]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:03<00:10,  3.46it/s]Epoch 15 [Train]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:04<00:10,  3.46it/s]Epoch 15 [Train]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:04<00:10,  3.45it/s]Epoch 15 [Train]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:04<00:10,  3.45it/s]Epoch 15 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:04<00:09,  3.45it/s]Epoch 15 [Train]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:05<00:09,  3.45it/s]Epoch 15 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:05<00:09,  3.45it/s]Epoch 15 [Train]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:05<00:08,  3.45it/s]Epoch 15 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:06<00:08,  3.45it/s]Epoch 15 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:06<00:08,  3.44it/s]Epoch 15 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:06<00:08,  3.44it/s]Epoch 15 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:06<00:07,  3.41it/s]Epoch 15 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:07<00:07,  3.42it/s]Epoch 15 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:07<00:07,  3.43it/s]Epoch 15 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:07<00:06,  3.43it/s]Epoch 15 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:08<00:06,  3.44it/s]Epoch 15 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:08<00:06,  3.44it/s]Epoch 15 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:08<00:06,  3.44it/s]Epoch 15 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:09<00:05,  3.44it/s]Epoch 15 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:09<00:05,  3.44it/s]Epoch 15 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:09<00:05,  3.44it/s]Epoch 15 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:09<00:04,  3.44it/s]Epoch 15 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:10<00:04,  3.44it/s]Epoch 15 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:10<00:04,  3.45it/s]Epoch 15 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:10<00:04,  3.45it/s]Epoch 15 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:11<00:03,  3.45it/s]Epoch 15 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:11<00:03,  3.45it/s]Epoch 15 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:11<00:03,  3.45it/s]Epoch 15 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:11<00:02,  3.45it/s]Epoch 15 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:12<00:02,  3.45it/s]Epoch 15 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:12<00:02,  3.45it/s]Epoch 15 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:12<00:02,  3.45it/s]Epoch 15 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:13<00:01,  3.45it/s]Epoch 15 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:13<00:01,  3.45it/s]Epoch 15 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:13<00:01,  3.45it/s]Epoch 15 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:13<00:00,  3.45it/s]Epoch 15 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:14<00:00,  3.46it/s]Epoch 15 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:14<00:00,  3.45it/s]Epoch 15 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:14<00:00,  4.13it/s]                                                                 Epoch 15 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 15 [Val]:  15%|‚ñà‚ñå        | 2/13 [00:00<00:01, 11.00it/s]Epoch 15 [Val]:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:00<00:00, 10.93it/s]Epoch 15 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:00<00:00, 10.94it/s]Epoch 15 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:00<00:00, 10.96it/s]Epoch 15 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [00:00<00:00, 10.96it/s]Epoch 15 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [00:01<00:00, 10.96it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-02 23:59:10,688][__main__][INFO] - Epoch 15: Val Loss: 1.0047, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.3010269576379974, 'f1_weighted': 0.5272563199837313, 'precision_macro': 0.26752533783783783, 'recall_macro': 0.35741525423728815}
[2025-06-02 23:59:10,689][__main__][INFO] - Early stopping triggered after 15 epochs
[2025-06-02 23:59:10,690][__main__][INFO] - Saving best confusion matrix with accuracy: 0.6733
(0, 0, 8, 6)
<numpy.flatiter object at 0x7f39a677d4a0>
<numpy.flatiter object at 0x7f39a677d4a0>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7f3808a14540>
<numpy.flatiter object at 0x7f3808a14540>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f3808a14540>
<numpy.flatiter object at 0x7f3808a14540>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7f3808a14b40>
<numpy.flatiter object at 0x7f3808a14b40>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7f3808a14b40>
<numpy.flatiter object at 0x7f3808a14b40>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:              Best Accuracy ‚ñÅ‚ñÜ‚ñà
wandb:                 Train Loss ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñÖ
wandb:        Validation Accuracy ‚ñá‚ñá‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñá
wandb:            Validation Loss ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÅ
wandb:        Validation accuracy ‚ñá‚ñá‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñá
wandb:        Validation f1_macro ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ
wandb:     Validation f1_weighted ‚ñÜ‚ñÜ‚ñÅ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÉ‚ñá
wandb: Validation precision_macro ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ
wandb:    Validation recall_macro ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÑ
wandb: 
wandb: Run summary:
wandb:              Best Accuracy 0.67327
wandb:                 Train Loss 0.98248
wandb:        Validation Accuracy 0.58416
wandb:            Validation Loss 1.00474
wandb:        Validation accuracy 0.58416
wandb:        Validation f1_macro 0.30103
wandb:     Validation f1_weighted 0.52726
wandb: Validation precision_macro 0.26753
wandb:    Validation recall_macro 0.35742
wandb: 
wandb: üöÄ View run noble-sweep-1 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5izjzx6k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_235451-5izjzx6k/logs
wandb: Agent Starting Run: cng0hx69 with config:
wandb: 	Fdropout_rate: 0.3535391601378939
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.3847901689080376
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.1157996722003428
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250602_235917-cng0hx69
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cng0hx69
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.

(-8.8125, -7.0, 8.8125, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
Training started...
Epoch 0 [Train]:   0%|          | 0/2 [00:00<?, ?it/s]                                                      wandb:                                                                                
wandb: üöÄ View run splendid-sweep-2 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cng0hx69
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_235917-cng0hx69/logs
wandb: ERROR Run cng0hx69 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 99, in train
wandb: ERROR     output = model(*features)
wandb: ERROR              ^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/src/models/naim_text.py", line 478, in forward
wandb: ERROR     text_features = self.vitbi(input_ids, attention_mask)
wandb: ERROR                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/src/models/naim_text.py", line 145, in forward
wandb: ERROR     outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward
wandb: ERROR     encoder_outputs = self.encoder(
wandb: ERROR                       ^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward
wandb: ERROR     layer_outputs = layer_module(
wandb: ERROR                     ^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
wandb: ERROR     self_attention_outputs = self.attention(
wandb: ERROR                              ^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 524, in forward
wandb: ERROR     attention_output = self.output(self_outputs[0], hidden_states)
wandb: ERROR                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 466, in forward
wandb: ERROR     hidden_states = self.dense(hidden_states)
wandb: ERROR                     ^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
wandb: ERROR     return F.linear(input, self.weight, self.bias)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 20.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.01 GiB memory in use. Of the allocated memory 46.69 GiB is allocated by PyTorch, and 119.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nsyau1v8 with config:
wandb: 	Fdropout_rate: 0.25114902092639835
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.29066710341905255
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.12883685629232616
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250602_235928-nsyau1v8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nsyau1v8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run clean-sweep-3 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nsyau1v8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_235928-nsyau1v8/logs
wandb: ERROR Run nsyau1v8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 18.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.01 GiB memory in use. Of the allocated memory 46.69 GiB is allocated by PyTorch, and 120.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sde3c4r6 with config:
wandb: 	Fdropout_rate: 0.3122604871675706
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.3374996340323778
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.0744860750099777
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250602_235939-sde3c4r6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/sde3c4r6
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run summer-sweep-4 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/sde3c4r6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_235939-sde3c4r6/logs
wandb: ERROR Run sde3c4r6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 14.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.70 GiB is allocated by PyTorch, and 120.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: clyuilyf with config:
wandb: 	Fdropout_rate: 0.4578301910507758
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.31319580057492413
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.06657441948194165
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250602_235949-clyuilyf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/clyuilyf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run quiet-sweep-5 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/clyuilyf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_235949-clyuilyf/logs
wandb: ERROR Run clyuilyf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 14.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.70 GiB is allocated by PyTorch, and 119.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: o3mc3m2a with config:
wandb: 	Fdropout_rate: 0.29735195404743686
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.1555603344495012
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.019497653458662854
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000000-o3mc3m2a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/o3mc3m2a
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run charmed-sweep-6 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/o3mc3m2a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000000-o3mc3m2a/logs
wandb: ERROR Run o3mc3m2a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 12.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.70 GiB is allocated by PyTorch, and 121.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: tdlx4mw2 with config:
wandb: 	Fdropout_rate: 0.1163224467288634
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.30987501509098936
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 64
wandb: 	learning_rate: 0.022300109552152052
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000011-tdlx4mw2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/tdlx4mw2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run trim-sweep-7 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/tdlx4mw2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000011-tdlx4mw2/logs
wandb: ERROR Run tdlx4mw2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 10.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.70 GiB is allocated by PyTorch, and 120.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8v8r7yiv with config:
wandb: 	Fdropout_rate: 0.418005815752039
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.20994338058360065
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.03477191663637548
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000022-8v8r7yiv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/8v8r7yiv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lunar-sweep-8 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/8v8r7yiv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000022-8v8r7yiv/logs
wandb: ERROR Run 8v8r7yiv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 6.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.70 GiB is allocated by PyTorch, and 120.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6m57pf3z with config:
wandb: 	Fdropout_rate: 0.4590728574314103
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2893892763718777
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.0410451599150892
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000032-6m57pf3z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/6m57pf3z
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run decent-sweep-9 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/6m57pf3z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000032-6m57pf3z/logs
wandb: ERROR Run 6m57pf3z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 4.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 121.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: m7g7el29 with config:
wandb: 	Fdropout_rate: 0.3985926842145081
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2993978087701715
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.02800162666287374
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000043-m7g7el29
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/m7g7el29
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fanciful-sweep-10 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/m7g7el29
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000043-m7g7el29/logs
wandb: ERROR Run m7g7el29 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 121.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: jdg3m7jg with config:
wandb: 	Fdropout_rate: 0.3112496492325372
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.39206472134422776
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.07488023467097522
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000054-jdg3m7jg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/jdg3m7jg
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run true-sweep-11 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/jdg3m7jg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000054-jdg3m7jg/logs
wandb: ERROR Run jdg3m7jg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 2 more times]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dhaw7ia8 with config:
wandb: 	Fdropout_rate: 0.10379914855191946
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.22650499611662545
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.048876285036505554
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000105-dhaw7ia8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/dhaw7ia8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run jolly-sweep-12 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/dhaw7ia8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000105-dhaw7ia8/logs
wandb: ERROR Run dhaw7ia8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ka9xs9gn with config:
wandb: 	Fdropout_rate: 0.10703633925393632
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.3570372345809548
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.04768051887022462
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000116-ka9xs9gn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ka9xs9gn
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run worthy-sweep-13 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ka9xs9gn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000116-ka9xs9gn/logs
wandb: ERROR Run ka9xs9gn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s53wgsx2 with config:
wandb: 	Fdropout_rate: 0.1357639935871065
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.20320815871244713
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.08862609910018118
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000127-s53wgsx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/s53wgsx2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run laced-sweep-14 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/s53wgsx2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000127-s53wgsx2/logs
wandb: ERROR Run s53wgsx2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: q67tt91n with config:
wandb: 	Fdropout_rate: 0.11674899636066748
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.21886698102172544
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.11043053710833556
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000142-q67tt91n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/q67tt91n
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run solar-sweep-15 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/q67tt91n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000142-q67tt91n/logs
wandb: ERROR Run q67tt91n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: tg4jcb4b with config:
wandb: 	Fdropout_rate: 0.35971663497969597
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.21168061453252768
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06548840423714905
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000153-tg4jcb4b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/tg4jcb4b
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run likely-sweep-16 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/tg4jcb4b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000153-tg4jcb4b/logs
wandb: ERROR Run tg4jcb4b errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: zk9xq9ae with config:
wandb: 	Fdropout_rate: 0.22974211216054607
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.1540369265134493
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.11596726980120296
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000209-zk9xq9ae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/zk9xq9ae
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run curious-sweep-17 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/zk9xq9ae
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000209-zk9xq9ae/logs
wandb: ERROR Run zk9xq9ae errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vh7vnt0z with config:
wandb: 	Fdropout_rate: 0.21800063790324145
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.387207461137256
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.11189769027591032
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000221-vh7vnt0z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/vh7vnt0z
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run pretty-sweep-18 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/vh7vnt0z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000221-vh7vnt0z/logs
wandb: ERROR Run vh7vnt0z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: l9olkjsv with config:
wandb: 	Fdropout_rate: 0.3777039354280023
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.17849337163679463
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.042349568100081975
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000231-l9olkjsv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/l9olkjsv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run unique-sweep-19 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/l9olkjsv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000231-l9olkjsv/logs
wandb: ERROR Run l9olkjsv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y4rox315 with config:
wandb: 	Fdropout_rate: 0.2692816286351343
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.15146724866501443
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.025074659249581435
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000242-y4rox315
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/y4rox315
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run drawn-sweep-20 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/y4rox315
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000242-y4rox315/logs
wandb: ERROR Run y4rox315 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5q78g4ja with config:
wandb: 	Fdropout_rate: 0.3957570277429928
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.32325650295660024
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.0765027000688242
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000258-5q78g4ja
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5q78g4ja
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run ruby-sweep-21 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5q78g4ja
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000258-5q78g4ja/logs
wandb: ERROR Run 5q78g4ja errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lwia5aoh with config:
wandb: 	Fdropout_rate: 0.23649323590195948
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.3128988596643997
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.04230938818993002
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000309-lwia5aoh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/lwia5aoh
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run super-sweep-22 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/lwia5aoh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000309-lwia5aoh/logs
wandb: ERROR Run lwia5aoh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 0e6ehro8 with config:
wandb: 	Fdropout_rate: 0.4978742043656277
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3067907415143962
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.0963192893596982
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000320-0e6ehro8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/0e6ehro8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run expert-sweep-23 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/0e6ehro8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000320-0e6ehro8/logs
wandb: ERROR Run 0e6ehro8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7pjfrmf7 with config:
wandb: 	Fdropout_rate: 0.1869466283651996
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.23303691602767992
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.023059765002101124
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000331-7pjfrmf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7pjfrmf7
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fancy-sweep-24 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7pjfrmf7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000331-7pjfrmf7/logs
wandb: ERROR Run 7pjfrmf7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8caba0vx with config:
wandb: 	Fdropout_rate: 0.4692477946632019
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.22250055168243216
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.07511492195304362
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000346-8caba0vx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/8caba0vx
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fearless-sweep-25 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/8caba0vx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000346-8caba0vx/logs
wandb: ERROR Run 8caba0vx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 0tu1dy86 with config:
wandb: 	Fdropout_rate: 0.1705821654777091
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.17122754540150592
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.06424902799919083
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000403-0tu1dy86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/0tu1dy86
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run vocal-sweep-26 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/0tu1dy86
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000403-0tu1dy86/logs
wandb: ERROR Run 0tu1dy86 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9ly8dr1z with config:
wandb: 	Fdropout_rate: 0.35172911707991283
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.3347237647086049
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.0598741510296202
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000419-9ly8dr1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/9ly8dr1z
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run elated-sweep-27 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/9ly8dr1z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000419-9ly8dr1z/logs
wandb: ERROR Run 9ly8dr1z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rzocni3u with config:
wandb: 	Fdropout_rate: 0.11370366038611444
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.13452681265659705
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 16
wandb: 	learning_rate: 0.1061892623501316
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000431-rzocni3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/rzocni3u
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fine-sweep-28 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/rzocni3u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000431-rzocni3u/logs
wandb: ERROR Run rzocni3u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: n7l50ah0 with config:
wandb: 	Fdropout_rate: 0.1565672218354165
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.23100535621544277
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.05070884185942359
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000446-n7l50ah0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/n7l50ah0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run jumping-sweep-29 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/n7l50ah0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000446-n7l50ah0/logs
wandb: ERROR Run n7l50ah0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jt7zqbv0 with config:
wandb: 	Fdropout_rate: 0.19914967987970728
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.24712598667114016
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.07139941710879358
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000502-jt7zqbv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/jt7zqbv0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run ethereal-sweep-30 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/jt7zqbv0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000502-jt7zqbv0/logs
wandb: ERROR Run jt7zqbv0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lt9hm64w with config:
wandb: 	Fdropout_rate: 0.1506896620271373
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.1497318789038095
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 64
wandb: 	learning_rate: 0.023968003410309913
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000509-lt9hm64w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/lt9hm64w
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run chocolate-sweep-31 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/lt9hm64w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000509-lt9hm64w/logs
wandb: ERROR Run lt9hm64w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: atd554ih with config:
wandb: 	Fdropout_rate: 0.3435665568357257
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.3268380385651237
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 8
wandb: 	d_token: 16
wandb: 	learning_rate: 0.02801983732397834
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000520-atd554ih
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/atd554ih
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run misty-sweep-32 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/atd554ih
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000520-atd554ih/logs
wandb: ERROR Run atd554ih errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: odg04m0q with config:
wandb: 	Fdropout_rate: 0.4183648609113312
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.38306041933855184
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.03418417638675563
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000535-odg04m0q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/odg04m0q
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run proud-sweep-33 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/odg04m0q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000535-odg04m0q/logs
wandb: ERROR Run odg04m0q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: v4rl23wf with config:
wandb: 	Fdropout_rate: 0.22778567540706057
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.2362454120527448
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03696683832694609
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000547-v4rl23wf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/v4rl23wf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run happy-sweep-34 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/v4rl23wf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000547-v4rl23wf/logs
wandb: ERROR Run v4rl23wf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nrdg31as with config:
wandb: 	Fdropout_rate: 0.48334873655761135
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.38118708294418846
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.02508162714044951
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000557-nrdg31as
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nrdg31as
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run ethereal-sweep-35 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nrdg31as
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000557-nrdg31as/logs
wandb: ERROR Run nrdg31as errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cf10bewv with config:
wandb: 	Fdropout_rate: 0.3012952097257623
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.23565376965365745
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.02506835655387296
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000608-cf10bewv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cf10bewv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run earnest-sweep-36 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cf10bewv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000608-cf10bewv/logs
wandb: ERROR Run cf10bewv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sz18wrr6 with config:
wandb: 	Fdropout_rate: 0.12254505669720488
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.27054939034091985
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.035878067857417456
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000619-sz18wrr6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/sz18wrr6
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run drawn-sweep-37 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/sz18wrr6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000619-sz18wrr6/logs
wandb: ERROR Run sz18wrr6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9p3zfk4v with config:
wandb: 	Fdropout_rate: 0.3934404315358596
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2663982218710676
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03352409420085492
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000636-9p3zfk4v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/9p3zfk4v
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run electric-sweep-38 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/9p3zfk4v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000636-9p3zfk4v/logs
wandb: ERROR Run 9p3zfk4v errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vvuhv5pf with config:
wandb: 	Fdropout_rate: 0.4143161074289441
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.2986853428842652
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.12313381010282654
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000648-vvuhv5pf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/vvuhv5pf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run earthy-sweep-39 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/vvuhv5pf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000648-vvuhv5pf/logs
wandb: ERROR Run vvuhv5pf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 38mix5z8 with config:
wandb: 	Fdropout_rate: 0.11449516233169638
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.26632842014113656
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.04306476337371884
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000704-38mix5z8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/38mix5z8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lyric-sweep-40 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/38mix5z8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000704-38mix5z8/logs
wandb: ERROR Run 38mix5z8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gipb3gbl with config:
wandb: 	Fdropout_rate: 0.14347006829352563
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3454394377534953
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.04575033380912364
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000712-gipb3gbl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/gipb3gbl
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run earthy-sweep-41 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/gipb3gbl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000712-gipb3gbl/logs
wandb: ERROR Run gipb3gbl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 84enkp3b with config:
wandb: 	Fdropout_rate: 0.3434921333701071
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.222095443829245
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.08019058141901117
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000723-84enkp3b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/84enkp3b
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run still-sweep-42 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/84enkp3b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000723-84enkp3b/logs
wandb: ERROR Run 84enkp3b errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a77i3cx8 with config:
wandb: 	Fdropout_rate: 0.2509037243753389
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.24762779463857051
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.10280572571472628
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000734-a77i3cx8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/a77i3cx8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run neat-sweep-43 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/a77i3cx8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000734-a77i3cx8/logs
wandb: ERROR Run a77i3cx8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xwwuj5ly with config:
wandb: 	Fdropout_rate: 0.3567765390021763
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.10911611397127026
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.0758454790654345
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000745-xwwuj5ly
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/xwwuj5ly
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run visionary-sweep-44 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/xwwuj5ly
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000745-xwwuj5ly/logs
wandb: ERROR Run xwwuj5ly errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7ibcw1kd with config:
wandb: 	Fdropout_rate: 0.4672015211791763
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.1752503115052803
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 32
wandb: 	learning_rate: 0.021060705000632417
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000756-7ibcw1kd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7ibcw1kd
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run autumn-sweep-45 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7ibcw1kd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000756-7ibcw1kd/logs
wandb: ERROR Run 7ibcw1kd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h8qg6j84 with config:
wandb: 	Fdropout_rate: 0.11987310235113796
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.18778125713049132
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0330682658207308
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000811-h8qg6j84
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/h8qg6j84
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run iconic-sweep-46 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/h8qg6j84
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000811-h8qg6j84/logs
wandb: ERROR Run h8qg6j84 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8k8i4uef with config:
wandb: 	Fdropout_rate: 0.44398451433697594
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.3599890155827318
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.05074450964307921
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000823-8k8i4uef
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/8k8i4uef
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run desert-sweep-47 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/8k8i4uef
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000823-8k8i4uef/logs
wandb: ERROR Run 8k8i4uef errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5xrod9by with config:
wandb: 	Fdropout_rate: 0.18558284843389825
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.2903569925766912
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0642094617649183
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000833-5xrod9by
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5xrod9by
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run copper-sweep-48 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5xrod9by
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000833-5xrod9by/logs
wandb: ERROR Run 5xrod9by errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rb0v99s3 with config:
wandb: 	Fdropout_rate: 0.46650818716267295
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3766356405653254
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.038779295484042714
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000844-rb0v99s3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/rb0v99s3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lively-sweep-49 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/rb0v99s3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000844-rb0v99s3/logs
wandb: ERROR Run rb0v99s3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: pjges872 with config:
wandb: 	Fdropout_rate: 0.27050385233888363
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.25669885271224296
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.055686844896019
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000855-pjges872
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/pjges872
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run glowing-sweep-50 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/pjges872
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000855-pjges872/logs
wandb: ERROR Run pjges872 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: mzu8cixe with config:
wandb: 	Fdropout_rate: 0.23803004738867128
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.11939826753375649
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.0761414756259658
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000926-mzu8cixe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/mzu8cixe
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dainty-sweep-51 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/mzu8cixe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000926-mzu8cixe/logs
wandb: ERROR Run mzu8cixe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wtyxsla5 with config:
wandb: 	Fdropout_rate: 0.4343418712506024
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2567991086031778
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.08632311670078884
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_000937-wtyxsla5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/wtyxsla5
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run happy-sweep-52 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/wtyxsla5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_000937-wtyxsla5/logs
wandb: ERROR Run wtyxsla5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jdbnb8i3 with config:
wandb: 	Fdropout_rate: 0.19438158362833843
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.3195955209830438
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.05924324138829736
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001006-jdbnb8i3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/jdbnb8i3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run colorful-sweep-53 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/jdbnb8i3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001006-jdbnb8i3/logs
wandb: ERROR Run jdbnb8i3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8836g7y5 with config:
wandb: 	Fdropout_rate: 0.2581781157124897
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.34710874177260653
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.03005785878758152
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001017-8836g7y5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/8836g7y5
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zesty-sweep-54 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/8836g7y5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001017-8836g7y5/logs
wandb: ERROR Run 8836g7y5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ak1dfrbj with config:
wandb: 	Fdropout_rate: 0.41013873967371695
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.20163663373266896
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03350222986200073
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001028-ak1dfrbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ak1dfrbj
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run likely-sweep-55 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ak1dfrbj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001028-ak1dfrbj/logs
wandb: ERROR Run ak1dfrbj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lgc4bts1 with config:
wandb: 	Fdropout_rate: 0.4410790671107968
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.28884667443360607
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.10161170260034191
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001039-lgc4bts1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/lgc4bts1
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run autumn-sweep-56 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/lgc4bts1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001039-lgc4bts1/logs
wandb: ERROR Run lgc4bts1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5j3cgw0x with config:
wandb: 	Fdropout_rate: 0.14480240669319927
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3437029797553287
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.05381505655552615
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001115-5j3cgw0x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5j3cgw0x
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dainty-sweep-57 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5j3cgw0x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001115-5j3cgw0x/logs
wandb: ERROR Run 5j3cgw0x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: uevu9em8 with config:
wandb: 	Fdropout_rate: 0.11575044075289707
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.1325254487350945
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.032081635223572325
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001123-uevu9em8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/uevu9em8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run true-sweep-58 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/uevu9em8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001123-uevu9em8/logs
wandb: ERROR Run uevu9em8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: g2h0751k with config:
wandb: 	Fdropout_rate: 0.2331857118045663
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.20533214870523825
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.0446362534797428
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001134-g2h0751k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/g2h0751k
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dazzling-sweep-59 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/g2h0751k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001134-g2h0751k/logs
wandb: ERROR Run g2h0751k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: z1eqah1i with config:
wandb: 	Fdropout_rate: 0.44863989458044307
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.270291894998549
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.047065317244411695
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001145-z1eqah1i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/z1eqah1i
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run robust-sweep-60 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/z1eqah1i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001145-z1eqah1i/logs
wandb: ERROR Run z1eqah1i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 0b3a6dy9 with config:
wandb: 	Fdropout_rate: 0.3646254392622525
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.26810328128222227
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.06494954134172837
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001156-0b3a6dy9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/0b3a6dy9
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run eternal-sweep-61 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/0b3a6dy9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001156-0b3a6dy9/logs
wandb: ERROR Run 0b3a6dy9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: o1iir01x with config:
wandb: 	Fdropout_rate: 0.18429247836953783
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.23672484095161597
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.02551757542932836
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001206-o1iir01x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/o1iir01x
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run unique-sweep-62 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/o1iir01x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001206-o1iir01x/logs
wandb: ERROR Run o1iir01x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: u0wkrge3 with config:
wandb: 	Fdropout_rate: 0.43685153954611655
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.3345245020406612
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.12306058890315824
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001217-u0wkrge3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/u0wkrge3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run swift-sweep-63 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/u0wkrge3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001217-u0wkrge3/logs
wandb: ERROR Run u0wkrge3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ccbn6fe0 with config:
wandb: 	Fdropout_rate: 0.3630775610217728
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.21222863338156983
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.02443814537103972
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001229-ccbn6fe0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ccbn6fe0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run deep-sweep-64 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ccbn6fe0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001229-ccbn6fe0/logs
wandb: ERROR Run ccbn6fe0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: acp0zjuk with config:
wandb: 	Fdropout_rate: 0.11443155302408568
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.1549090834452382
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 32
wandb: 	learning_rate: 0.10855879839637105
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001244-acp0zjuk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/acp0zjuk
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run gentle-sweep-65 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/acp0zjuk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001244-acp0zjuk/logs
wandb: ERROR Run acp0zjuk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ohhzjxnr with config:
wandb: 	Fdropout_rate: 0.1104167586062184
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.13911789598448848
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 64
wandb: 	learning_rate: 0.11590997163511474
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001300-ohhzjxnr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ohhzjxnr
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run morning-sweep-66 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ohhzjxnr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001300-ohhzjxnr/logs
wandb: ERROR Run ohhzjxnr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1pozwjmf with config:
wandb: 	Fdropout_rate: 0.2210506226167258
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.38779856481501906
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.03649905321529926
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001312-1pozwjmf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/1pozwjmf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run cool-sweep-67 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/1pozwjmf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001312-1pozwjmf/logs
wandb: ERROR Run 1pozwjmf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2tiye24l with config:
wandb: 	Fdropout_rate: 0.4340533745671007
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.2957203157871575
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.020267319362879368
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001327-2tiye24l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/2tiye24l
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lilac-sweep-68 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/2tiye24l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001327-2tiye24l/logs
wandb: ERROR Run 2tiye24l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n8ok8hgp with config:
wandb: 	Fdropout_rate: 0.15675044670287278
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.1757195162036248
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 32
wandb: 	learning_rate: 0.10275076468083771
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001339-n8ok8hgp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/n8ok8hgp
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run distinctive-sweep-69 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/n8ok8hgp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001339-n8ok8hgp/logs
wandb: ERROR Run n8ok8hgp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7ebiocuz with config:
wandb: 	Fdropout_rate: 0.1333572497015587
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2442505729440832
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.04511729320153985
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001350-7ebiocuz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7ebiocuz
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run whole-sweep-70 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7ebiocuz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001350-7ebiocuz/logs
wandb: ERROR Run 7ebiocuz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: u86teu7w with config:
wandb: 	Fdropout_rate: 0.3631884037743659
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.3743384152458066
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.04147632003649121
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001406-u86teu7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/u86teu7w
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run happy-sweep-71 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/u86teu7w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001406-u86teu7w/logs
wandb: ERROR Run u86teu7w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: q7ppjam1 with config:
wandb: 	Fdropout_rate: 0.1987872501252082
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.33171169274116996
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.022933035804238652
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001422-q7ppjam1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-72
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/q7ppjam1
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zesty-sweep-72 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/q7ppjam1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001422-q7ppjam1/logs
wandb: ERROR Run q7ppjam1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: zj8ycjdr with config:
wandb: 	Fdropout_rate: 0.2809956117911553
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.15913592069850674
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.051969329147375384
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001433-zj8ycjdr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/zj8ycjdr
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run visionary-sweep-73 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/zj8ycjdr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001433-zj8ycjdr/logs
wandb: ERROR Run zj8ycjdr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bq57mdio with config:
wandb: 	Fdropout_rate: 0.4823758488576751
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.1572042907017079
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.04315910732854016
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001444-bq57mdio
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/bq57mdio
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run polar-sweep-74 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/bq57mdio
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001444-bq57mdio/logs
wandb: ERROR Run bq57mdio errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 012nfoyo with config:
wandb: 	Fdropout_rate: 0.2430823638558556
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.12693018264300837
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.02857740395269459
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001455-012nfoyo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/012nfoyo
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run trim-sweep-75 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/012nfoyo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001455-012nfoyo/logs
wandb: ERROR Run 012nfoyo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: u3nk91a9 with config:
wandb: 	Fdropout_rate: 0.21364330634668025
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.37102541909691367
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.054699289156502726
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001506-u3nk91a9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-76
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/u3nk91a9
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run rare-sweep-76 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/u3nk91a9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001506-u3nk91a9/logs
wandb: ERROR Run u3nk91a9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cn8g1tb5 with config:
wandb: 	Fdropout_rate: 0.12182451375729388
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.1141892667274068
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0925726788061554
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001517-cn8g1tb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cn8g1tb5
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run upbeat-sweep-77 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cn8g1tb5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001517-cn8g1tb5/logs
wandb: ERROR Run cn8g1tb5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ebt631vi with config:
wandb: 	Fdropout_rate: 0.2671215662465947
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.3014800702717824
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.08015112531123135
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001533-ebt631vi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ebt631vi
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run polished-sweep-78 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ebt631vi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001533-ebt631vi/logs
wandb: ERROR Run ebt631vi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: awa1t9oh with config:
wandb: 	Fdropout_rate: 0.3837118891140416
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.320289791962854
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.11536189160127
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001544-awa1t9oh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/awa1t9oh
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lyric-sweep-79 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/awa1t9oh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001544-awa1t9oh/logs
wandb: ERROR Run awa1t9oh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2q0w08ro with config:
wandb: 	Fdropout_rate: 0.32666775290240513
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.2075354639469507
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.059427606662932904
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001555-2q0w08ro
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/2q0w08ro
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fragrant-sweep-80 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/2q0w08ro
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001555-2q0w08ro/logs
wandb: ERROR Run 2q0w08ro errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 83a8c8vw with config:
wandb: 	Fdropout_rate: 0.18751233233421663
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.2209277938738252
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.0196838075356099
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001606-83a8c8vw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/83a8c8vw
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run vocal-sweep-81 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/83a8c8vw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001606-83a8c8vw/logs
wandb: ERROR Run 83a8c8vw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5t3qgexw with config:
wandb: 	Fdropout_rate: 0.36800218324558587
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.22284453374362911
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03934123033135086
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001617-5t3qgexw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-82
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5t3qgexw
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run upbeat-sweep-82 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5t3qgexw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001617-5t3qgexw/logs
wandb: ERROR Run 5t3qgexw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7pra2knf with config:
wandb: 	Fdropout_rate: 0.24380489884860196
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.2458971204375029
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 64
wandb: 	learning_rate: 0.13419912072415946
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001628-7pra2knf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7pra2knf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fallen-sweep-83 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7pra2knf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001628-7pra2knf/logs
wandb: ERROR Run 7pra2knf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xz31n7he with config:
wandb: 	Fdropout_rate: 0.39447853920898257
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.3381543624755324
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.06619561305377161
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001639-xz31n7he
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/xz31n7he
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run silver-sweep-84 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/xz31n7he
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001639-xz31n7he/logs
wandb: ERROR Run xz31n7he errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s3ephkbp with config:
wandb: 	Fdropout_rate: 0.17016773212663133
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.13031434584684182
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.03176918752629844
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001650-s3ephkbp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/s3ephkbp
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lively-sweep-85 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/s3ephkbp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001650-s3ephkbp/logs
wandb: ERROR Run s3ephkbp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s6qj7z6p with config:
wandb: 	Fdropout_rate: 0.4031276811646478
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.33322764350862133
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.08943901612855132
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001701-s6qj7z6p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/s6qj7z6p
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run earnest-sweep-86 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/s6qj7z6p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001701-s6qj7z6p/logs
wandb: ERROR Run s6qj7z6p errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: fv0azttf with config:
wandb: 	Fdropout_rate: 0.20236769323389925
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3708194905885298
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.019848609133884143
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001711-fv0azttf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/fv0azttf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run vocal-sweep-87 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/fv0azttf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001711-fv0azttf/logs
wandb: ERROR Run fv0azttf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: drohzm2s with config:
wandb: 	Fdropout_rate: 0.4144360829552818
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.28710056663180145
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.08287304429166868
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001723-drohzm2s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-88
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/drohzm2s
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dazzling-sweep-88 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/drohzm2s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001723-drohzm2s/logs
wandb: ERROR Run drohzm2s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2pktxvmz with config:
wandb: 	Fdropout_rate: 0.3907984801047347
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.1088444369039822
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.019155162572214383
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001738-2pktxvmz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/2pktxvmz
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run hearty-sweep-89 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/2pktxvmz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001738-2pktxvmz/logs
wandb: ERROR Run 2pktxvmz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: x43wikz1 with config:
wandb: 	Fdropout_rate: 0.21072045012169136
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.3655059815368345
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.07625239189810781
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001754-x43wikz1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/x43wikz1
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run true-sweep-90 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/x43wikz1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001754-x43wikz1/logs
wandb: ERROR Run x43wikz1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mfrcra5l with config:
wandb: 	Fdropout_rate: 0.2615931440216548
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.36519687819095936
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.035031889992796494
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001806-mfrcra5l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/mfrcra5l
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fresh-sweep-91 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/mfrcra5l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001806-mfrcra5l/logs
wandb: ERROR Run mfrcra5l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: d7q0sghf with config:
wandb: 	Fdropout_rate: 0.4650822247335217
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3211154955123297
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.019831967470889656
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001821-d7q0sghf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-92
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/d7q0sghf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run cool-sweep-92 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/d7q0sghf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001821-d7q0sghf/logs
wandb: ERROR Run d7q0sghf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: aez9sruw with config:
wandb: 	Fdropout_rate: 0.2934704088303167
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2995844263534818
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.03549367353656365
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001858-aez9sruw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/aez9sruw
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fiery-sweep-93 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/aez9sruw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001858-aez9sruw/logs
wandb: ERROR Run aez9sruw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dananhkc with config:
wandb: 	Fdropout_rate: 0.1726028624400572
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.1982934206612681
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.1284112591506988
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001910-dananhkc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/dananhkc
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fresh-sweep-94 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/dananhkc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001910-dananhkc/logs
wandb: ERROR Run dananhkc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1b2ypc5g with config:
wandb: 	Fdropout_rate: 0.15112173074055502
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.1288917478894539
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 64
wandb: 	learning_rate: 0.02277396323524227
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001921-1b2ypc5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/1b2ypc5g
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run wandering-sweep-95 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/1b2ypc5g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001921-1b2ypc5g/logs
wandb: ERROR Run 1b2ypc5g errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gw47b0w7 with config:
wandb: 	Fdropout_rate: 0.1655190958096316
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.2279829114259481
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.029898780148808857
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001932-gw47b0w7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/gw47b0w7
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lively-sweep-96 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/gw47b0w7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001932-gw47b0w7/logs
wandb: ERROR Run gw47b0w7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: d2n1rl4z with config:
wandb: 	Fdropout_rate: 0.1981313036236049
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.17973315975619575
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 32
wandb: 	learning_rate: 0.03203679815200092
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_001959-d2n1rl4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/d2n1rl4z
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zesty-sweep-97 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/d2n1rl4z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_001959-d2n1rl4z/logs
wandb: ERROR Run d2n1rl4z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6bw5hmua with config:
wandb: 	Fdropout_rate: 0.1743640592518726
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.35237514134843373
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.09374427274224124
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_002007-6bw5hmua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/6bw5hmua
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run honest-sweep-98 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/6bw5hmua
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_002007-6bw5hmua/logs
wandb: ERROR Run 6bw5hmua errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: i0144yy2 with config:
wandb: 	Fdropout_rate: 0.2328740533768397
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.25797421113141955
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06508708308271219
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_002018-i0144yy2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-99
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/i0144yy2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run glorious-sweep-99 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/i0144yy2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_002018-i0144yy2/logs
wandb: ERROR Run i0144yy2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: d3ya39ms with config:
wandb: 	Fdropout_rate: 0.18084574334630243
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.2627950057054217
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.10695361970456332
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-02/23-54-49/wandb/run-20250603_002029-d3ya39ms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/kxuhcx7m
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/d3ya39ms
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run sweet-sweep-100 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/d3ya39ms
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_002029-d3ya39ms/logs
wandb: ERROR Run d3ya39ms errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.71 GiB is allocated by PyTorch, and 119.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
wandb: Currently logged in as: vinakhiem120 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py:304: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="./configs", config_name="main")
/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. learning_rate uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
Create sweep with ID: xefe9qf1
Sweep URL: https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
[2025-06-03 01:27:19,481][wandb.agents.pyagent][INFO] - Starting sweep agent: entity=None, project=None, count=100
wandb: Agent Starting Run: 55m39ab3 with config:
wandb: 	Fdropout_rate: 0.1068133418652782
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.12956923593358266
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 16
wandb: 	learning_rate: 0.02027630128693605
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_012720-55m39ab3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/55m39ab3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
Training started...
Epoch 0 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:27:27,247][__main__][INFO] - Train Epoch: 0 [0/101 (0%)]	Loss: 1.729741
Epoch 0 [Train]:   1%|          | 1/101 [00:00<00:58,  1.72it/s]Epoch 0 [Train]:   2%|‚ñè         | 2/101 [00:00<00:33,  2.94it/s]Epoch 0 [Train]:   3%|‚ñé         | 3/101 [00:00<00:25,  3.86it/s]Epoch 0 [Train]:   4%|‚ñç         | 4/101 [00:01<00:22,  4.39it/s]Epoch 0 [Train]:   5%|‚ñç         | 5/101 [00:01<00:19,  4.85it/s]Epoch 0 [Train]:   6%|‚ñå         | 6/101 [00:01<00:18,  5.21it/s]Epoch 0 [Train]:   7%|‚ñã         | 7/101 [00:01<00:17,  5.47it/s]Epoch 0 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.63it/s]Epoch 0 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.75it/s]Epoch 0 [Train]:  10%|‚ñâ         | 10/101 [00:02<00:15,  5.81it/s]Epoch 0 [Train]:  11%|‚ñà         | 11/101 [00:02<00:15,  5.89it/s]Epoch 0 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.90it/s]Epoch 0 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.95it/s]Epoch 0 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.96it/s]Epoch 0 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.99it/s]Epoch 0 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:03<00:14,  6.00it/s]Epoch 0 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:03<00:14,  5.86it/s]Epoch 0 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.92it/s]Epoch 0 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.96it/s]Epoch 0 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.98it/s]Epoch 0 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.98it/s]Epoch 0 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:04<00:13,  5.98it/s]Epoch 0 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.98it/s]Epoch 0 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:12,  5.97it/s]Epoch 0 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.83it/s]Epoch 0 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.88it/s]Epoch 0 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.92it/s]Epoch 0 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:05<00:12,  5.94it/s]Epoch 0 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.96it/s]Epoch 0 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:11,  5.99it/s]Epoch 0 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.92it/s]Epoch 0 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.94it/s]Epoch 0 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.97it/s]Epoch 0 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:06<00:11,  6.00it/s]Epoch 0 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:10,  6.00it/s]Epoch 0 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:10,  6.00it/s]Epoch 0 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.98it/s]Epoch 0 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.99it/s]Epoch 0 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  6.00it/s]Epoch 0 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.94it/s]Epoch 0 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.89it/s]Epoch 0 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.54it/s]Epoch 0 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.58it/s]Epoch 0 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.71it/s]Epoch 0 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:08<00:09,  5.79it/s]Epoch 0 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.86it/s]Epoch 0 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.73it/s]Epoch 0 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.76it/s]Epoch 0 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.78it/s]Epoch 0 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.72it/s]Epoch 0 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:09<00:08,  5.81it/s]Epoch 0 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.87it/s]Epoch 0 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.80it/s]Epoch 0 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.34it/s]Epoch 0 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.32it/s]Epoch 0 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:08,  5.53it/s]Epoch 0 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:10<00:07,  5.66it/s]Epoch 0 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.74it/s]Epoch 0 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.82it/s]Epoch 0 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.88it/s]Epoch 0 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.91it/s]Epoch 0 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.91it/s]Epoch 0 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.94it/s]Epoch 0 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.96it/s]Epoch 0 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.79it/s]Epoch 0 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.69it/s]Epoch 0 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.79it/s]Epoch 0 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:12<00:05,  5.84it/s]Epoch 0 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.89it/s]Epoch 0 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.93it/s]Epoch 0 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.93it/s]Epoch 0 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.95it/s]Epoch 0 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.95it/s]Epoch 0 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:13<00:04,  5.97it/s]Epoch 0 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.96it/s]Epoch 0 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.96it/s]Epoch 0 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.95it/s]Epoch 0 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.97it/s]Epoch 0 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.97it/s]Epoch 0 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.76it/s]Epoch 0 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.67it/s]Epoch 0 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.67it/s]Epoch 0 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.33it/s]Epoch 0 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:03,  5.48it/s]Epoch 0 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.63it/s]Epoch 0 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.73it/s]Epoch 0 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.82it/s]Epoch 0 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.87it/s]Epoch 0 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.91it/s]Epoch 0 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.90it/s]Epoch 0 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.93it/s]Epoch 0 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.96it/s]Epoch 0 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.97it/s]Epoch 0 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.98it/s]Epoch 0 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.99it/s]Epoch 0 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  6.00it/s]Epoch 0 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  6.01it/s]Epoch 0 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  6.02it/s]Epoch 0 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.98it/s]Epoch 0 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.79it/s][2025-06-03 01:27:44,265][__main__][INFO] - Train Epoch: 0 [100/101 (99%)]	Loss: 0.361302
Epoch 0 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.21it/s]                                                                  Epoch 0 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 0 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.47it/s]Epoch 0 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.81it/s]Epoch 0 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.31it/s]Epoch 0 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.28it/s]Epoch 0 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 16.94it/s]Epoch 0 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.07it/s]Epoch 0 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.13it/s]Epoch 0 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.33it/s]Epoch 0 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.16it/s]Epoch 0 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.14it/s]Epoch 0 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.17it/s]Epoch 0 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.12it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:27:45,759][__main__][INFO] - Epoch 0: Val Loss: 1.1709, Accuracy: 0.6040, Metrics: {'accuracy': 0.6039603960396039, 'f1_macro': 0.30230769230769233, 'f1_weighted': 0.5364813404417365, 'precision_macro': 0.2677230046948357, 'recall_macro': 0.349364406779661}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee23041cc0>
<numpy.flatiter object at 0x7fee23041cc0>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee230506c0>
<numpy.flatiter object at 0x7fee230506c0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee230506c0>
<numpy.flatiter object at 0x7fee230506c0>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee23050cc0>
<numpy.flatiter object at 0x7fee23050cc0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23050cc0>
<numpy.flatiter object at 0x7fee23050cc0>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:27:45,865][__main__][INFO] - Saved best model at epoch 0 with accuracy: 0.6040
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:27:46,032][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
[2025-06-03 01:27:46,566][__main__][INFO] - Saved model at epoch 0
Epoch 1 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:27:46,798][__main__][INFO] - Train Epoch: 1 [0/101 (0%)]	Loss: 0.594575
Epoch 1 [Train]:   1%|          | 1/101 [00:00<00:20,  4.90it/s]Epoch 1 [Train]:   2%|‚ñè         | 2/101 [00:00<00:18,  5.39it/s]Epoch 1 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.57it/s]Epoch 1 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.67it/s]Epoch 1 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.75it/s]Epoch 1 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.79it/s]Epoch 1 [Train]:   7%|‚ñã         | 7/101 [00:01<00:15,  5.90it/s]Epoch 1 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.92it/s]Epoch 1 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.87it/s]Epoch 1 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:16,  5.53it/s]Epoch 1 [Train]:  11%|‚ñà         | 11/101 [00:01<00:16,  5.60it/s]Epoch 1 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.58it/s]Epoch 1 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.68it/s]Epoch 1 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.77it/s]Epoch 1 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.69it/s]Epoch 1 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.78it/s]Epoch 1 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.82it/s]Epoch 1 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.85it/s]Epoch 1 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.62it/s]Epoch 1 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.68it/s]Epoch 1 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.76it/s]Epoch 1 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:15,  5.02it/s]Epoch 1 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:15,  5.17it/s]Epoch 1 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:14,  5.17it/s]Epoch 1 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:14,  5.38it/s]Epoch 1 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.55it/s]Epoch 1 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:13,  5.68it/s]Epoch 1 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.77it/s]Epoch 1 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.82it/s]Epoch 1 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.87it/s]Epoch 1 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.90it/s]Epoch 1 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.89it/s]Epoch 1 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.90it/s]Epoch 1 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.85it/s]Epoch 1 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.88it/s]Epoch 1 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:10,  5.91it/s]Epoch 1 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.94it/s]Epoch 1 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.96it/s]Epoch 1 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.94it/s]Epoch 1 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.96it/s]Epoch 1 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.97it/s]Epoch 1 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:09,  5.98it/s]Epoch 1 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.94it/s]Epoch 1 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.96it/s]Epoch 1 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.97it/s]Epoch 1 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.98it/s]Epoch 1 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.97it/s]Epoch 1 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.97it/s]Epoch 1 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.98it/s]Epoch 1 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.98it/s]Epoch 1 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.97it/s]Epoch 1 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.96it/s]Epoch 1 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.97it/s]Epoch 1 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.96it/s]Epoch 1 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.71it/s]Epoch 1 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 1 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.81it/s]Epoch 1 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:08,  5.33it/s]Epoch 1 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.44it/s]Epoch 1 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.51it/s]Epoch 1 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.53it/s]Epoch 1 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.66it/s]Epoch 1 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.79it/s]Epoch 1 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.85it/s]Epoch 1 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.89it/s]Epoch 1 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.92it/s]Epoch 1 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.94it/s]Epoch 1 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.97it/s]Epoch 1 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.99it/s]Epoch 1 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.94it/s]Epoch 1 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.90it/s]Epoch 1 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.80it/s]Epoch 1 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.87it/s]Epoch 1 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.88it/s]Epoch 1 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.80it/s]Epoch 1 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.79it/s]Epoch 1 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.80it/s]Epoch 1 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.84it/s]Epoch 1 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.89it/s]Epoch 1 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.93it/s]Epoch 1 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.92it/s]Epoch 1 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.94it/s]Epoch 1 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.92it/s]Epoch 1 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.95it/s]Epoch 1 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.92it/s]Epoch 1 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.94it/s]Epoch 1 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.94it/s]Epoch 1 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.92it/s]Epoch 1 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.93it/s]Epoch 1 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.96it/s]Epoch 1 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.94it/s]Epoch 1 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.93it/s]Epoch 1 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.81it/s]Epoch 1 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.87it/s]Epoch 1 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.91it/s]Epoch 1 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.54it/s]Epoch 1 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.63it/s]Epoch 1 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.72it/s]Epoch 1 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.54it/s]Epoch 1 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.68it/s][2025-06-03 01:28:04,009][__main__][INFO] - Train Epoch: 1 [100/101 (99%)]	Loss: 0.263048
Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.14it/s]                                                                  Epoch 1 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 1 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.97it/s]Epoch 1 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.94it/s]Epoch 1 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.97it/s]Epoch 1 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.92it/s]Epoch 1 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.93it/s]Epoch 1 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.82it/s]Epoch 1 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.64it/s]Epoch 1 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 16.98it/s]Epoch 1 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.28it/s]Epoch 1 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.41it/s]Epoch 1 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.56it/s]Epoch 1 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.63it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:28:05,464][__main__][INFO] - Epoch 1: Val Loss: 0.8569, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.5123048668503214, 'f1_weighted': 0.6280919000991009, 'precision_macro': 0.47405529953917047, 'recall_macro': 0.5596879815100154}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee2331a250>
<numpy.flatiter object at 0x7fee2331a250>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee23299480>
<numpy.flatiter object at 0x7fee23299480>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23299480>
<numpy.flatiter object at 0x7fee23299480>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee232b9100>
<numpy.flatiter object at 0x7fee232b9100>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee232b9100>
<numpy.flatiter object at 0x7fee232b9100>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:28:05,547][__main__][INFO] - Saved best model at epoch 1 with accuracy: 0.6634
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:28:05,689][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 2 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:28:05,900][__main__][INFO] - Train Epoch: 2 [0/101 (0%)]	Loss: 0.113718
Epoch 2 [Train]:   1%|          | 1/101 [00:00<00:17,  5.62it/s]Epoch 2 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.84it/s]Epoch 2 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.91it/s]Epoch 2 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.95it/s]Epoch 2 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.66it/s]Epoch 2 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.77it/s]Epoch 2 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.85it/s]Epoch 2 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.87it/s]Epoch 2 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.58it/s]Epoch 2 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:17,  5.13it/s]Epoch 2 [Train]:  11%|‚ñà         | 11/101 [00:02<00:17,  5.07it/s]Epoch 2 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:16,  5.28it/s]Epoch 2 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:16,  5.47it/s]Epoch 2 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.56it/s]Epoch 2 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.43it/s]Epoch 2 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:15,  5.54it/s]Epoch 2 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:03<00:14,  5.67it/s]Epoch 2 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.71it/s]Epoch 2 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.80it/s]Epoch 2 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.86it/s]Epoch 2 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.91it/s]Epoch 2 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.93it/s]Epoch 2 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.95it/s]Epoch 2 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:12,  5.93it/s]Epoch 2 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.93it/s]Epoch 2 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.94it/s]Epoch 2 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.96it/s]Epoch 2 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.96it/s]Epoch 2 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.95it/s]Epoch 2 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.84it/s]Epoch 2 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.61it/s]Epoch 2 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.73it/s]Epoch 2 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.75it/s]Epoch 2 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.81it/s]Epoch 2 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.76it/s]Epoch 2 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.71it/s]Epoch 2 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.79it/s]Epoch 2 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.84it/s]Epoch 2 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.89it/s]Epoch 2 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.91it/s]Epoch 2 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.91it/s]Epoch 2 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:09,  5.93it/s]Epoch 2 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 2 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.79it/s]Epoch 2 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.84it/s]Epoch 2 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.89it/s]Epoch 2 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.90it/s]Epoch 2 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.92it/s]Epoch 2 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.94it/s]Epoch 2 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.90it/s]Epoch 2 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.89it/s]Epoch 2 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.91it/s]Epoch 2 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.89it/s]Epoch 2 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.90it/s]Epoch 2 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.92it/s]Epoch 2 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.94it/s]Epoch 2 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.93it/s]Epoch 2 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.87it/s]Epoch 2 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.68it/s]Epoch 2 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.77it/s]Epoch 2 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.83it/s]Epoch 2 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.86it/s]Epoch 2 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.67it/s]Epoch 2 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.61it/s]Epoch 2 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.47it/s]Epoch 2 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.43it/s]Epoch 2 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:06,  5.58it/s]Epoch 2 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.65it/s]Epoch 2 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.64it/s]Epoch 2 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.45it/s]Epoch 2 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.26it/s]Epoch 2 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.39it/s]Epoch 2 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:05,  5.38it/s]Epoch 2 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:13<00:05,  4.91it/s]Epoch 2 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:05,  5.09it/s]Epoch 2 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.22it/s]Epoch 2 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.17it/s]Epoch 2 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.21it/s]Epoch 2 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:04,  5.40it/s]Epoch 2 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.56it/s]Epoch 2 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.68it/s]Epoch 2 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.76it/s]Epoch 2 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.63it/s]Epoch 2 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.73it/s]Epoch 2 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.80it/s]Epoch 2 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.81it/s]Epoch 2 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.87it/s]Epoch 2 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.67it/s]Epoch 2 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.72it/s]Epoch 2 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.78it/s]Epoch 2 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.82it/s]Epoch 2 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.77it/s]Epoch 2 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.64it/s]Epoch 2 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.73it/s]Epoch 2 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.77it/s]Epoch 2 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.78it/s]Epoch 2 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:17<00:00,  5.83it/s]Epoch 2 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.87it/s]Epoch 2 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.87it/s]Epoch 2 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.88it/s][2025-06-03 01:28:23,404][__main__][INFO] - Train Epoch: 2 [100/101 (99%)]	Loss: 0.201627
Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.06it/s]                                                                  Epoch 2 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 2 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.54it/s]Epoch 2 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 16.13it/s]Epoch 2 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 15.49it/s]Epoch 2 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 15.80it/s]Epoch 2 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:01, 14.44it/s]Epoch 2 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:01, 13.69it/s]Epoch 2 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:01<00:00, 12.25it/s]Epoch 2 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:01<00:00, 12.41it/s]Epoch 2 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 13.73it/s]Epoch 2 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 14.71it/s]Epoch 2 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 13.48it/s]Epoch 2 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 14.39it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:28:25,197][__main__][INFO] - Epoch 2: Val Loss: 0.7123, Accuracy: 0.7327, Metrics: {'accuracy': 0.7326732673267327, 'f1_macro': 0.5402665823718455, 'f1_weighted': 0.6826267643359879, 'precision_macro': 0.5647252321981424, 'recall_macro': 0.5338790446841294}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee23378f90>
<numpy.flatiter object at 0x7fee23378f90>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee233a2300>
<numpy.flatiter object at 0x7fee233a2300>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee233a2300>
<numpy.flatiter object at 0x7fee233a2300>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee233a2900>
<numpy.flatiter object at 0x7fee233a2900>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee233a2900>
<numpy.flatiter object at 0x7fee233a2900>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:28:25,270][__main__][INFO] - Saved best model at epoch 2 with accuracy: 0.7327
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:28:25,411][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 3 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:28:25,616][__main__][INFO] - Train Epoch: 3 [0/101 (0%)]	Loss: 0.456718
Epoch 3 [Train]:   1%|          | 1/101 [00:00<00:16,  5.92it/s]Epoch 3 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.95it/s]Epoch 3 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.65it/s]Epoch 3 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.75it/s]Epoch 3 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.68it/s]Epoch 3 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.67it/s]Epoch 3 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.75it/s]Epoch 3 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.77it/s]Epoch 3 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.81it/s]Epoch 3 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.83it/s]Epoch 3 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.81it/s]Epoch 3 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.84it/s]Epoch 3 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.87it/s]Epoch 3 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.87it/s]Epoch 3 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.60it/s]Epoch 3 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:16,  5.13it/s]Epoch 3 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:03<00:15,  5.36it/s]Epoch 3 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:15,  5.51it/s]Epoch 3 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.63it/s]Epoch 3 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.73it/s]Epoch 3 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.81it/s]Epoch 3 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.86it/s]Epoch 3 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.88it/s]Epoch 3 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.91it/s]Epoch 3 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.94it/s]Epoch 3 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.95it/s]Epoch 3 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.95it/s]Epoch 3 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.96it/s]Epoch 3 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.97it/s]Epoch 3 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:11,  5.97it/s]Epoch 3 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  6.01it/s]Epoch 3 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.98it/s]Epoch 3 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.98it/s]Epoch 3 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.98it/s]Epoch 3 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.97it/s]Epoch 3 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:10,  5.95it/s]Epoch 3 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.95it/s]Epoch 3 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.95it/s]Epoch 3 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.94it/s]Epoch 3 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.95it/s]Epoch 3 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.96it/s]Epoch 3 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:09,  5.96it/s]Epoch 3 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.95it/s]Epoch 3 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.95it/s]Epoch 3 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.91it/s]Epoch 3 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.90it/s]Epoch 3 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.92it/s]Epoch 3 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.93it/s]Epoch 3 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.94it/s]Epoch 3 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.94it/s]Epoch 3 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.95it/s]Epoch 3 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.96it/s]Epoch 3 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.95it/s]Epoch 3 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.94it/s]Epoch 3 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.94it/s]Epoch 3 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.93it/s]Epoch 3 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.94it/s]Epoch 3 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.94it/s]Epoch 3 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.92it/s]Epoch 3 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.93it/s]Epoch 3 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.94it/s]Epoch 3 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.94it/s]Epoch 3 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.94it/s]Epoch 3 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.94it/s]Epoch 3 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.94it/s]Epoch 3 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.93it/s]Epoch 3 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.94it/s]Epoch 3 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.94it/s]Epoch 3 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.93it/s]Epoch 3 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.94it/s]Epoch 3 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.95it/s]Epoch 3 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.93it/s]Epoch 3 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.93it/s]Epoch 3 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.94it/s]Epoch 3 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.91it/s]Epoch 3 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.91it/s]Epoch 3 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.93it/s]Epoch 3 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.93it/s]Epoch 3 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.93it/s]Epoch 3 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.94it/s]Epoch 3 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.95it/s]Epoch 3 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.94it/s]Epoch 3 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.94it/s]Epoch 3 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.95it/s]Epoch 3 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.94it/s]Epoch 3 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.93it/s]Epoch 3 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.93it/s]Epoch 3 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:14<00:02,  5.93it/s]Epoch 3 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.93it/s]Epoch 3 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.94it/s]Epoch 3 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.95it/s]Epoch 3 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.94it/s]Epoch 3 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.95it/s]Epoch 3 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:15<00:01,  5.96it/s]Epoch 3 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.96it/s]Epoch 3 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.95it/s]Epoch 3 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.96it/s]Epoch 3 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.96it/s]Epoch 3 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.94it/s]Epoch 3 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:16<00:00,  5.95it/s][2025-06-03 01:28:42,551][__main__][INFO] - Train Epoch: 3 [100/101 (99%)]	Loss: 0.717469
Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.34it/s]                                                                  Epoch 3 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 3 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 18.09it/s]Epoch 3 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 18.02it/s]Epoch 3 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.98it/s]Epoch 3 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.96it/s]Epoch 3 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.94it/s]Epoch 3 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.94it/s]Epoch 3 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.93it/s]Epoch 3 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.94it/s]Epoch 3 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.94it/s]Epoch 3 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.88it/s]Epoch 3 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.56it/s]Epoch 3 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.51it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:28:43,993][__main__][INFO] - Epoch 3: Val Loss: 0.8021, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.43018489447060876, 'f1_weighted': 0.5761416016012905, 'precision_macro': 0.4981060606060606, 'recall_macro': 0.4298536209553159}
Epoch 4 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:28:44,165][__main__][INFO] - Train Epoch: 4 [0/101 (0%)]	Loss: 0.486422
Epoch 4 [Train]:   1%|          | 1/101 [00:00<00:17,  5.88it/s]Epoch 4 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.90it/s]Epoch 4 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.90it/s]Epoch 4 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.91it/s]Epoch 4 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.91it/s]Epoch 4 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.91it/s]Epoch 4 [Train]:   7%|‚ñã         | 7/101 [00:01<00:15,  5.91it/s]Epoch 4 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.85it/s]Epoch 4 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.88it/s]Epoch 4 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.90it/s]Epoch 4 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.90it/s]Epoch 4 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.92it/s]Epoch 4 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.93it/s]Epoch 4 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.92it/s]Epoch 4 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.93it/s]Epoch 4 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.94it/s]Epoch 4 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.93it/s]Epoch 4 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:13,  5.94it/s]Epoch 4 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.95it/s]Epoch 4 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.94it/s]Epoch 4 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.94it/s]Epoch 4 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.94it/s]Epoch 4 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.93it/s]Epoch 4 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:12,  5.93it/s]Epoch 4 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.93it/s]Epoch 4 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.93it/s]Epoch 4 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.92it/s]Epoch 4 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.93it/s]Epoch 4 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.93it/s]Epoch 4 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:11,  5.92it/s]Epoch 4 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.91it/s]Epoch 4 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.91it/s]Epoch 4 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.90it/s]Epoch 4 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.92it/s]Epoch 4 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.91it/s]Epoch 4 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:10,  5.91it/s]Epoch 4 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.92it/s]Epoch 4 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.90it/s]Epoch 4 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.90it/s]Epoch 4 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.92it/s]Epoch 4 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:06<00:10,  5.91it/s]Epoch 4 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:09,  5.91it/s]Epoch 4 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.92it/s]Epoch 4 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.90it/s]Epoch 4 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.91it/s]Epoch 4 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.88it/s]Epoch 4 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:07<00:09,  5.87it/s]Epoch 4 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.89it/s]Epoch 4 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.88it/s]Epoch 4 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.89it/s]Epoch 4 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.91it/s]Epoch 4 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.91it/s]Epoch 4 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:08<00:08,  5.91it/s]Epoch 4 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.91it/s]Epoch 4 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.87it/s]Epoch 4 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.88it/s]Epoch 4 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.84it/s]Epoch 4 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.76it/s]Epoch 4 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.76it/s]Epoch 4 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.81it/s]Epoch 4 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.83it/s]Epoch 4 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.87it/s]Epoch 4 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.88it/s]Epoch 4 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.88it/s]Epoch 4 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.89it/s]Epoch 4 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.90it/s]Epoch 4 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.90it/s]Epoch 4 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.91it/s]Epoch 4 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.91it/s]Epoch 4 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.91it/s]Epoch 4 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.91it/s]Epoch 4 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.91it/s]Epoch 4 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.91it/s]Epoch 4 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.92it/s]Epoch 4 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.92it/s]Epoch 4 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.87it/s]Epoch 4 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.88it/s]Epoch 4 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.89it/s]Epoch 4 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.91it/s]Epoch 4 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.90it/s]Epoch 4 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.89it/s]Epoch 4 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.90it/s]Epoch 4 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.90it/s]Epoch 4 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.90it/s]Epoch 4 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.90it/s]Epoch 4 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.89it/s]Epoch 4 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.91it/s]Epoch 4 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:14<00:02,  5.91it/s]Epoch 4 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.90it/s]Epoch 4 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.87it/s]Epoch 4 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.89it/s]Epoch 4 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.89it/s]Epoch 4 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.90it/s]Epoch 4 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:15<00:01,  5.90it/s]Epoch 4 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.90it/s]Epoch 4 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.90it/s]Epoch 4 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.90it/s]Epoch 4 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.91it/s]Epoch 4 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.91it/s]Epoch 4 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:16<00:00,  5.90it/s][2025-06-03 01:29:01,077][__main__][INFO] - Train Epoch: 4 [100/101 (99%)]	Loss: 1.481073
Epoch 4 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.30it/s]                                                                  Epoch 4 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 4 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.94it/s]Epoch 4 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.92it/s]Epoch 4 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.86it/s]Epoch 4 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.84it/s]Epoch 4 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.82it/s]Epoch 4 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.82it/s]Epoch 4 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.81it/s]Epoch 4 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.82it/s]Epoch 4 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.84it/s]Epoch 4 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.81it/s]Epoch 4 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.83it/s]Epoch 4 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.81it/s]                                                              [2025-06-03 01:29:02,516][__main__][INFO] - Epoch 4: Val Loss: 0.7542, Accuracy: 0.7327, Metrics: {'accuracy': 0.7326732673267327, 'f1_macro': 0.5890128531349905, 'f1_weighted': 0.7018721249693965, 'precision_macro': 0.7125, 'recall_macro': 0.5606317411402156}
Epoch 5 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:29:02,688][__main__][INFO] - Train Epoch: 5 [0/101 (0%)]	Loss: 0.250121
Epoch 5 [Train]:   1%|          | 1/101 [00:00<00:16,  5.91it/s]Epoch 5 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.75it/s]Epoch 5 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.83it/s]Epoch 5 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.86it/s]Epoch 5 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.87it/s]Epoch 5 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.89it/s]Epoch 5 [Train]:   7%|‚ñã         | 7/101 [00:01<00:15,  5.90it/s]Epoch 5 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.90it/s]Epoch 5 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.91it/s]Epoch 5 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.91it/s]Epoch 5 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.91it/s]Epoch 5 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.92it/s]Epoch 5 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.92it/s]Epoch 5 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.91it/s]Epoch 5 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.90it/s]Epoch 5 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.90it/s]Epoch 5 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.91it/s]Epoch 5 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.91it/s]Epoch 5 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.90it/s]Epoch 5 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.91it/s]Epoch 5 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.91it/s]Epoch 5 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.90it/s]Epoch 5 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.90it/s]Epoch 5 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.90it/s]Epoch 5 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.90it/s]Epoch 5 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.90it/s]Epoch 5 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.90it/s]Epoch 5 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.90it/s]Epoch 5 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.91it/s]Epoch 5 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.90it/s]Epoch 5 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.90it/s]Epoch 5 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.88it/s]Epoch 5 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.89it/s]Epoch 5 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.90it/s]Epoch 5 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.90it/s]Epoch 5 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.90it/s]Epoch 5 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.91it/s]Epoch 5 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.91it/s]Epoch 5 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.90it/s]Epoch 5 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.91it/s]Epoch 5 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:06<00:10,  5.90it/s]Epoch 5 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:09,  5.90it/s]Epoch 5 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.90it/s]Epoch 5 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.88it/s]Epoch 5 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.89it/s]Epoch 5 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.88it/s]Epoch 5 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:07<00:09,  5.88it/s]Epoch 5 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.89it/s]Epoch 5 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.88it/s]Epoch 5 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.90it/s]Epoch 5 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.89it/s]Epoch 5 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.88it/s]Epoch 5 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:08<00:08,  5.89it/s]Epoch 5 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.89it/s]Epoch 5 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.90it/s]Epoch 5 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.80it/s]Epoch 5 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.84it/s]Epoch 5 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.86it/s]Epoch 5 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.87it/s]Epoch 5 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.87it/s]Epoch 5 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.87it/s]Epoch 5 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.88it/s]Epoch 5 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.88it/s]Epoch 5 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.88it/s]Epoch 5 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.89it/s]Epoch 5 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.89it/s]Epoch 5 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.89it/s]Epoch 5 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.90it/s]Epoch 5 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.90it/s]Epoch 5 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.89it/s]Epoch 5 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.88it/s]Epoch 5 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.86it/s]Epoch 5 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.88it/s]Epoch 5 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.87it/s]Epoch 5 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.87it/s]Epoch 5 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.88it/s]Epoch 5 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.87it/s]Epoch 5 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.88it/s]Epoch 5 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.88it/s]Epoch 5 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.88it/s]Epoch 5 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.88it/s]Epoch 5 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.88it/s]Epoch 5 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.89it/s]Epoch 5 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.87it/s]Epoch 5 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.87it/s]Epoch 5 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.76it/s]Epoch 5 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.80it/s]Epoch 5 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:14<00:02,  5.82it/s]Epoch 5 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.81it/s]Epoch 5 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.82it/s]Epoch 5 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.83it/s]Epoch 5 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.84it/s]Epoch 5 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.85it/s]Epoch 5 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:15<00:01,  5.87it/s]Epoch 5 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.88it/s]Epoch 5 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.88it/s]Epoch 5 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.90it/s]Epoch 5 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.90it/s]Epoch 5 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.89it/s]Epoch 5 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:16<00:00,  5.91it/s][2025-06-03 01:29:19,649][__main__][INFO] - Train Epoch: 5 [100/101 (99%)]	Loss: 0.184682
Epoch 5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.30it/s]                                                                  Epoch 5 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 5 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.95it/s]Epoch 5 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.90it/s]Epoch 5 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.87it/s]Epoch 5 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.85it/s]Epoch 5 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.83it/s]Epoch 5 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.82it/s]Epoch 5 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.84it/s]Epoch 5 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.84it/s]Epoch 5 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.87it/s]Epoch 5 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.84it/s]Epoch 5 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.86it/s]Epoch 5 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.85it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:29:21,086][__main__][INFO] - Epoch 5: Val Loss: 0.8170, Accuracy: 0.7030, Metrics: {'accuracy': 0.7029702970297029, 'f1_macro': 0.5021276595744681, 'f1_weighted': 0.6386349273225195, 'precision_macro': 0.5460027100271002, 'recall_macro': 0.49834360554699536}
Epoch 6 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:29:21,257][__main__][INFO] - Train Epoch: 6 [0/101 (0%)]	Loss: 0.943914
Epoch 6 [Train]:   1%|          | 1/101 [00:00<00:16,  5.93it/s]Epoch 6 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.76it/s]Epoch 6 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.84it/s]Epoch 6 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.84it/s]Epoch 6 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 6 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.88it/s]Epoch 6 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.85it/s]Epoch 6 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.87it/s]Epoch 6 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.88it/s]Epoch 6 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.88it/s]Epoch 6 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.90it/s]Epoch 6 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.90it/s]Epoch 6 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.90it/s]Epoch 6 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.90it/s]Epoch 6 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.89it/s]Epoch 6 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.89it/s]Epoch 6 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.88it/s]Epoch 6 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.87it/s]Epoch 6 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.88it/s]Epoch 6 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.89it/s]Epoch 6 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.90it/s]Epoch 6 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.90it/s]Epoch 6 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.91it/s]Epoch 6 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.91it/s]Epoch 6 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.91it/s]Epoch 6 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.90it/s]Epoch 6 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.90it/s]Epoch 6 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.91it/s]Epoch 6 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.90it/s]Epoch 6 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.90it/s]Epoch 6 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.91it/s]Epoch 6 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.69it/s]Epoch 6 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.74it/s]Epoch 6 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.66it/s]Epoch 6 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.64it/s]Epoch 6 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.71it/s]Epoch 6 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.76it/s]Epoch 6 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.81it/s]Epoch 6 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.84it/s]Epoch 6 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.85it/s]Epoch 6 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.87it/s]Epoch 6 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.88it/s]Epoch 6 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.89it/s]Epoch 6 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.91it/s]Epoch 6 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.90it/s]Epoch 6 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.90it/s]Epoch 6 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.91it/s]Epoch 6 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:08,  5.91it/s]Epoch 6 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.90it/s]Epoch 6 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.91it/s]Epoch 6 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.90it/s]Epoch 6 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.91it/s]Epoch 6 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.91it/s]Epoch 6 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.90it/s]Epoch 6 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.90it/s]Epoch 6 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.89it/s]Epoch 6 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.87it/s]Epoch 6 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.89it/s]Epoch 6 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.88it/s]Epoch 6 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.87it/s]Epoch 6 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.88it/s]Epoch 6 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.87it/s]Epoch 6 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.89it/s]Epoch 6 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.90it/s]Epoch 6 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.89it/s]Epoch 6 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.90it/s]Epoch 6 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.90it/s]Epoch 6 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.90it/s]Epoch 6 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.90it/s]Epoch 6 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.89it/s]Epoch 6 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.90it/s]Epoch 6 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.91it/s]Epoch 6 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.90it/s]Epoch 6 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.91it/s]Epoch 6 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.91it/s]Epoch 6 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.89it/s]Epoch 6 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.91it/s]Epoch 6 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.80it/s]Epoch 6 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.73it/s]Epoch 6 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.79it/s]Epoch 6 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.82it/s]Epoch 6 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.84it/s]Epoch 6 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.87it/s]Epoch 6 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.86it/s]Epoch 6 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.87it/s]Epoch 6 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.88it/s]Epoch 6 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.87it/s]Epoch 6 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:14<00:02,  5.88it/s]Epoch 6 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.89it/s]Epoch 6 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.88it/s]Epoch 6 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.88it/s]Epoch 6 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.88it/s]Epoch 6 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.89it/s]Epoch 6 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.90it/s]Epoch 6 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.89it/s]Epoch 6 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.90it/s]Epoch 6 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.91it/s]Epoch 6 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.90it/s]Epoch 6 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.90it/s]Epoch 6 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.90it/s][2025-06-03 01:29:38,251][__main__][INFO] - Train Epoch: 6 [100/101 (99%)]	Loss: 0.116894
Epoch 6 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.16it/s]                                                                  Epoch 6 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 6 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.99it/s]Epoch 6 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.90it/s]Epoch 6 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.85it/s]Epoch 6 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.84it/s]Epoch 6 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.85it/s]Epoch 6 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.83it/s]Epoch 6 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.84it/s]Epoch 6 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.83it/s]Epoch 6 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.85it/s]Epoch 6 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.82it/s]Epoch 6 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.83it/s]Epoch 6 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.82it/s]                                                              [2025-06-03 01:29:39,703][__main__][INFO] - Epoch 6: Val Loss: 0.6885, Accuracy: 0.7723, Metrics: {'accuracy': 0.7722772277227723, 'f1_macro': 0.6594866071428571, 'f1_weighted': 0.7536377298444131, 'precision_macro': 0.806219806763285, 'recall_macro': 0.6228235747303543}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee23414600>
<numpy.flatiter object at 0x7fee23414600>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee23419600>
<numpy.flatiter object at 0x7fee23419600>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23419600>
<numpy.flatiter object at 0x7fee23419600>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee23419c00>
<numpy.flatiter object at 0x7fee23419c00>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23419c00>
<numpy.flatiter object at 0x7fee23419c00>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:29:39,772][__main__][INFO] - Saved best model at epoch 6 with accuracy: 0.7723
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:29:39,911][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 7 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:29:40,121][__main__][INFO] - Train Epoch: 7 [0/101 (0%)]	Loss: 0.661052
Epoch 7 [Train]:   1%|          | 1/101 [00:00<00:17,  5.87it/s]Epoch 7 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.88it/s]Epoch 7 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.89it/s]Epoch 7 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.85it/s]Epoch 7 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 7 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.87it/s]Epoch 7 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.87it/s]Epoch 7 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.88it/s]Epoch 7 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.87it/s]Epoch 7 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.88it/s]Epoch 7 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.88it/s]Epoch 7 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.88it/s]Epoch 7 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.89it/s]Epoch 7 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.88it/s]Epoch 7 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.89it/s]Epoch 7 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.89it/s]Epoch 7 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.88it/s]Epoch 7 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.89it/s]Epoch 7 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.88it/s]Epoch 7 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.88it/s]Epoch 7 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.88it/s]Epoch 7 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.88it/s]Epoch 7 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.90it/s]Epoch 7 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.88it/s]Epoch 7 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.88it/s]Epoch 7 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.89it/s]Epoch 7 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.86it/s]Epoch 7 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.87it/s]Epoch 7 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.87it/s]Epoch 7 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.87it/s]Epoch 7 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.89it/s]Epoch 7 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.89it/s]Epoch 7 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.89it/s]Epoch 7 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.89it/s]Epoch 7 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.88it/s]Epoch 7 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.89it/s]Epoch 7 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.90it/s]Epoch 7 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.89it/s]Epoch 7 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.90it/s]Epoch 7 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.90it/s]Epoch 7 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:06<00:10,  5.89it/s]Epoch 7 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.90it/s]Epoch 7 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.90it/s]Epoch 7 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.90it/s]Epoch 7 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.90it/s]Epoch 7 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.89it/s]Epoch 7 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:07<00:09,  5.89it/s]Epoch 7 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.88it/s]Epoch 7 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.87it/s]Epoch 7 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.88it/s]Epoch 7 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.88it/s]Epoch 7 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.89it/s]Epoch 7 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.89it/s]Epoch 7 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:07,  5.88it/s]Epoch 7 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.89it/s]Epoch 7 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.88it/s]Epoch 7 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.89it/s]Epoch 7 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.88it/s]Epoch 7 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.88it/s]Epoch 7 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.89it/s]Epoch 7 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.89it/s]Epoch 7 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.89it/s]Epoch 7 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.88it/s]Epoch 7 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.87it/s]Epoch 7 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.89it/s]Epoch 7 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.89it/s]Epoch 7 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.88it/s]Epoch 7 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.88it/s]Epoch 7 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.88it/s]Epoch 7 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.88it/s]Epoch 7 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.87it/s]Epoch 7 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.87it/s]Epoch 7 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.85it/s]Epoch 7 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.84it/s]Epoch 7 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.86it/s]Epoch 7 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.86it/s]Epoch 7 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.84it/s]Epoch 7 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.85it/s]Epoch 7 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.85it/s]Epoch 7 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.86it/s]Epoch 7 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.79it/s]Epoch 7 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.81it/s]Epoch 7 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.82it/s]Epoch 7 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.84it/s]Epoch 7 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.84it/s]Epoch 7 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.86it/s]Epoch 7 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.87it/s]Epoch 7 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:14<00:02,  5.86it/s]Epoch 7 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.87it/s]Epoch 7 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.86it/s]Epoch 7 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.87it/s]Epoch 7 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.87it/s]Epoch 7 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.86it/s]Epoch 7 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:15<00:01,  5.87it/s]Epoch 7 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.86it/s]Epoch 7 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.87it/s]Epoch 7 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.88it/s]Epoch 7 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.87it/s]Epoch 7 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.87it/s]Epoch 7 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.86it/s][2025-06-03 01:29:57,108][__main__][INFO] - Train Epoch: 7 [100/101 (99%)]	Loss: 0.537551
Epoch 7 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.26it/s]                                                                  Epoch 7 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 7 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.65it/s]Epoch 7 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.70it/s]Epoch 7 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.72it/s]Epoch 7 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.75it/s]Epoch 7 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.75it/s]Epoch 7 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.76it/s]Epoch 7 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.76it/s]Epoch 7 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.40it/s]Epoch 7 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.43it/s]Epoch 7 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.50it/s]Epoch 7 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.58it/s]Epoch 7 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.64it/s]                                                              [2025-06-03 01:29:58,563][__main__][INFO] - Epoch 7: Val Loss: 0.7976, Accuracy: 0.6931, Metrics: {'accuracy': 0.693069306930693, 'f1_macro': 0.5436853002070393, 'f1_weighted': 0.671943094930611, 'precision_macro': 0.6902985074626866, 'recall_macro': 0.5149653312788907}
Epoch 8 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:29:58,735][__main__][INFO] - Train Epoch: 8 [0/101 (0%)]	Loss: 0.309225
Epoch 8 [Train]:   1%|          | 1/101 [00:00<00:17,  5.88it/s]Epoch 8 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.89it/s]Epoch 8 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.89it/s]Epoch 8 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.80it/s]Epoch 8 [Train]:   5%|‚ñç         | 5/101 [00:00<00:17,  5.62it/s]Epoch 8 [Train]:   6%|‚ñå         | 6/101 [00:01<00:17,  5.53it/s]Epoch 8 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.64it/s]Epoch 8 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.65it/s]Epoch 8 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.72it/s]Epoch 8 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.77it/s]Epoch 8 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.79it/s]Epoch 8 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.83it/s]Epoch 8 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.84it/s]Epoch 8 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.85it/s]Epoch 8 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.85it/s]Epoch 8 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.85it/s]Epoch 8 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.87it/s]Epoch 8 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.87it/s]Epoch 8 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.87it/s]Epoch 8 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.86it/s]Epoch 8 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.85it/s]Epoch 8 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.84it/s]Epoch 8 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.85it/s]Epoch 8 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.86it/s]Epoch 8 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.86it/s]Epoch 8 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.86it/s]Epoch 8 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.85it/s]Epoch 8 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.85it/s]Epoch 8 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.87it/s]Epoch 8 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.63it/s]Epoch 8 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.70it/s]Epoch 8 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.68it/s]Epoch 8 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.74it/s]Epoch 8 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.78it/s]Epoch 8 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.81it/s]Epoch 8 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.84it/s]Epoch 8 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.84it/s]Epoch 8 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.86it/s]Epoch 8 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.86it/s]Epoch 8 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.86it/s]Epoch 8 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.88it/s]Epoch 8 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.86it/s]Epoch 8 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.84it/s]Epoch 8 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.84it/s]Epoch 8 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.86it/s]Epoch 8 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.87it/s]Epoch 8 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.87it/s]Epoch 8 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.87it/s]Epoch 8 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.87it/s]Epoch 8 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.88it/s]Epoch 8 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.88it/s]Epoch 8 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.88it/s]Epoch 8 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.89it/s]Epoch 8 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.87it/s]Epoch 8 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.88it/s]Epoch 8 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.88it/s]Epoch 8 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.86it/s]Epoch 8 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.82it/s]Epoch 8 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.83it/s]Epoch 8 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.85it/s]Epoch 8 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.85it/s]Epoch 8 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.86it/s]Epoch 8 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.87it/s]Epoch 8 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.86it/s]Epoch 8 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.87it/s]Epoch 8 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.87it/s]Epoch 8 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.87it/s]Epoch 8 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.86it/s]Epoch 8 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.86it/s]Epoch 8 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.87it/s]Epoch 8 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.86it/s]Epoch 8 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.87it/s]Epoch 8 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.87it/s]Epoch 8 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.86it/s]Epoch 8 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.87it/s]Epoch 8 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.85it/s]Epoch 8 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.86it/s]Epoch 8 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.87it/s]Epoch 8 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.86it/s]Epoch 8 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.87it/s]Epoch 8 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.86it/s]Epoch 8 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.88it/s]Epoch 8 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.88it/s]Epoch 8 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.88it/s]Epoch 8 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.87it/s]Epoch 8 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.86it/s]Epoch 8 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.87it/s]Epoch 8 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.86it/s]Epoch 8 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.86it/s]Epoch 8 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.86it/s]Epoch 8 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.85it/s]Epoch 8 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.86it/s]Epoch 8 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.85it/s]Epoch 8 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.85it/s]Epoch 8 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.85it/s]Epoch 8 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.86it/s]Epoch 8 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.87it/s]Epoch 8 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.87it/s]Epoch 8 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.88it/s]Epoch 8 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.88it/s][2025-06-03 01:30:15,822][__main__][INFO] - Train Epoch: 8 [100/101 (99%)]	Loss: 0.758182
Epoch 8 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.27it/s]                                                                  Epoch 8 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 8 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.65it/s]Epoch 8 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.76it/s]Epoch 8 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.77it/s]Epoch 8 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.79it/s]Epoch 8 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.77it/s]Epoch 8 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.80it/s]Epoch 8 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.79it/s]Epoch 8 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.80it/s]Epoch 8 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.80it/s]Epoch 8 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.79it/s]Epoch 8 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.79it/s]Epoch 8 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.78it/s]                                                              [2025-06-03 01:30:17,265][__main__][INFO] - Epoch 8: Val Loss: 0.8030, Accuracy: 0.7327, Metrics: {'accuracy': 0.7326732673267327, 'f1_macro': 0.6320664629488159, 'f1_weighted': 0.7087490261631205, 'precision_macro': 0.7015981735159816, 'recall_macro': 0.6015408320493066}
Epoch 9 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:30:17,438][__main__][INFO] - Train Epoch: 9 [0/101 (0%)]	Loss: 0.353699
Epoch 9 [Train]:   1%|          | 1/101 [00:00<00:17,  5.84it/s]Epoch 9 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.86it/s]Epoch 9 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.85it/s]Epoch 9 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.87it/s]Epoch 9 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 9 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.85it/s]Epoch 9 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.86it/s]Epoch 9 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.86it/s]Epoch 9 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.88it/s]Epoch 9 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.87it/s]Epoch 9 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.87it/s]Epoch 9 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.87it/s]Epoch 9 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:14,  5.87it/s]Epoch 9 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.88it/s]Epoch 9 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.87it/s]Epoch 9 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.87it/s]Epoch 9 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.87it/s]Epoch 9 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.86it/s]Epoch 9 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.88it/s]Epoch 9 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.87it/s]Epoch 9 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.88it/s]Epoch 9 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.88it/s]Epoch 9 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.86it/s]Epoch 9 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.87it/s]Epoch 9 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.86it/s]Epoch 9 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.86it/s]Epoch 9 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.86it/s]Epoch 9 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.86it/s]Epoch 9 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.87it/s]Epoch 9 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.86it/s]Epoch 9 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.87it/s]Epoch 9 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.87it/s]Epoch 9 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.87it/s]Epoch 9 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.89it/s]Epoch 9 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.84it/s]Epoch 9 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.85it/s]Epoch 9 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.86it/s]Epoch 9 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.87it/s]Epoch 9 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.87it/s]Epoch 9 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.87it/s]Epoch 9 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:06<00:10,  5.88it/s]Epoch 9 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.88it/s]Epoch 9 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.86it/s]Epoch 9 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.87it/s]Epoch 9 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.87it/s]Epoch 9 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.87it/s]Epoch 9 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.88it/s]Epoch 9 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.88it/s]Epoch 9 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.88it/s]Epoch 9 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.87it/s]Epoch 9 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.88it/s]Epoch 9 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.86it/s]Epoch 9 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.87it/s]Epoch 9 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.86it/s]Epoch 9 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.86it/s]Epoch 9 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.88it/s]Epoch 9 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.87it/s]Epoch 9 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.87it/s]Epoch 9 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.87it/s]Epoch 9 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:06,  5.86it/s]Epoch 9 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.85it/s]Epoch 9 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.84it/s]Epoch 9 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.86it/s]Epoch 9 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.87it/s]Epoch 9 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.87it/s]Epoch 9 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.87it/s]Epoch 9 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.87it/s]Epoch 9 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.88it/s]Epoch 9 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.87it/s]Epoch 9 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.86it/s]Epoch 9 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.86it/s]Epoch 9 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.85it/s]Epoch 9 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.86it/s]Epoch 9 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.85it/s]Epoch 9 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.86it/s]Epoch 9 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.85it/s]Epoch 9 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.86it/s]Epoch 9 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.86it/s]Epoch 9 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.86it/s]Epoch 9 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.87it/s]Epoch 9 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.85it/s]Epoch 9 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:13<00:03,  5.86it/s]Epoch 9 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.87it/s]Epoch 9 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.86it/s]Epoch 9 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.86it/s]Epoch 9 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.86it/s]Epoch 9 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.88it/s]Epoch 9 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.88it/s]Epoch 9 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.83it/s]Epoch 9 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.76it/s]Epoch 9 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.76it/s]Epoch 9 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.79it/s]Epoch 9 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.82it/s]Epoch 9 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.83it/s]Epoch 9 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.85it/s]Epoch 9 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.86it/s]Epoch 9 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.85it/s]Epoch 9 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.85it/s]Epoch 9 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.85it/s]Epoch 9 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.86it/s][2025-06-03 01:30:34,466][__main__][INFO] - Train Epoch: 9 [100/101 (99%)]	Loss: 0.425415
Epoch 9 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.23it/s]                                                                  Epoch 9 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 9 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.84it/s]Epoch 9 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.76it/s]Epoch 9 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.73it/s]Epoch 9 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.73it/s]Epoch 9 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.72it/s]Epoch 9 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.72it/s]Epoch 9 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.70it/s]Epoch 9 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.72it/s]Epoch 9 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.72it/s]Epoch 9 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.69it/s]Epoch 9 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.71it/s]Epoch 9 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.68it/s]                                                              [2025-06-03 01:30:35,914][__main__][INFO] - Epoch 9: Val Loss: 0.7551, Accuracy: 0.7030, Metrics: {'accuracy': 0.7029702970297029, 'f1_macro': 0.639452380952381, 'f1_weighted': 0.6957218293257897, 'precision_macro': 0.6734265734265734, 'recall_macro': 0.625808936825886}
Epoch 10 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:30:36,087][__main__][INFO] - Train Epoch: 10 [0/101 (0%)]	Loss: 0.581935
Epoch 10 [Train]:   1%|          | 1/101 [00:00<00:17,  5.86it/s]Epoch 10 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.87it/s]Epoch 10 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.86it/s]Epoch 10 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.87it/s]Epoch 10 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 10 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.87it/s]Epoch 10 [Train]:   7%|‚ñã         | 7/101 [00:01<00:15,  5.88it/s]Epoch 10 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.87it/s]Epoch 10 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.88it/s]Epoch 10 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.86it/s]Epoch 10 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.86it/s]Epoch 10 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.87it/s]Epoch 10 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.86it/s]Epoch 10 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.87it/s]Epoch 10 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.86it/s]Epoch 10 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.87it/s]Epoch 10 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.86it/s]Epoch 10 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.86it/s]Epoch 10 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:13,  5.86it/s]Epoch 10 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.85it/s]Epoch 10 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.87it/s]Epoch 10 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.86it/s]Epoch 10 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.86it/s]Epoch 10 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.86it/s]Epoch 10 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.85it/s]Epoch 10 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.87it/s]Epoch 10 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.87it/s]Epoch 10 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.87it/s]Epoch 10 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.87it/s]Epoch 10 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.86it/s]Epoch 10 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.86it/s]Epoch 10 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.85it/s]Epoch 10 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.86it/s]Epoch 10 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.86it/s]Epoch 10 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.85it/s]Epoch 10 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.86it/s]Epoch 10 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.84it/s]Epoch 10 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.86it/s]Epoch 10 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.86it/s]Epoch 10 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.86it/s]Epoch 10 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:06<00:10,  5.87it/s]Epoch 10 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.86it/s]Epoch 10 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.87it/s]Epoch 10 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.86it/s]Epoch 10 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.86it/s]Epoch 10 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.80it/s]Epoch 10 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.83it/s]Epoch 10 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.85it/s]Epoch 10 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.85it/s]Epoch 10 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.85it/s]Epoch 10 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.85it/s]Epoch 10 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.86it/s]Epoch 10 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.86it/s]Epoch 10 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.86it/s]Epoch 10 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.87it/s]Epoch 10 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.84it/s]Epoch 10 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.85it/s]Epoch 10 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.85it/s]Epoch 10 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.85it/s]Epoch 10 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.83it/s]Epoch 10 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.84it/s]Epoch 10 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.84it/s]Epoch 10 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.84it/s]Epoch 10 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.86it/s]Epoch 10 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.85it/s]Epoch 10 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.86it/s]Epoch 10 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.86it/s]Epoch 10 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.86it/s]Epoch 10 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.87it/s]Epoch 10 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.82it/s]Epoch 10 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.82it/s]Epoch 10 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.82it/s]Epoch 10 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.83it/s]Epoch 10 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.83it/s]Epoch 10 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.85it/s]Epoch 10 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.85it/s]Epoch 10 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.86it/s]Epoch 10 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.86it/s]Epoch 10 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.84it/s]Epoch 10 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.65it/s]Epoch 10 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.70it/s]Epoch 10 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.75it/s]Epoch 10 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.78it/s]Epoch 10 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.79it/s]Epoch 10 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.80it/s]Epoch 10 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.81it/s]Epoch 10 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.81it/s]Epoch 10 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.82it/s]Epoch 10 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.83it/s]Epoch 10 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.84it/s]Epoch 10 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.85it/s]Epoch 10 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.85it/s]Epoch 10 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.84it/s]Epoch 10 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.84it/s]Epoch 10 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.75it/s]Epoch 10 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.73it/s]Epoch 10 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.78it/s]Epoch 10 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.79it/s]Epoch 10 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.81it/s]Epoch 10 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.81it/s][2025-06-03 01:30:53,174][__main__][INFO] - Train Epoch: 10 [100/101 (99%)]	Loss: 0.648160
Epoch 10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.21it/s]                                                                   Epoch 10 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 10 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.88it/s]Epoch 10 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.84it/s]Epoch 10 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.78it/s]Epoch 10 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.78it/s]Epoch 10 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.59it/s]Epoch 10 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.54it/s]Epoch 10 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.62it/s]Epoch 10 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.66it/s]Epoch 10 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.71it/s]Epoch 10 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.69it/s]Epoch 10 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.72it/s]Epoch 10 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.73it/s]                                                               [2025-06-03 01:30:54,622][__main__][INFO] - Epoch 10: Val Loss: 0.7926, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.6190569487983282, 'f1_weighted': 0.6665813133037443, 'precision_macro': 0.6142957919273709, 'recall_macro': 0.6253852080123267}
Epoch 11 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:30:54,795][__main__][INFO] - Train Epoch: 11 [0/101 (0%)]	Loss: 0.201911
Epoch 11 [Train]:   1%|          | 1/101 [00:00<00:17,  5.86it/s]Epoch 11 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.85it/s]Epoch 11 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.86it/s]Epoch 11 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.84it/s]Epoch 11 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.84it/s]Epoch 11 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.82it/s]Epoch 11 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.84it/s]Epoch 11 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.85it/s]Epoch 11 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.84it/s]Epoch 11 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.85it/s]Epoch 11 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.84it/s]Epoch 11 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.84it/s]Epoch 11 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.82it/s]Epoch 11 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.84it/s]Epoch 11 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.82it/s]Epoch 11 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.83it/s]Epoch 11 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.84it/s]Epoch 11 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.84it/s]Epoch 11 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.85it/s]Epoch 11 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.84it/s]Epoch 11 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.85it/s]Epoch 11 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.84it/s]Epoch 11 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.84it/s]Epoch 11 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.84it/s]Epoch 11 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.84it/s]Epoch 11 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.79it/s]Epoch 11 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.81it/s]Epoch 11 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.81it/s]Epoch 11 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.82it/s]Epoch 11 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.84it/s]Epoch 11 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.84it/s]Epoch 11 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.84it/s]Epoch 11 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.83it/s]Epoch 11 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.84it/s]Epoch 11 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.84it/s]Epoch 11 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.85it/s]Epoch 11 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.85it/s]Epoch 11 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.84it/s]Epoch 11 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.85it/s]Epoch 11 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.84it/s]Epoch 11 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.85it/s]Epoch 11 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.84it/s]Epoch 11 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.86it/s]Epoch 11 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.86it/s]Epoch 11 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.85it/s]Epoch 11 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.86it/s]Epoch 11 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.85it/s]Epoch 11 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.86it/s]Epoch 11 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.84it/s]Epoch 11 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.84it/s]Epoch 11 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.84it/s]Epoch 11 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.84it/s]Epoch 11 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.84it/s]Epoch 11 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.84it/s]Epoch 11 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.84it/s]Epoch 11 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.84it/s]Epoch 11 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.85it/s]Epoch 11 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.84it/s]Epoch 11 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.83it/s]Epoch 11 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.85it/s]Epoch 11 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.85it/s]Epoch 11 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.85it/s]Epoch 11 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.83it/s]Epoch 11 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.76it/s]Epoch 11 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.55it/s]Epoch 11 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.02it/s]Epoch 11 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:06,  5.15it/s]Epoch 11 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:06,  5.33it/s]Epoch 11 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.47it/s]Epoch 11 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.58it/s]Epoch 11 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.67it/s]Epoch 11 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.72it/s]Epoch 11 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.75it/s]Epoch 11 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.78it/s]Epoch 11 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.79it/s]Epoch 11 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.82it/s]Epoch 11 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.80it/s]Epoch 11 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.79it/s]Epoch 11 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.77it/s]Epoch 11 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.80it/s]Epoch 11 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.80it/s]Epoch 11 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.82it/s]Epoch 11 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.81it/s]Epoch 11 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.81it/s]Epoch 11 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.78it/s]Epoch 11 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.78it/s]Epoch 11 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.80it/s]Epoch 11 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.82it/s]Epoch 11 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.82it/s]Epoch 11 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.82it/s]Epoch 11 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.82it/s]Epoch 11 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.83it/s]Epoch 11 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.82it/s]Epoch 11 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.83it/s]Epoch 11 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.81it/s]Epoch 11 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.82it/s]Epoch 11 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.82it/s]Epoch 11 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.83it/s]Epoch 11 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.83it/s]Epoch 11 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.83it/s][2025-06-03 01:31:12,017][__main__][INFO] - Train Epoch: 11 [100/101 (99%)]	Loss: 0.062764
Epoch 11 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.23it/s]                                                                   Epoch 11 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 11 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.72it/s]Epoch 11 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.74it/s]Epoch 11 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.76it/s]Epoch 11 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.75it/s]Epoch 11 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.78it/s]Epoch 11 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.77it/s]Epoch 11 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.77it/s]Epoch 11 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.76it/s]Epoch 11 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.77it/s]Epoch 11 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.75it/s]Epoch 11 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.75it/s]Epoch 11 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.74it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:31:13,462][__main__][INFO] - Epoch 11: Val Loss: 0.7653, Accuracy: 0.7327, Metrics: {'accuracy': 0.7326732673267327, 'f1_macro': 0.5458074534161491, 'f1_weighted': 0.6749277412213271, 'precision_macro': 0.5720464135021097, 'recall_macro': 0.5460708782742681}
Epoch 12 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:31:13,635][__main__][INFO] - Train Epoch: 12 [0/101 (0%)]	Loss: 0.150823
Epoch 12 [Train]:   1%|          | 1/101 [00:00<00:17,  5.83it/s]Epoch 12 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.84it/s]Epoch 12 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.85it/s]Epoch 12 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.85it/s]Epoch 12 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 12 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.85it/s]Epoch 12 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.86it/s]Epoch 12 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.84it/s]Epoch 12 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.85it/s]Epoch 12 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.86it/s]Epoch 12 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.84it/s]Epoch 12 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.84it/s]Epoch 12 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.83it/s]Epoch 12 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.85it/s]Epoch 12 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.84it/s]Epoch 12 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.85it/s]Epoch 12 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.85it/s]Epoch 12 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.85it/s]Epoch 12 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.84it/s]Epoch 12 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.84it/s]Epoch 12 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.85it/s]Epoch 12 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.82it/s]Epoch 12 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.82it/s]Epoch 12 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.82it/s]Epoch 12 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.84it/s]Epoch 12 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.84it/s]Epoch 12 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.85it/s]Epoch 12 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.84it/s]Epoch 12 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.84it/s]Epoch 12 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.85it/s]Epoch 12 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.84it/s]Epoch 12 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.85it/s]Epoch 12 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.83it/s]Epoch 12 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.83it/s]Epoch 12 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.83it/s]Epoch 12 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.83it/s]Epoch 12 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.83it/s]Epoch 12 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.84it/s]Epoch 12 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.85it/s]Epoch 12 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.86it/s]Epoch 12 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.86it/s]Epoch 12 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.85it/s]Epoch 12 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.85it/s]Epoch 12 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.84it/s]Epoch 12 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.83it/s]Epoch 12 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.83it/s]Epoch 12 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.76it/s]Epoch 12 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.78it/s]Epoch 12 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.81it/s]Epoch 12 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.81it/s]Epoch 12 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.83it/s]Epoch 12 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.83it/s]Epoch 12 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.83it/s]Epoch 12 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.84it/s]Epoch 12 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.84it/s]Epoch 12 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.85it/s]Epoch 12 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.84it/s]Epoch 12 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.85it/s]Epoch 12 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.84it/s]Epoch 12 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.85it/s]Epoch 12 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.82it/s]Epoch 12 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.83it/s]Epoch 12 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.83it/s]Epoch 12 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.84it/s]Epoch 12 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.85it/s]Epoch 12 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.84it/s]Epoch 12 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.84it/s]Epoch 12 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.83it/s]Epoch 12 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.83it/s]Epoch 12 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.83it/s]Epoch 12 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.84it/s]Epoch 12 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.84it/s]Epoch 12 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.85it/s]Epoch 12 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.84it/s]Epoch 12 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.83it/s]Epoch 12 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.84it/s]Epoch 12 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.83it/s]Epoch 12 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.84it/s]Epoch 12 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.83it/s]Epoch 12 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.85it/s]Epoch 12 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.85it/s]Epoch 12 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.85it/s]Epoch 12 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.85it/s]Epoch 12 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.84it/s]Epoch 12 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.85it/s]Epoch 12 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.83it/s]Epoch 12 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.83it/s]Epoch 12 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.78it/s]Epoch 12 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.80it/s]Epoch 12 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.79it/s]Epoch 12 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.81it/s]Epoch 12 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.81it/s]Epoch 12 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.81it/s]Epoch 12 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.81it/s]Epoch 12 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.83it/s]Epoch 12 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.83it/s]Epoch 12 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.85it/s]Epoch 12 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.83it/s]Epoch 12 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.83it/s]Epoch 12 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.84it/s][2025-06-03 01:31:30,737][__main__][INFO] - Train Epoch: 12 [100/101 (99%)]	Loss: 0.048589
Epoch 12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.22it/s]                                                                   Epoch 12 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 12 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.90it/s]Epoch 12 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.85it/s]Epoch 12 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.78it/s]Epoch 12 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.75it/s]Epoch 12 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.72it/s]Epoch 12 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.75it/s]Epoch 12 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.74it/s]Epoch 12 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.75it/s]Epoch 12 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.73it/s]Epoch 12 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.71it/s]Epoch 12 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.73it/s]Epoch 12 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.72it/s]                                                               [2025-06-03 01:31:32,182][__main__][INFO] - Epoch 12: Val Loss: 0.9105, Accuracy: 0.6832, Metrics: {'accuracy': 0.6831683168316832, 'f1_macro': 0.5010875274959782, 'f1_weighted': 0.6137022694685503, 'precision_macro': 0.7482201533406353, 'recall_macro': 0.5020608628659476}
Epoch 13 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:31:32,356][__main__][INFO] - Train Epoch: 13 [0/101 (0%)]	Loss: 0.143362
Epoch 13 [Train]:   1%|          | 1/101 [00:00<00:17,  5.84it/s]Epoch 13 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.70it/s]Epoch 13 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.77it/s]Epoch 13 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.78it/s]Epoch 13 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.82it/s]Epoch 13 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.82it/s]Epoch 13 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.83it/s]Epoch 13 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.84it/s]Epoch 13 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.83it/s]Epoch 13 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.83it/s]Epoch 13 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.83it/s]Epoch 13 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.84it/s]Epoch 13 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.83it/s]Epoch 13 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.84it/s]Epoch 13 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.84it/s]Epoch 13 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.85it/s]Epoch 13 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.85it/s]Epoch 13 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.84it/s]Epoch 13 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.85it/s]Epoch 13 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.70it/s]Epoch 13 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 13 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.76it/s]Epoch 13 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.78it/s]Epoch 13 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.79it/s]Epoch 13 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.81it/s]Epoch 13 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.81it/s]Epoch 13 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.81it/s]Epoch 13 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.83it/s]Epoch 13 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.83it/s]Epoch 13 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.84it/s]Epoch 13 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.84it/s]Epoch 13 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.84it/s]Epoch 13 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.84it/s]Epoch 13 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.83it/s]Epoch 13 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.84it/s]Epoch 13 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.83it/s]Epoch 13 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.82it/s]Epoch 13 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.82it/s]Epoch 13 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.79it/s]Epoch 13 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:11,  5.37it/s]Epoch 13 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.49it/s]Epoch 13 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.59it/s]Epoch 13 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.63it/s]Epoch 13 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:10,  5.70it/s]Epoch 13 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.73it/s]Epoch 13 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.76it/s]Epoch 13 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.77it/s]Epoch 13 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.80it/s]Epoch 13 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.78it/s]Epoch 13 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.80it/s]Epoch 13 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.79it/s]Epoch 13 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.81it/s]Epoch 13 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.81it/s]Epoch 13 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.82it/s]Epoch 13 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.83it/s]Epoch 13 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.83it/s]Epoch 13 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.83it/s]Epoch 13 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.83it/s]Epoch 13 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.84it/s]Epoch 13 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.83it/s]Epoch 13 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.85it/s]Epoch 13 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.84it/s]Epoch 13 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.84it/s]Epoch 13 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.84it/s]Epoch 13 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.70it/s]Epoch 13 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.74it/s]Epoch 13 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.76it/s]Epoch 13 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.78it/s]Epoch 13 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.81it/s]Epoch 13 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.81it/s]Epoch 13 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.74it/s]Epoch 13 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.77it/s]Epoch 13 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.78it/s]Epoch 13 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.79it/s]Epoch 13 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.80it/s]Epoch 13 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.81it/s]Epoch 13 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.81it/s]Epoch 13 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.82it/s]Epoch 13 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.81it/s]Epoch 13 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.81it/s]Epoch 13 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.81it/s]Epoch 13 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.82it/s]Epoch 13 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.81it/s]Epoch 13 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.81it/s]Epoch 13 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.82it/s]Epoch 13 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.81it/s]Epoch 13 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.81it/s]Epoch 13 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.81it/s]Epoch 13 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.81it/s]Epoch 13 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.81it/s]Epoch 13 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.82it/s]Epoch 13 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.81it/s]Epoch 13 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.82it/s]Epoch 13 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.82it/s]Epoch 13 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.83it/s]Epoch 13 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.82it/s]Epoch 13 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.83it/s]Epoch 13 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.82it/s]Epoch 13 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.83it/s]Epoch 13 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.83it/s][2025-06-03 01:31:49,570][__main__][INFO] - Train Epoch: 13 [100/101 (99%)]	Loss: 0.353264
Epoch 13 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.22it/s]                                                                   Epoch 13 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 13 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.84it/s]Epoch 13 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.77it/s]Epoch 13 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.72it/s]Epoch 13 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.73it/s]Epoch 13 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.72it/s]Epoch 13 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.69it/s]Epoch 13 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.70it/s]Epoch 13 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.69it/s]Epoch 13 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.72it/s]Epoch 13 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.69it/s]Epoch 13 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.71it/s]Epoch 13 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.71it/s]                                                               [2025-06-03 01:31:51,018][__main__][INFO] - Epoch 13: Val Loss: 0.8090, Accuracy: 0.6832, Metrics: {'accuracy': 0.6831683168316832, 'f1_macro': 0.6243400412796698, 'f1_weighted': 0.6789134250886388, 'precision_macro': 0.6708291708291708, 'recall_macro': 0.59884437596302}
Epoch 14 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:31:51,191][__main__][INFO] - Train Epoch: 14 [0/101 (0%)]	Loss: 0.511021
Epoch 14 [Train]:   1%|          | 1/101 [00:00<00:17,  5.86it/s]Epoch 14 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.83it/s]Epoch 14 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.83it/s]Epoch 14 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.82it/s]Epoch 14 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.84it/s]Epoch 14 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.82it/s]Epoch 14 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.83it/s]Epoch 14 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.83it/s]Epoch 14 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.83it/s]Epoch 14 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.83it/s]Epoch 14 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.82it/s]Epoch 14 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.83it/s]Epoch 14 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.83it/s]Epoch 14 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.83it/s]Epoch 14 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.82it/s]Epoch 14 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.83it/s]Epoch 14 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.82it/s]Epoch 14 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.83it/s]Epoch 14 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.82it/s]Epoch 14 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.83it/s]Epoch 14 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.83it/s]Epoch 14 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.82it/s]Epoch 14 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.83it/s]Epoch 14 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.82it/s]Epoch 14 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.79it/s]Epoch 14 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.81it/s]Epoch 14 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.81it/s]Epoch 14 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.82it/s]Epoch 14 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.82it/s]Epoch 14 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.83it/s]Epoch 14 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.83it/s]Epoch 14 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.81it/s]Epoch 14 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.82it/s]Epoch 14 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.81it/s]Epoch 14 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.82it/s]Epoch 14 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.82it/s]Epoch 14 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.83it/s]Epoch 14 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.81it/s]Epoch 14 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.83it/s]Epoch 14 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.82it/s]Epoch 14 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.83it/s]Epoch 14 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.83it/s]Epoch 14 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.83it/s]Epoch 14 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.82it/s]Epoch 14 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.82it/s]Epoch 14 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.81it/s]Epoch 14 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.82it/s]Epoch 14 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.82it/s]Epoch 14 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.83it/s]Epoch 14 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.83it/s]Epoch 14 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.83it/s]Epoch 14 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.83it/s]Epoch 14 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.82it/s]Epoch 14 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.75it/s]Epoch 14 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.77it/s]Epoch 14 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.79it/s]Epoch 14 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.80it/s]Epoch 14 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.81it/s]Epoch 14 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.82it/s]Epoch 14 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.82it/s]Epoch 14 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.82it/s]Epoch 14 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.83it/s]Epoch 14 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.82it/s]Epoch 14 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.83it/s]Epoch 14 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.82it/s]Epoch 14 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.84it/s]Epoch 14 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.83it/s]Epoch 14 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.84it/s]Epoch 14 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.83it/s]Epoch 14 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.83it/s]Epoch 14 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.80it/s]Epoch 14 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.82it/s]Epoch 14 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.81it/s]Epoch 14 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.82it/s]Epoch 14 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.82it/s]Epoch 14 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.83it/s]Epoch 14 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.83it/s]Epoch 14 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.82it/s]Epoch 14 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.83it/s]Epoch 14 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.82it/s]Epoch 14 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.83it/s]Epoch 14 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.83it/s]Epoch 14 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.84it/s]Epoch 14 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.84it/s]Epoch 14 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.84it/s]Epoch 14 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.83it/s]Epoch 14 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.83it/s]Epoch 14 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.84it/s]Epoch 14 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.83it/s]Epoch 14 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.84it/s]Epoch 14 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.83it/s]Epoch 14 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.84it/s]Epoch 14 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.84it/s]Epoch 14 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.85it/s]Epoch 14 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.83it/s]Epoch 14 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.83it/s]Epoch 14 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.83it/s]Epoch 14 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.83it/s]Epoch 14 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.82it/s]Epoch 14 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.80it/s][2025-06-03 01:32:08,328][__main__][INFO] - Train Epoch: 14 [100/101 (99%)]	Loss: 1.508695
Epoch 14 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.19it/s]                                                                   Epoch 14 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 14 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.80it/s]Epoch 14 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.71it/s]Epoch 14 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.69it/s]Epoch 14 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.63it/s]Epoch 14 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.64it/s]Epoch 14 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.65it/s]Epoch 14 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.66it/s]Epoch 14 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.69it/s]Epoch 14 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.69it/s]Epoch 14 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.64it/s]Epoch 14 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.67it/s]Epoch 14 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.64it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:32:09,781][__main__][INFO] - Epoch 14: Val Loss: 0.8423, Accuracy: 0.7525, Metrics: {'accuracy': 0.7524752475247525, 'f1_macro': 0.567738000197343, 'f1_weighted': 0.7041945877026862, 'precision_macro': 0.5549428104575164, 'recall_macro': 0.5875963020030817}
Epoch 15 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:32:09,960][__main__][INFO] - Train Epoch: 15 [0/101 (0%)]	Loss: 0.855426
Epoch 15 [Train]:   1%|          | 1/101 [00:00<00:17,  5.63it/s]Epoch 15 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.73it/s]Epoch 15 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.78it/s]Epoch 15 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.79it/s]Epoch 15 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.81it/s]Epoch 15 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.81it/s]Epoch 15 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.82it/s]Epoch 15 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.82it/s]Epoch 15 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.83it/s]Epoch 15 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.83it/s]Epoch 15 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.80it/s]Epoch 15 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:16,  5.53it/s]Epoch 15 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.61it/s]Epoch 15 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.64it/s]Epoch 15 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.70it/s]Epoch 15 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.73it/s]Epoch 15 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.77it/s]Epoch 15 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.79it/s]Epoch 15 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.81it/s]Epoch 15 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.80it/s]Epoch 15 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.82it/s]Epoch 15 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.83it/s]Epoch 15 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.84it/s]Epoch 15 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.84it/s]Epoch 15 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.84it/s]Epoch 15 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.84it/s]Epoch 15 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.83it/s]Epoch 15 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.83it/s]Epoch 15 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.83it/s]Epoch 15 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.84it/s]Epoch 15 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.83it/s]Epoch 15 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.85it/s]Epoch 15 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.77it/s]Epoch 15 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.79it/s]Epoch 15 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.79it/s]Epoch 15 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.81it/s]Epoch 15 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.81it/s]Epoch 15 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.83it/s]Epoch 15 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.83it/s]Epoch 15 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.83it/s]Epoch 15 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.83it/s]Epoch 15 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.81it/s]Epoch 15 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.81it/s]Epoch 15 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.82it/s]Epoch 15 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.84it/s]Epoch 15 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.83it/s]Epoch 15 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.84it/s]Epoch 15 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.83it/s]Epoch 15 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.83it/s]Epoch 15 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.82it/s]Epoch 15 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.82it/s]Epoch 15 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.82it/s]Epoch 15 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.83it/s]Epoch 15 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.83it/s]Epoch 15 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.83it/s]Epoch 15 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.83it/s]Epoch 15 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.83it/s]Epoch 15 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.84it/s]Epoch 15 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.82it/s]Epoch 15 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.83it/s]Epoch 15 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.82it/s]Epoch 15 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.84it/s]Epoch 15 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.82it/s]Epoch 15 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.84it/s]Epoch 15 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.83it/s]Epoch 15 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.50it/s]Epoch 15 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:06,  5.60it/s]Epoch 15 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.67it/s]Epoch 15 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.73it/s]Epoch 15 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.76it/s]Epoch 15 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.78it/s]Epoch 15 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.81it/s]Epoch 15 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.80it/s]Epoch 15 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.81it/s]Epoch 15 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.81it/s]Epoch 15 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.83it/s]Epoch 15 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.83it/s]Epoch 15 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.84it/s]Epoch 15 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.84it/s]Epoch 15 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.83it/s]Epoch 15 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.84it/s]Epoch 15 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.83it/s]Epoch 15 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.83it/s]Epoch 15 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.82it/s]Epoch 15 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.84it/s]Epoch 15 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.83it/s]Epoch 15 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.75it/s]Epoch 15 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.75it/s]Epoch 15 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.78it/s]Epoch 15 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.80it/s]Epoch 15 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.81it/s]Epoch 15 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.81it/s]Epoch 15 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.83it/s]Epoch 15 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.82it/s]Epoch 15 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.83it/s]Epoch 15 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.83it/s]Epoch 15 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.84it/s]Epoch 15 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.85it/s]Epoch 15 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.84it/s]Epoch 15 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.83it/s][2025-06-03 01:32:27,153][__main__][INFO] - Train Epoch: 15 [100/101 (99%)]	Loss: 2.451589
Epoch 15 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.22it/s]                                                                   Epoch 15 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 15 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.37it/s]Epoch 15 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.64it/s]Epoch 15 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.23it/s]Epoch 15 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.40it/s]Epoch 15 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.52it/s]Epoch 15 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.58it/s]Epoch 15 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.65it/s]Epoch 15 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.67it/s]Epoch 15 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.69it/s]Epoch 15 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.70it/s]Epoch 15 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.70it/s]Epoch 15 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.72it/s]                                                               [2025-06-03 01:32:28,609][__main__][INFO] - Epoch 15: Val Loss: 0.9323, Accuracy: 0.6337, Metrics: {'accuracy': 0.6336633663366337, 'f1_macro': 0.5760629481150259, 'f1_weighted': 0.6387804172445616, 'precision_macro': 0.6186111111111111, 'recall_macro': 0.5800269645608629}
Epoch 16 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:32:28,782][__main__][INFO] - Train Epoch: 16 [0/101 (0%)]	Loss: 0.205748
Epoch 16 [Train]:   1%|          | 1/101 [00:00<00:17,  5.82it/s]Epoch 16 [Train]:   2%|‚ñè         | 2/101 [00:00<00:16,  5.86it/s]Epoch 16 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.86it/s]Epoch 16 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.85it/s]Epoch 16 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.86it/s]Epoch 16 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.83it/s]Epoch 16 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.84it/s]Epoch 16 [Train]:   8%|‚ñä         | 8/101 [00:01<00:15,  5.83it/s]Epoch 16 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.84it/s]Epoch 16 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.83it/s]Epoch 16 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.84it/s]Epoch 16 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.83it/s]Epoch 16 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.84it/s]Epoch 16 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:14,  5.84it/s]Epoch 16 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.84it/s]Epoch 16 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.84it/s]Epoch 16 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.83it/s]Epoch 16 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.83it/s]Epoch 16 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.83it/s]Epoch 16 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:13,  5.84it/s]Epoch 16 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.84it/s]Epoch 16 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.85it/s]Epoch 16 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.84it/s]Epoch 16 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.84it/s]Epoch 16 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:12,  5.85it/s]Epoch 16 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:12,  5.85it/s]Epoch 16 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.85it/s]Epoch 16 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.83it/s]Epoch 16 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:04<00:12,  5.82it/s]Epoch 16 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.82it/s]Epoch 16 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:11,  5.85it/s]Epoch 16 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.85it/s]Epoch 16 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.86it/s]Epoch 16 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.86it/s]Epoch 16 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:05<00:11,  5.86it/s]Epoch 16 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.86it/s]Epoch 16 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:10,  5.86it/s]Epoch 16 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.87it/s]Epoch 16 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.82it/s]Epoch 16 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.84it/s]Epoch 16 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.82it/s]Epoch 16 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.83it/s]Epoch 16 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:09,  5.84it/s]Epoch 16 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.84it/s]Epoch 16 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.84it/s]Epoch 16 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.84it/s]Epoch 16 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.85it/s]Epoch 16 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.83it/s]Epoch 16 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:08,  5.85it/s]Epoch 16 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.83it/s]Epoch 16 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.85it/s]Epoch 16 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:08<00:08,  5.86it/s]Epoch 16 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.86it/s]Epoch 16 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.87it/s]Epoch 16 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.86it/s]Epoch 16 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.88it/s]Epoch 16 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.88it/s]Epoch 16 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:09<00:07,  5.87it/s]Epoch 16 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.88it/s]Epoch 16 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.78it/s]Epoch 16 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.81it/s]Epoch 16 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.83it/s]Epoch 16 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.85it/s]Epoch 16 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:10<00:06,  5.84it/s]Epoch 16 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.85it/s]Epoch 16 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:05,  5.85it/s]Epoch 16 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.86it/s]Epoch 16 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.87it/s]Epoch 16 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:11<00:05,  5.86it/s]Epoch 16 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:11<00:05,  5.87it/s]Epoch 16 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.87it/s]Epoch 16 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:04,  5.87it/s]Epoch 16 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.87it/s]Epoch 16 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.87it/s]Epoch 16 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:12<00:04,  5.87it/s]Epoch 16 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:12<00:04,  5.86it/s]Epoch 16 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.86it/s]Epoch 16 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.85it/s]Epoch 16 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.85it/s]Epoch 16 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.87it/s]Epoch 16 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:13<00:03,  5.87it/s]Epoch 16 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.88it/s]Epoch 16 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.89it/s]Epoch 16 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.88it/s]Epoch 16 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.88it/s]Epoch 16 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.88it/s]Epoch 16 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:14<00:02,  5.88it/s]Epoch 16 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.89it/s]Epoch 16 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.88it/s]Epoch 16 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.89it/s]Epoch 16 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.88it/s]Epoch 16 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:15<00:01,  5.88it/s]Epoch 16 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:15<00:01,  5.89it/s]Epoch 16 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.89it/s]Epoch 16 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.89it/s]Epoch 16 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.87it/s]Epoch 16 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.85it/s]Epoch 16 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:16<00:00,  5.62it/s]Epoch 16 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:16<00:00,  5.56it/s]Epoch 16 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.65it/s][2025-06-03 01:32:45,867][__main__][INFO] - Train Epoch: 16 [100/101 (99%)]	Loss: 0.067472
Epoch 16 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.08it/s]                                                                   Epoch 16 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 16 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.96it/s]Epoch 16 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.91it/s]Epoch 16 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.86it/s]Epoch 16 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.84it/s]Epoch 16 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.85it/s]Epoch 16 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.84it/s]Epoch 16 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.86it/s]Epoch 16 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.84it/s]Epoch 16 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.88it/s]Epoch 16 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.86it/s]Epoch 16 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.88it/s]Epoch 16 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.86it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:32:47,303][__main__][INFO] - Epoch 16: Val Loss: 0.9282, Accuracy: 0.7030, Metrics: {'accuracy': 0.7029702970297029, 'f1_macro': 0.5026202704119065, 'f1_weighted': 0.6339331763592458, 'precision_macro': 0.5376693766937669, 'recall_macro': 0.5085708782742682}
[2025-06-03 01:32:47,304][__main__][INFO] - Early stopping triggered after 16 epochs
[2025-06-03 01:32:47,305][__main__][INFO] - Saving best confusion matrix with accuracy: 0.7723
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee23a1f990>
<numpy.flatiter object at 0x7fee23a1f990>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee23a09600>
<numpy.flatiter object at 0x7fee23a09600>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23a09600>
<numpy.flatiter object at 0x7fee23a09600>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee23a09c00>
<numpy.flatiter object at 0x7fee23a09c00>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee23a09c00>
<numpy.flatiter object at 0x7fee23a09c00>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:              Best Accuracy ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                 Train Loss ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñÅ‚ñÅ
wandb:        Validation Accuracy ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÖ
wandb:            Validation Loss ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ
wandb:        Validation accuracy ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÖ
wandb:        Validation f1_macro ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:     Validation f1_weighted ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ
wandb: Validation precision_macro ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:    Validation recall_macro ‚ñÅ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÖ
wandb: 
wandb: Run summary:
wandb:              Best Accuracy 0.77228
wandb:                 Train Loss 0.06747
wandb:        Validation Accuracy 0.70297
wandb:            Validation Loss 0.92821
wandb:        Validation accuracy 0.70297
wandb:        Validation f1_macro 0.50262
wandb:     Validation f1_weighted 0.63393
wandb: Validation precision_macro 0.53767
wandb:    Validation recall_macro 0.50857
wandb: 
wandb: üöÄ View run prime-sweep-1 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/55m39ab3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_012720-55m39ab3/logs
wandb: Agent Starting Run: nofyn6e2 with config:
wandb: 	Fdropout_rate: 0.4316929312119674
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.3841574777136745
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.036224765907955016
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013254-nofyn6e2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nofyn6e2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.

(-4.375, -7.0, 4.375, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
Training started...
Epoch 0 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:32:58,512][__main__][INFO] - Train Epoch: 0 [0/101 (0%)]	Loss: 1.104934
Epoch 0 [Train]:   1%|          | 1/101 [00:00<00:19,  5.10it/s]Epoch 0 [Train]:   2%|‚ñè         | 2/101 [00:00<00:18,  5.46it/s]Epoch 0 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.54it/s]Epoch 0 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.59it/s]Epoch 0 [Train]:   5%|‚ñç         | 5/101 [00:00<00:17,  5.63it/s]Epoch 0 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.66it/s]Epoch 0 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.67it/s]Epoch 0 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.69it/s]Epoch 0 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.69it/s]Epoch 0 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.71it/s]Epoch 0 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.71it/s]Epoch 0 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.71it/s]Epoch 0 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.72it/s]Epoch 0 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.71it/s]Epoch 0 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.72it/s]Epoch 0 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.73it/s]Epoch 0 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.73it/s]Epoch 0 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.74it/s]Epoch 0 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.74it/s]Epoch 0 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.74it/s]Epoch 0 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.74it/s]Epoch 0 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 0 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.70it/s]Epoch 0 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.71it/s]Epoch 0 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.71it/s]Epoch 0 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.72it/s]Epoch 0 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.71it/s]Epoch 0 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.71it/s]Epoch 0 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.72it/s]Epoch 0 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.70it/s]Epoch 0 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.71it/s]Epoch 0 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.72it/s]Epoch 0 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.72it/s]Epoch 0 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.72it/s]Epoch 0 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.73it/s]Epoch 0 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.72it/s]Epoch 0 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.69it/s]Epoch 0 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.69it/s]Epoch 0 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.71it/s]Epoch 0 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.73it/s]Epoch 0 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.72it/s]Epoch 0 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.72it/s]Epoch 0 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 0 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.71it/s]Epoch 0 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.71it/s]Epoch 0 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.71it/s]Epoch 0 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.71it/s]Epoch 0 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.71it/s]Epoch 0 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.71it/s]Epoch 0 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.70it/s]Epoch 0 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.70it/s]Epoch 0 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.70it/s]Epoch 0 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.70it/s]Epoch 0 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.71it/s]Epoch 0 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.72it/s]Epoch 0 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 0 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.74it/s]Epoch 0 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.75it/s]Epoch 0 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.75it/s]Epoch 0 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.73it/s]Epoch 0 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.73it/s]Epoch 0 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.70it/s]Epoch 0 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.67it/s]Epoch 0 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.69it/s]Epoch 0 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.68it/s]Epoch 0 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.67it/s]Epoch 0 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.69it/s]Epoch 0 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.71it/s]Epoch 0 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.71it/s]Epoch 0 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.72it/s]Epoch 0 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.71it/s]Epoch 0 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.72it/s]Epoch 0 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.72it/s]Epoch 0 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.71it/s]Epoch 0 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 0 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.71it/s]Epoch 0 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.71it/s]Epoch 0 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.73it/s]Epoch 0 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 0 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.73it/s]Epoch 0 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.73it/s]Epoch 0 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.72it/s]Epoch 0 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.73it/s]Epoch 0 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.72it/s]Epoch 0 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.70it/s]Epoch 0 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.70it/s]Epoch 0 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.71it/s]Epoch 0 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.73it/s]Epoch 0 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.74it/s]Epoch 0 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.73it/s]Epoch 0 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.73it/s]Epoch 0 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.73it/s]Epoch 0 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.73it/s]Epoch 0 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.73it/s]Epoch 0 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.71it/s]Epoch 0 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.71it/s]Epoch 0 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.73it/s]Epoch 0 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.68it/s]Epoch 0 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.71it/s]Epoch 0 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.71it/s][2025-06-03 01:33:15,976][__main__][INFO] - Train Epoch: 0 [100/101 (99%)]	Loss: 1.054998
Epoch 0 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.09it/s]                                                                  Epoch 0 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 0 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.26it/s]Epoch 0 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.10it/s]Epoch 0 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.13it/s]Epoch 0 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.18it/s]Epoch 0 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.22it/s]Epoch 0 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.21it/s]Epoch 0 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.20it/s]Epoch 0 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.23it/s]Epoch 0 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.20it/s]Epoch 0 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.17it/s]Epoch 0 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.19it/s]Epoch 0 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.18it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:33:17,469][__main__][INFO] - Epoch 0: Val Loss: 1.2934, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fef75904e20>
<numpy.flatiter object at 0x7fef75904e20>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fef761c4fc0>
<numpy.flatiter object at 0x7fef761c4fc0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fef761c4fc0>
<numpy.flatiter object at 0x7fef761c4fc0>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fef761bfbc0>
<numpy.flatiter object at 0x7fef761bfbc0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fef761bfbc0>
<numpy.flatiter object at 0x7fef761bfbc0>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:33:17,593][__main__][INFO] - Saved best model at epoch 0 with accuracy: 0.5842
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.3125, -4.0, 112.3125, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.3125, -4.0, 112.3125, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:33:17,704][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
[2025-06-03 01:33:17,736][__main__][INFO] - Saved model at epoch 0
Epoch 1 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:33:17,912][__main__][INFO] - Train Epoch: 1 [0/101 (0%)]	Loss: 0.582068
Epoch 1 [Train]:   1%|          | 1/101 [00:00<00:17,  5.71it/s]Epoch 1 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.73it/s]Epoch 1 [Train]:   3%|‚ñé         | 3/101 [00:00<00:18,  5.36it/s]Epoch 1 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.39it/s]Epoch 1 [Train]:   5%|‚ñç         | 5/101 [00:00<00:17,  5.45it/s]Epoch 1 [Train]:   6%|‚ñå         | 6/101 [00:01<00:17,  5.55it/s]Epoch 1 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.59it/s]Epoch 1 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.63it/s]Epoch 1 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.64it/s]Epoch 1 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:16,  5.66it/s]Epoch 1 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.67it/s]Epoch 1 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.69it/s]Epoch 1 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.70it/s]Epoch 1 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.70it/s]Epoch 1 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.72it/s]Epoch 1 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.70it/s]Epoch 1 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:03<00:15,  5.54it/s]Epoch 1 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.58it/s]Epoch 1 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.61it/s]Epoch 1 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.65it/s]Epoch 1 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:14,  5.66it/s]Epoch 1 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.65it/s]Epoch 1 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.63it/s]Epoch 1 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.66it/s]Epoch 1 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.68it/s]Epoch 1 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.50it/s]Epoch 1 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:13,  5.57it/s]Epoch 1 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:13,  5.61it/s]Epoch 1 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.64it/s]Epoch 1 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.67it/s]Epoch 1 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.69it/s]Epoch 1 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.69it/s]Epoch 1 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:12,  5.64it/s]Epoch 1 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:06<00:11,  5.61it/s]Epoch 1 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.64it/s]Epoch 1 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.67it/s]Epoch 1 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.68it/s]Epoch 1 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.68it/s]Epoch 1 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.67it/s]Epoch 1 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.69it/s]Epoch 1 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.69it/s]Epoch 1 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.70it/s]Epoch 1 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.54it/s]Epoch 1 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:10,  5.60it/s]Epoch 1 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.64it/s]Epoch 1 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:12,  4.51it/s]Epoch 1 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:11,  4.82it/s]Epoch 1 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:10,  5.06it/s]Epoch 1 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.25it/s]Epoch 1 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:09<00:09,  5.37it/s]Epoch 1 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:09<00:09,  5.48it/s]Epoch 1 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.54it/s]Epoch 1 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.59it/s]Epoch 1 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.64it/s]Epoch 1 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.65it/s]Epoch 1 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:10<00:07,  5.65it/s]Epoch 1 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:10<00:07,  5.67it/s]Epoch 1 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.68it/s]Epoch 1 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.70it/s]Epoch 1 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.70it/s]Epoch 1 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.69it/s]Epoch 1 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:11<00:06,  5.69it/s]Epoch 1 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.67it/s]Epoch 1 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.68it/s]Epoch 1 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.69it/s]Epoch 1 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.70it/s]Epoch 1 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.70it/s]Epoch 1 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:12<00:05,  5.65it/s]Epoch 1 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.65it/s]Epoch 1 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.67it/s]Epoch 1 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.67it/s]Epoch 1 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.69it/s]Epoch 1 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:13<00:04,  5.69it/s]Epoch 1 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:13<00:04,  5.67it/s]Epoch 1 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.70it/s]Epoch 1 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.70it/s]Epoch 1 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.70it/s]Epoch 1 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.71it/s]Epoch 1 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:14<00:03,  5.72it/s]Epoch 1 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.70it/s]Epoch 1 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.70it/s]Epoch 1 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.70it/s]Epoch 1 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.69it/s]Epoch 1 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.71it/s]Epoch 1 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:15<00:02,  5.72it/s]Epoch 1 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.72it/s]Epoch 1 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.73it/s]Epoch 1 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.73it/s]Epoch 1 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.72it/s]Epoch 1 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:16<00:01,  5.73it/s]Epoch 1 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:16<00:01,  5.71it/s]Epoch 1 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.71it/s]Epoch 1 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.72it/s]Epoch 1 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.71it/s]Epoch 1 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.72it/s]Epoch 1 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:17<00:00,  5.73it/s]Epoch 1 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:17<00:00,  5.72it/s]Epoch 1 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.72it/s]Epoch 1 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.71it/s]Epoch 1 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.63it/s][2025-06-03 01:33:35,667][__main__][INFO] - Train Epoch: 1 [100/101 (99%)]	Loss: 1.061031
Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.01it/s]                                                                  Epoch 1 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 1 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 15.10it/s]Epoch 1 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 16.23it/s]Epoch 1 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 16.60it/s]Epoch 1 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 16.87it/s]Epoch 1 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.04it/s]Epoch 1 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.11it/s]Epoch 1 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.07it/s]Epoch 1 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 16.65it/s]Epoch 1 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 16.83it/s]Epoch 1 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 16.96it/s]Epoch 1 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.07it/s]Epoch 1 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.11it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:33:37,184][__main__][INFO] - Epoch 1: Val Loss: 1.1491, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.184375, 'f1_weighted': 0.43081683168316837, 'precision_macro': 0.14603960396039603, 'recall_macro': 0.25}
Epoch 2 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:33:37,369][__main__][INFO] - Train Epoch: 2 [0/101 (0%)]	Loss: 0.709159
Epoch 2 [Train]:   1%|          | 1/101 [00:00<00:18,  5.48it/s]Epoch 2 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.61it/s]Epoch 2 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.67it/s]Epoch 2 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.68it/s]Epoch 2 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.70it/s]Epoch 2 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.72it/s]Epoch 2 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.71it/s]Epoch 2 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.72it/s]Epoch 2 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.72it/s]Epoch 2 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.72it/s]Epoch 2 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.73it/s]Epoch 2 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.72it/s]Epoch 2 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.71it/s]Epoch 2 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.72it/s]Epoch 2 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.71it/s]Epoch 2 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.70it/s]Epoch 2 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.72it/s]Epoch 2 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.71it/s]Epoch 2 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.74it/s]Epoch 2 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.73it/s]Epoch 2 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 2 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.73it/s]Epoch 2 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.70it/s]Epoch 2 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.71it/s]Epoch 2 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.70it/s]Epoch 2 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.70it/s]Epoch 2 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.70it/s]Epoch 2 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.69it/s]Epoch 2 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.69it/s]Epoch 2 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.69it/s]Epoch 2 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.69it/s]Epoch 2 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.68it/s]Epoch 2 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.69it/s]Epoch 2 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.70it/s]Epoch 2 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.69it/s]Epoch 2 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.70it/s]Epoch 2 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.71it/s]Epoch 2 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.70it/s]Epoch 2 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.71it/s]Epoch 2 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.71it/s]Epoch 2 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.70it/s]Epoch 2 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.71it/s]Epoch 2 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.70it/s]Epoch 2 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:10,  5.68it/s]Epoch 2 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.68it/s]Epoch 2 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.67it/s]Epoch 2 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.69it/s]Epoch 2 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.69it/s]Epoch 2 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.69it/s]Epoch 2 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.70it/s]Epoch 2 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.69it/s]Epoch 2 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.70it/s]Epoch 2 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.70it/s]Epoch 2 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.71it/s]Epoch 2 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.70it/s]Epoch 2 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.71it/s]Epoch 2 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.71it/s]Epoch 2 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.70it/s]Epoch 2 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.71it/s]Epoch 2 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.70it/s]Epoch 2 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.70it/s]Epoch 2 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.71it/s]Epoch 2 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.71it/s]Epoch 2 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.72it/s]Epoch 2 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.72it/s]Epoch 2 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.72it/s]Epoch 2 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.72it/s]Epoch 2 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.72it/s]Epoch 2 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.71it/s]Epoch 2 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.72it/s]Epoch 2 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.75it/s]Epoch 2 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.73it/s]Epoch 2 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.73it/s]Epoch 2 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.73it/s]Epoch 2 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 2 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.71it/s]Epoch 2 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.73it/s]Epoch 2 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.74it/s]Epoch 2 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 2 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.38it/s]Epoch 2 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.44it/s]Epoch 2 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.47it/s]Epoch 2 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.54it/s]Epoch 2 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:03,  5.53it/s]Epoch 2 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.59it/s]Epoch 2 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.50it/s]Epoch 2 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.56it/s]Epoch 2 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.60it/s]Epoch 2 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.64it/s]Epoch 2 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.52it/s]Epoch 2 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:16<00:01,  5.29it/s]Epoch 2 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.41it/s]Epoch 2 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.50it/s]Epoch 2 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.56it/s]Epoch 2 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.61it/s]Epoch 2 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.65it/s]Epoch 2 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:17<00:00,  5.67it/s]Epoch 2 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.69it/s]Epoch 2 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.70it/s]Epoch 2 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.71it/s][2025-06-03 01:33:54,959][__main__][INFO] - Train Epoch: 2 [100/101 (99%)]	Loss: 0.328989
Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.04it/s]                                                                  Epoch 2 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 2 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.33it/s]Epoch 2 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.21it/s]Epoch 2 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.23it/s]Epoch 2 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.22it/s]Epoch 2 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.17it/s]Epoch 2 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.17it/s]Epoch 2 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.16it/s]Epoch 2 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.20it/s]Epoch 2 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.21it/s]Epoch 2 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.18it/s]Epoch 2 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.14it/s]Epoch 2 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.18it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:33:56,452][__main__][INFO] - Epoch 2: Val Loss: 0.9323, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.43928316806014645, 'f1_weighted': 0.6059996316567297, 'precision_macro': 0.5395220588235294, 'recall_macro': 0.4239599383667181}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fef7584ad20>
<numpy.flatiter object at 0x7fef7584ad20>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee233f1ff0>
<numpy.flatiter object at 0x7fee233f1ff0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee233f1ff0>
<numpy.flatiter object at 0x7fee233f1ff0>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee233f25f0>
<numpy.flatiter object at 0x7fee233f25f0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee233f25f0>
<numpy.flatiter object at 0x7fee233f25f0>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:33:56,525][__main__][INFO] - Saved best model at epoch 2 with accuracy: 0.6634
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.75, -7.0, 8.75, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:33:56,640][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 3 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:33:56,850][__main__][INFO] - Train Epoch: 3 [0/101 (0%)]	Loss: 0.735738
Epoch 3 [Train]:   1%|          | 1/101 [00:00<00:17,  5.64it/s]Epoch 3 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.67it/s]Epoch 3 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.70it/s]Epoch 3 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.71it/s]Epoch 3 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.71it/s]Epoch 3 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.71it/s]Epoch 3 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.68it/s]Epoch 3 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.70it/s]Epoch 3 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.70it/s]Epoch 3 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.70it/s]Epoch 3 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.71it/s]Epoch 3 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.72it/s]Epoch 3 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.71it/s]Epoch 3 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.72it/s]Epoch 3 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.71it/s]Epoch 3 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.71it/s]Epoch 3 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.72it/s]Epoch 3 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.68it/s]Epoch 3 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.69it/s]Epoch 3 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.71it/s]Epoch 3 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:14,  5.71it/s]Epoch 3 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 3 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.72it/s]Epoch 3 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.73it/s]Epoch 3 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.74it/s]Epoch 3 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.73it/s]Epoch 3 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.74it/s]Epoch 3 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.74it/s]Epoch 3 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.72it/s]Epoch 3 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.73it/s]Epoch 3 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.74it/s]Epoch 3 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.74it/s]Epoch 3 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.73it/s]Epoch 3 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.73it/s]Epoch 3 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.73it/s]Epoch 3 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.72it/s]Epoch 3 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.73it/s]Epoch 3 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.72it/s]Epoch 3 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.71it/s]Epoch 3 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.72it/s]Epoch 3 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.69it/s]Epoch 3 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.70it/s]Epoch 3 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.71it/s]Epoch 3 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.71it/s]Epoch 3 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.72it/s]Epoch 3 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.71it/s]Epoch 3 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.71it/s]Epoch 3 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.72it/s]Epoch 3 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.73it/s]Epoch 3 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.74it/s]Epoch 3 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.73it/s]Epoch 3 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.74it/s]Epoch 3 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.75it/s]Epoch 3 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.74it/s]Epoch 3 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.75it/s]Epoch 3 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.69it/s]Epoch 3 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.69it/s]Epoch 3 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.71it/s]Epoch 3 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.72it/s]Epoch 3 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.74it/s]Epoch 3 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.64it/s]Epoch 3 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.67it/s]Epoch 3 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.68it/s]Epoch 3 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.70it/s]Epoch 3 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.72it/s]Epoch 3 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.72it/s]Epoch 3 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.73it/s]Epoch 3 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.72it/s]Epoch 3 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.73it/s]Epoch 3 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.74it/s]Epoch 3 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.74it/s]Epoch 3 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.74it/s]Epoch 3 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.72it/s]Epoch 3 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.73it/s]Epoch 3 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.73it/s]Epoch 3 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.73it/s]Epoch 3 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.73it/s]Epoch 3 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.71it/s]Epoch 3 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 3 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.73it/s]Epoch 3 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.73it/s]Epoch 3 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.74it/s]Epoch 3 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.72it/s]Epoch 3 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.72it/s]Epoch 3 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.72it/s]Epoch 3 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.71it/s]Epoch 3 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.73it/s]Epoch 3 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.73it/s]Epoch 3 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.74it/s]Epoch 3 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.74it/s]Epoch 3 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.73it/s]Epoch 3 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.73it/s]Epoch 3 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.74it/s]Epoch 3 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.73it/s]Epoch 3 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.74it/s]Epoch 3 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.74it/s]Epoch 3 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.75it/s]Epoch 3 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 3 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.74it/s]Epoch 3 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.74it/s][2025-06-03 01:34:14,289][__main__][INFO] - Train Epoch: 3 [100/101 (99%)]	Loss: 0.912284
Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.12it/s]                                                                  Epoch 3 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 3 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.38it/s]Epoch 3 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.35it/s]Epoch 3 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.29it/s]Epoch 3 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.28it/s]Epoch 3 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.29it/s]Epoch 3 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.26it/s]Epoch 3 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.24it/s]Epoch 3 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.26it/s]Epoch 3 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.27it/s]Epoch 3 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.23it/s]Epoch 3 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.24it/s]Epoch 3 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.25it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:34:15,776][__main__][INFO] - Epoch 3: Val Loss: 0.8128, Accuracy: 0.6832, Metrics: {'accuracy': 0.6831683168316832, 'f1_macro': 0.5394913695529145, 'f1_weighted': 0.6512268374545434, 'precision_macro': 0.5506868131868132, 'recall_macro': 0.5477080123266564}
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee2332ff20>
<numpy.flatiter object at 0x7fee2332ff20>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fee2342c230>
<numpy.flatiter object at 0x7fee2342c230>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee2342c230>
<numpy.flatiter object at 0x7fee2342c230>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee2342c230>
<numpy.flatiter object at 0x7fee2342c230>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee2342c230>
<numpy.flatiter object at 0x7fee2342c230>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
[2025-06-03 01:34:15,836][__main__][INFO] - Saved best model at epoch 3 with accuracy: 0.6832
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-112.375, -4.0, 112.375, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-112.375, -4.0, 112.375, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
[2025-06-03 01:34:15,939][tensorboardX.summary][INFO] - Summary name Validation/Confusion Matrix is illegal; using Validation/Confusion_Matrix instead.
Epoch 4 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:34:16,145][__main__][INFO] - Train Epoch: 4 [0/101 (0%)]	Loss: 1.162746
Epoch 4 [Train]:   1%|          | 1/101 [00:00<00:17,  5.74it/s]Epoch 4 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.76it/s]Epoch 4 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.73it/s]Epoch 4 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.74it/s]Epoch 4 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.74it/s]Epoch 4 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.74it/s]Epoch 4 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.73it/s]Epoch 4 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.71it/s]Epoch 4 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.71it/s]Epoch 4 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.72it/s]Epoch 4 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.72it/s]Epoch 4 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.73it/s]Epoch 4 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.73it/s]Epoch 4 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.74it/s]Epoch 4 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.73it/s]Epoch 4 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.72it/s]Epoch 4 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.74it/s]Epoch 4 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.73it/s]Epoch 4 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.73it/s]Epoch 4 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.74it/s]Epoch 4 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:14,  5.71it/s]Epoch 4 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 4 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.72it/s]Epoch 4 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.72it/s]Epoch 4 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.73it/s]Epoch 4 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.73it/s]Epoch 4 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.74it/s]Epoch 4 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.74it/s]Epoch 4 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.74it/s]Epoch 4 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.75it/s]Epoch 4 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.73it/s]Epoch 4 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.74it/s]Epoch 4 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.75it/s]Epoch 4 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 4 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.74it/s]Epoch 4 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.73it/s]Epoch 4 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.71it/s]Epoch 4 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.72it/s]Epoch 4 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.72it/s]Epoch 4 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.74it/s]Epoch 4 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.73it/s]Epoch 4 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.73it/s]Epoch 4 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 4 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.72it/s]Epoch 4 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.72it/s]Epoch 4 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.73it/s]Epoch 4 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.64it/s]Epoch 4 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.58it/s]Epoch 4 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.58it/s]Epoch 4 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:09,  5.62it/s]Epoch 4 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.66it/s]Epoch 4 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.69it/s]Epoch 4 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.70it/s]Epoch 4 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.70it/s]Epoch 4 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.72it/s]Epoch 4 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.73it/s]Epoch 4 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.73it/s]Epoch 4 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.69it/s]Epoch 4 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.59it/s]Epoch 4 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.62it/s]Epoch 4 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.65it/s]Epoch 4 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.67it/s]Epoch 4 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.70it/s]Epoch 4 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.71it/s]Epoch 4 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.72it/s]Epoch 4 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.72it/s]Epoch 4 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.73it/s]Epoch 4 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.72it/s]Epoch 4 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.72it/s]Epoch 4 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.73it/s]Epoch 4 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.72it/s]Epoch 4 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.72it/s]Epoch 4 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.73it/s]Epoch 4 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.71it/s]Epoch 4 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 4 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.73it/s]Epoch 4 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.74it/s]Epoch 4 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.74it/s]Epoch 4 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.73it/s]Epoch 4 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.73it/s]Epoch 4 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.73it/s]Epoch 4 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.73it/s]Epoch 4 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.74it/s]Epoch 4 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.72it/s]Epoch 4 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.72it/s]Epoch 4 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.73it/s]Epoch 4 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.72it/s]Epoch 4 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.72it/s]Epoch 4 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.73it/s]Epoch 4 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.73it/s]Epoch 4 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.74it/s]Epoch 4 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.74it/s]Epoch 4 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.73it/s]Epoch 4 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.74it/s]Epoch 4 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.74it/s]Epoch 4 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.72it/s]Epoch 4 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.72it/s]Epoch 4 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.72it/s]Epoch 4 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.72it/s]Epoch 4 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.73it/s][2025-06-03 01:34:33,601][__main__][INFO] - Train Epoch: 4 [100/101 (99%)]	Loss: 0.457361
Epoch 4 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.11it/s]                                                                  Epoch 4 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 4 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.37it/s]Epoch 4 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.34it/s]Epoch 4 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.34it/s]Epoch 4 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.30it/s]Epoch 4 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.28it/s]Epoch 4 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.29it/s]Epoch 4 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.30it/s]Epoch 4 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.28it/s]Epoch 4 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.29it/s]Epoch 4 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.29it/s]Epoch 4 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.27it/s]Epoch 4 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.25it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:34:35,085][__main__][INFO] - Epoch 4: Val Loss: 0.9374, Accuracy: 0.6535, Metrics: {'accuracy': 0.6534653465346535, 'f1_macro': 0.3782894736842105, 'f1_weighted': 0.5337415320479417, 'precision_macro': 0.3773521505376344, 'recall_macro': 0.40909090909090906}
Epoch 5 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:34:35,262][__main__][INFO] - Train Epoch: 5 [0/101 (0%)]	Loss: 1.104674
Epoch 5 [Train]:   1%|          | 1/101 [00:00<00:17,  5.72it/s]Epoch 5 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.74it/s]Epoch 5 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.74it/s]Epoch 5 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.74it/s]Epoch 5 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.72it/s]Epoch 5 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.73it/s]Epoch 5 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.73it/s]Epoch 5 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.73it/s]Epoch 5 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.73it/s]Epoch 5 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.73it/s]Epoch 5 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.74it/s]Epoch 5 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.74it/s]Epoch 5 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.75it/s]Epoch 5 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.75it/s]Epoch 5 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.74it/s]Epoch 5 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.74it/s]Epoch 5 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.73it/s]Epoch 5 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.72it/s]Epoch 5 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.72it/s]Epoch 5 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.71it/s]Epoch 5 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 5 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.73it/s]Epoch 5 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.73it/s]Epoch 5 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.74it/s]Epoch 5 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.74it/s]Epoch 5 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.74it/s]Epoch 5 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.75it/s]Epoch 5 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.75it/s]Epoch 5 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.75it/s]Epoch 5 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.75it/s]Epoch 5 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.76it/s]Epoch 5 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.75it/s]Epoch 5 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.74it/s]Epoch 5 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 5 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.74it/s]Epoch 5 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.74it/s]Epoch 5 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.73it/s]Epoch 5 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.74it/s]Epoch 5 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.74it/s]Epoch 5 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.74it/s]Epoch 5 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.75it/s]Epoch 5 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.73it/s]Epoch 5 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.75it/s]Epoch 5 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.75it/s]Epoch 5 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.75it/s]Epoch 5 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.76it/s]Epoch 5 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.76it/s]Epoch 5 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.76it/s]Epoch 5 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.75it/s]Epoch 5 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.75it/s]Epoch 5 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.75it/s]Epoch 5 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.75it/s]Epoch 5 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.75it/s]Epoch 5 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.74it/s]Epoch 5 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.74it/s]Epoch 5 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 5 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.72it/s]Epoch 5 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.73it/s]Epoch 5 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.73it/s]Epoch 5 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.75it/s]Epoch 5 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.75it/s]Epoch 5 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.75it/s]Epoch 5 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.75it/s]Epoch 5 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.75it/s]Epoch 5 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.76it/s]Epoch 5 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.75it/s]Epoch 5 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.76it/s]Epoch 5 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.74it/s]Epoch 5 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.74it/s]Epoch 5 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.75it/s]Epoch 5 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.75it/s]Epoch 5 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.76it/s]Epoch 5 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.75it/s]Epoch 5 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.76it/s]Epoch 5 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.75it/s]Epoch 5 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.76it/s]Epoch 5 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.75it/s]Epoch 5 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.68it/s]Epoch 5 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.71it/s]Epoch 5 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.72it/s]Epoch 5 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.72it/s]Epoch 5 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.74it/s]Epoch 5 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.74it/s]Epoch 5 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.76it/s]Epoch 5 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.74it/s]Epoch 5 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.75it/s]Epoch 5 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.75it/s]Epoch 5 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.75it/s]Epoch 5 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.77it/s]Epoch 5 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.75it/s]Epoch 5 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.76it/s]Epoch 5 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.76it/s]Epoch 5 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.76it/s]Epoch 5 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.76it/s]Epoch 5 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.76it/s]Epoch 5 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.76it/s]Epoch 5 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.74it/s]Epoch 5 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 5 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.73it/s]Epoch 5 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.74it/s][2025-06-03 01:34:52,638][__main__][INFO] - Train Epoch: 5 [100/101 (99%)]	Loss: 1.301963
Epoch 5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.13it/s]                                                                  Epoch 5 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 5 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.37it/s]Epoch 5 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.35it/s]Epoch 5 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.34it/s]Epoch 5 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.30it/s]Epoch 5 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.28it/s]Epoch 5 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.28it/s]Epoch 5 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.28it/s]Epoch 5 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.25it/s]Epoch 5 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.21it/s]Epoch 5 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.22it/s]Epoch 5 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.25it/s]Epoch 5 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.24it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:34:54,124][__main__][INFO] - Epoch 5: Val Loss: 0.9702, Accuracy: 0.6238, Metrics: {'accuracy': 0.6237623762376238, 'f1_macro': 0.2803817803817804, 'f1_weighted': 0.5299153615985299, 'precision_macro': 0.25787815126050423, 'recall_macro': 0.31652542372881354}
Epoch 6 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:34:54,308][__main__][INFO] - Train Epoch: 6 [0/101 (0%)]	Loss: 0.880728
Epoch 6 [Train]:   1%|          | 1/101 [00:00<00:18,  5.50it/s]Epoch 6 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.63it/s]Epoch 6 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.70it/s]Epoch 6 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.72it/s]Epoch 6 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.73it/s]Epoch 6 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.73it/s]Epoch 6 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.73it/s]Epoch 6 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.74it/s]Epoch 6 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.75it/s]Epoch 6 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.76it/s]Epoch 6 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.75it/s]Epoch 6 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.76it/s]Epoch 6 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.75it/s]Epoch 6 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.75it/s]Epoch 6 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.75it/s]Epoch 6 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.74it/s]Epoch 6 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.74it/s]Epoch 6 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.69it/s]Epoch 6 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.69it/s]Epoch 6 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.71it/s]Epoch 6 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:14,  5.71it/s]Epoch 6 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 6 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.74it/s]Epoch 6 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.73it/s]Epoch 6 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.74it/s]Epoch 6 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.74it/s]Epoch 6 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.75it/s]Epoch 6 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.75it/s]Epoch 6 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.75it/s]Epoch 6 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.76it/s]Epoch 6 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.75it/s]Epoch 6 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.74it/s]Epoch 6 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.74it/s]Epoch 6 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 6 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.76it/s]Epoch 6 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.75it/s]Epoch 6 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.76it/s]Epoch 6 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.74it/s]Epoch 6 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.75it/s]Epoch 6 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.76it/s]Epoch 6 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.76it/s]Epoch 6 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.76it/s]Epoch 6 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.76it/s]Epoch 6 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.76it/s]Epoch 6 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.76it/s]Epoch 6 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.74it/s]Epoch 6 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.75it/s]Epoch 6 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.76it/s]Epoch 6 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.75it/s]Epoch 6 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.75it/s]Epoch 6 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.76it/s]Epoch 6 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.75it/s]Epoch 6 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.76it/s]Epoch 6 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.76it/s]Epoch 6 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.75it/s]Epoch 6 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.76it/s]Epoch 6 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.76it/s]Epoch 6 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.76it/s]Epoch 6 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.75it/s]Epoch 6 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.75it/s]Epoch 6 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.75it/s]Epoch 6 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.75it/s]Epoch 6 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.76it/s]Epoch 6 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.75it/s]Epoch 6 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.76it/s]Epoch 6 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.75it/s]Epoch 6 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.75it/s]Epoch 6 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.75it/s]Epoch 6 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.76it/s]Epoch 6 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.76it/s]Epoch 6 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.73it/s]Epoch 6 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.73it/s]Epoch 6 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.74it/s]Epoch 6 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.74it/s]Epoch 6 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.75it/s]Epoch 6 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.74it/s]Epoch 6 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.75it/s]Epoch 6 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.74it/s]Epoch 6 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.74it/s]Epoch 6 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.75it/s]Epoch 6 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.75it/s]Epoch 6 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.76it/s]Epoch 6 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.75it/s]Epoch 6 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.75it/s]Epoch 6 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.75it/s]Epoch 6 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.76it/s]Epoch 6 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.77it/s]Epoch 6 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.76it/s]Epoch 6 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.75it/s]Epoch 6 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.74it/s]Epoch 6 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.75it/s]Epoch 6 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.74it/s]Epoch 6 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.75it/s]Epoch 6 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.76it/s]Epoch 6 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.74it/s]Epoch 6 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.75it/s]Epoch 6 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.75it/s]Epoch 6 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 6 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.75it/s]Epoch 6 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.75it/s][2025-06-03 01:35:11,669][__main__][INFO] - Train Epoch: 6 [100/101 (99%)]	Loss: 0.094511
Epoch 6 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.14it/s]                                                                  Epoch 6 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 6 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.10it/s]Epoch 6 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 16.14it/s]Epoch 6 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 16.24it/s]Epoch 6 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 16.51it/s]Epoch 6 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 16.76it/s]Epoch 6 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 16.94it/s]Epoch 6 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.06it/s]Epoch 6 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.11it/s]Epoch 6 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.16it/s]Epoch 6 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.19it/s]Epoch 6 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.22it/s]Epoch 6 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.22it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:35:13,179][__main__][INFO] - Epoch 6: Val Loss: 1.2694, Accuracy: 0.6238, Metrics: {'accuracy': 0.6237623762376238, 'f1_macro': 0.4389803921568627, 'f1_weighted': 0.5905587264608815, 'precision_macro': 0.5248044965786901, 'recall_macro': 0.44006163328197223}
Epoch 7 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:35:13,355][__main__][INFO] - Train Epoch: 7 [0/101 (0%)]	Loss: 1.318193
Epoch 7 [Train]:   1%|          | 1/101 [00:00<00:17,  5.74it/s]Epoch 7 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.77it/s]Epoch 7 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.74it/s]Epoch 7 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.76it/s]Epoch 7 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.76it/s]Epoch 7 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.76it/s]Epoch 7 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.75it/s]Epoch 7 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.77it/s]Epoch 7 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:15,  5.76it/s]Epoch 7 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.75it/s]Epoch 7 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.76it/s]Epoch 7 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.75it/s]Epoch 7 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.76it/s]Epoch 7 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.76it/s]Epoch 7 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.76it/s]Epoch 7 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.77it/s]Epoch 7 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.77it/s]Epoch 7 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.77it/s]Epoch 7 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.76it/s]Epoch 7 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.77it/s]Epoch 7 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.75it/s]Epoch 7 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.75it/s]Epoch 7 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.75it/s]Epoch 7 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.73it/s]Epoch 7 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.74it/s]Epoch 7 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.74it/s]Epoch 7 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.75it/s]Epoch 7 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.75it/s]Epoch 7 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.76it/s]Epoch 7 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.76it/s]Epoch 7 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.75it/s]Epoch 7 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.76it/s]Epoch 7 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.75it/s]Epoch 7 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 7 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.75it/s]Epoch 7 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.75it/s]Epoch 7 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.73it/s]Epoch 7 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.72it/s]Epoch 7 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.70it/s]Epoch 7 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.72it/s]Epoch 7 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.73it/s]Epoch 7 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.74it/s]Epoch 7 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.74it/s]Epoch 7 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.75it/s]Epoch 7 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.75it/s]Epoch 7 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.76it/s]Epoch 7 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.76it/s]Epoch 7 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.75it/s]Epoch 7 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.76it/s]Epoch 7 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.74it/s]Epoch 7 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.75it/s]Epoch 7 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.75it/s]Epoch 7 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.73it/s]Epoch 7 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.74it/s]Epoch 7 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.73it/s]Epoch 7 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 7 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.74it/s]Epoch 7 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.74it/s]Epoch 7 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.73it/s]Epoch 7 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.72it/s]Epoch 7 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.70it/s]Epoch 7 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.72it/s]Epoch 7 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.70it/s]Epoch 7 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.71it/s]Epoch 7 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.73it/s]Epoch 7 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.71it/s]Epoch 7 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.72it/s]Epoch 7 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.72it/s]Epoch 7 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.73it/s]Epoch 7 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.74it/s]Epoch 7 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.74it/s]Epoch 7 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.75it/s]Epoch 7 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.74it/s]Epoch 7 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.75it/s]Epoch 7 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.73it/s]Epoch 7 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.71it/s]Epoch 7 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.73it/s]Epoch 7 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.73it/s]Epoch 7 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.74it/s]Epoch 7 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.74it/s]Epoch 7 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.74it/s]Epoch 7 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.74it/s]Epoch 7 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.73it/s]Epoch 7 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.75it/s]Epoch 7 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.74it/s]Epoch 7 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.74it/s]Epoch 7 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.74it/s]Epoch 7 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.74it/s]Epoch 7 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.75it/s]Epoch 7 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.72it/s]Epoch 7 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.73it/s]Epoch 7 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.74it/s]Epoch 7 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.74it/s]Epoch 7 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.75it/s]Epoch 7 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.74it/s]Epoch 7 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.73it/s]Epoch 7 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.66it/s]Epoch 7 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.69it/s]Epoch 7 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.69it/s]Epoch 7 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.70it/s][2025-06-03 01:35:30,745][__main__][INFO] - Train Epoch: 7 [100/101 (99%)]	Loss: 0.395493
Epoch 7 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.09it/s]                                                                  Epoch 7 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 7 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 16.97it/s]Epoch 7 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.15it/s]Epoch 7 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.23it/s]Epoch 7 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.25it/s]Epoch 7 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.23it/s]Epoch 7 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.26it/s]Epoch 7 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.28it/s]Epoch 7 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.27it/s]Epoch 7 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.29it/s]Epoch 7 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.29it/s]Epoch 7 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.29it/s]Epoch 7 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.27it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:35:32,235][__main__][INFO] - Epoch 7: Val Loss: 0.8462, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.5219721070728494, 'f1_weighted': 0.629355088212957, 'precision_macro': 0.49961756993007, 'recall_macro': 0.5494607087827427}
Epoch 8 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:35:32,410][__main__][INFO] - Train Epoch: 8 [0/101 (0%)]	Loss: 0.668432
Epoch 8 [Train]:   1%|          | 1/101 [00:00<00:17,  5.76it/s]Epoch 8 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.76it/s]Epoch 8 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.77it/s]Epoch 8 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.77it/s]Epoch 8 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.76it/s]Epoch 8 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.67it/s]Epoch 8 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.70it/s]Epoch 8 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.68it/s]Epoch 8 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.69it/s]Epoch 8 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.71it/s]Epoch 8 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.71it/s]Epoch 8 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.72it/s]Epoch 8 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.72it/s]Epoch 8 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.72it/s]Epoch 8 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.73it/s]Epoch 8 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.72it/s]Epoch 8 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.73it/s]Epoch 8 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.73it/s]Epoch 8 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.74it/s]Epoch 8 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.75it/s]Epoch 8 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 8 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.74it/s]Epoch 8 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.74it/s]Epoch 8 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.74it/s]Epoch 8 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.75it/s]Epoch 8 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.75it/s]Epoch 8 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.76it/s]Epoch 8 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.73it/s]Epoch 8 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.72it/s]Epoch 8 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.73it/s]Epoch 8 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.74it/s]Epoch 8 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.75it/s]Epoch 8 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.75it/s]Epoch 8 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 8 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.75it/s]Epoch 8 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.75it/s]Epoch 8 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.75it/s]Epoch 8 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.75it/s]Epoch 8 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.76it/s]Epoch 8 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.76it/s]Epoch 8 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.74it/s]Epoch 8 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.74it/s]Epoch 8 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.74it/s]Epoch 8 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.75it/s]Epoch 8 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.73it/s]Epoch 8 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.72it/s]Epoch 8 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.74it/s]Epoch 8 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.74it/s]Epoch 8 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.75it/s]Epoch 8 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.74it/s]Epoch 8 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.74it/s]Epoch 8 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.74it/s]Epoch 8 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.73it/s]Epoch 8 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.72it/s]Epoch 8 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.72it/s]Epoch 8 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.73it/s]Epoch 8 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.73it/s]Epoch 8 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.71it/s]Epoch 8 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.73it/s]Epoch 8 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.74it/s]Epoch 8 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.75it/s]Epoch 8 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.76it/s]Epoch 8 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.76it/s]Epoch 8 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.76it/s]Epoch 8 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.75it/s]Epoch 8 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.75it/s]Epoch 8 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.75it/s]Epoch 8 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.75it/s]Epoch 8 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.75it/s]Epoch 8 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.75it/s]Epoch 8 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.75it/s]Epoch 8 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.75it/s]Epoch 8 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.75it/s]Epoch 8 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.75it/s]Epoch 8 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.75it/s]Epoch 8 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.76it/s]Epoch 8 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.75it/s]Epoch 8 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:03,  5.76it/s]Epoch 8 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.75it/s]Epoch 8 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.73it/s]Epoch 8 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.74it/s]Epoch 8 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.74it/s]Epoch 8 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.74it/s]Epoch 8 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.75it/s]Epoch 8 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.75it/s]Epoch 8 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.75it/s]Epoch 8 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.74it/s]Epoch 8 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.75it/s]Epoch 8 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.75it/s]Epoch 8 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.75it/s]Epoch 8 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.75it/s]Epoch 8 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.75it/s]Epoch 8 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.76it/s]Epoch 8 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.75it/s]Epoch 8 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.76it/s]Epoch 8 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.76it/s]Epoch 8 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.75it/s]Epoch 8 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 8 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.75it/s]Epoch 8 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.74it/s][2025-06-03 01:35:49,793][__main__][INFO] - Train Epoch: 8 [100/101 (99%)]	Loss: 1.174443
Epoch 8 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.12it/s]                                                                  Epoch 8 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 8 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.44it/s]Epoch 8 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.41it/s]Epoch 8 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.35it/s]Epoch 8 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.32it/s]Epoch 8 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.33it/s]Epoch 8 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.30it/s]Epoch 8 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.29it/s]Epoch 8 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.31it/s]Epoch 8 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.33it/s]Epoch 8 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.30it/s]Epoch 8 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.24it/s]Epoch 8 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.26it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:35:51,276][__main__][INFO] - Epoch 8: Val Loss: 0.8514, Accuracy: 0.6634, Metrics: {'accuracy': 0.6633663366336634, 'f1_macro': 0.4173581452104942, 'f1_weighted': 0.5599115616259613, 'precision_macro': 0.5083333333333333, 'recall_macro': 0.4400808936825886}
Epoch 9 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:35:51,453][__main__][INFO] - Train Epoch: 9 [0/101 (0%)]	Loss: 1.117917
Epoch 9 [Train]:   1%|          | 1/101 [00:00<00:17,  5.74it/s]Epoch 9 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.77it/s]Epoch 9 [Train]:   3%|‚ñé         | 3/101 [00:00<00:16,  5.77it/s]Epoch 9 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.76it/s]Epoch 9 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.76it/s]Epoch 9 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.74it/s]Epoch 9 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.74it/s]Epoch 9 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.75it/s]Epoch 9 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.74it/s]Epoch 9 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.75it/s]Epoch 9 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.75it/s]Epoch 9 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.76it/s]Epoch 9 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.76it/s]Epoch 9 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.75it/s]Epoch 9 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.76it/s]Epoch 9 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.77it/s]Epoch 9 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.77it/s]Epoch 9 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.76it/s]Epoch 9 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.76it/s]Epoch 9 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.75it/s]Epoch 9 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.76it/s]Epoch 9 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.76it/s]Epoch 9 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:03<00:13,  5.74it/s]Epoch 9 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.76it/s]Epoch 9 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.75it/s]Epoch 9 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.75it/s]Epoch 9 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.75it/s]Epoch 9 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.75it/s]Epoch 9 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.74it/s]Epoch 9 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.74it/s]Epoch 9 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.75it/s]Epoch 9 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.73it/s]Epoch 9 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.74it/s]Epoch 9 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.75it/s]Epoch 9 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.75it/s]Epoch 9 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.76it/s]Epoch 9 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.75it/s]Epoch 9 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.75it/s]Epoch 9 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.75it/s]Epoch 9 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.76it/s]Epoch 9 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.76it/s]Epoch 9 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.75it/s]Epoch 9 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.77it/s]Epoch 9 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.75it/s]Epoch 9 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.75it/s]Epoch 9 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:07<00:09,  5.75it/s]Epoch 9 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.75it/s]Epoch 9 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.75it/s]Epoch 9 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.74it/s]Epoch 9 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.75it/s]Epoch 9 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.74it/s]Epoch 9 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.74it/s]Epoch 9 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.73it/s]Epoch 9 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.74it/s]Epoch 9 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:07,  5.75it/s]Epoch 9 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 9 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.74it/s]Epoch 9 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.69it/s]Epoch 9 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.67it/s]Epoch 9 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.68it/s]Epoch 9 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.70it/s]Epoch 9 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.71it/s]Epoch 9 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.62it/s]Epoch 9 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.65it/s]Epoch 9 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.69it/s]Epoch 9 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.71it/s]Epoch 9 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.73it/s]Epoch 9 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.73it/s]Epoch 9 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.63it/s]Epoch 9 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.67it/s]Epoch 9 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.68it/s]Epoch 9 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.70it/s]Epoch 9 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.73it/s]Epoch 9 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.73it/s]Epoch 9 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.73it/s]Epoch 9 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.74it/s]Epoch 9 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.75it/s]Epoch 9 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.75it/s]Epoch 9 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.75it/s]Epoch 9 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.76it/s]Epoch 9 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.75it/s]Epoch 9 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.76it/s]Epoch 9 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.76it/s]Epoch 9 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.76it/s]Epoch 9 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.75it/s]Epoch 9 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.75it/s]Epoch 9 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.76it/s]Epoch 9 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.76it/s]Epoch 9 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.75it/s]Epoch 9 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.73it/s]Epoch 9 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.74it/s]Epoch 9 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.74it/s]Epoch 9 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.74it/s]Epoch 9 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.75it/s]Epoch 9 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.74it/s]Epoch 9 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.75it/s]Epoch 9 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.74it/s]Epoch 9 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 9 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.76it/s]Epoch 9 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.75it/s][2025-06-03 01:36:08,838][__main__][INFO] - Train Epoch: 9 [100/101 (99%)]	Loss: 0.636137
Epoch 9 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.13it/s]                                                                  Epoch 9 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 9 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.39it/s]Epoch 9 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.30it/s]Epoch 9 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.31it/s]Epoch 9 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.29it/s]Epoch 9 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.27it/s]Epoch 9 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.28it/s]Epoch 9 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.30it/s]Epoch 9 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.28it/s]Epoch 9 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.27it/s]Epoch 9 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.26it/s]Epoch 9 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.27it/s]Epoch 9 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.22it/s]                                                              /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:36:10,324][__main__][INFO] - Epoch 9: Val Loss: 0.8989, Accuracy: 0.6139, Metrics: {'accuracy': 0.6138613861386139, 'f1_macro': 0.4418938307030129, 'f1_weighted': 0.5864170348168245, 'precision_macro': 0.47673051075268824, 'recall_macro': 0.4460516178736518}
Epoch 10 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:36:10,501][__main__][INFO] - Train Epoch: 10 [0/101 (0%)]	Loss: 0.633568
Epoch 10 [Train]:   1%|          | 1/101 [00:00<00:17,  5.74it/s]Epoch 10 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.77it/s]Epoch 10 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.75it/s]Epoch 10 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.76it/s]Epoch 10 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.75it/s]Epoch 10 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.73it/s]Epoch 10 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.73it/s]Epoch 10 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.70it/s]Epoch 10 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.72it/s]Epoch 10 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.73it/s]Epoch 10 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.73it/s]Epoch 10 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.73it/s]Epoch 10 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.72it/s]Epoch 10 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.72it/s]Epoch 10 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.73it/s]Epoch 10 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.73it/s]Epoch 10 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.74it/s]Epoch 10 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.74it/s]Epoch 10 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.74it/s]Epoch 10 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.75it/s]Epoch 10 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.76it/s]Epoch 10 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.76it/s]Epoch 10 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.75it/s]Epoch 10 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.75it/s]Epoch 10 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.75it/s]Epoch 10 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.74it/s]Epoch 10 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.74it/s]Epoch 10 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.74it/s]Epoch 10 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.75it/s]Epoch 10 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.75it/s]Epoch 10 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.75it/s]Epoch 10 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:11,  5.75it/s]Epoch 10 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.76it/s]Epoch 10 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.76it/s]Epoch 10 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.75it/s]Epoch 10 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.76it/s]Epoch 10 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.75it/s]Epoch 10 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:10,  5.75it/s]Epoch 10 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.74it/s]Epoch 10 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:06<00:10,  5.75it/s]Epoch 10 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.76it/s]Epoch 10 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.75it/s]Epoch 10 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.75it/s]Epoch 10 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.74it/s]Epoch 10 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.74it/s]Epoch 10 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.74it/s]Epoch 10 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.74it/s]Epoch 10 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.75it/s]Epoch 10 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.74it/s]Epoch 10 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.74it/s]Epoch 10 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.75it/s]Epoch 10 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.68it/s]Epoch 10 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.70it/s]Epoch 10 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.72it/s]Epoch 10 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.72it/s]Epoch 10 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.74it/s]Epoch 10 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.74it/s]Epoch 10 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.75it/s]Epoch 10 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.75it/s]Epoch 10 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.74it/s]Epoch 10 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.74it/s]Epoch 10 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.73it/s]Epoch 10 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:10<00:06,  5.74it/s]Epoch 10 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.74it/s]Epoch 10 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.74it/s]Epoch 10 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.74it/s]Epoch 10 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.73it/s]Epoch 10 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.72it/s]Epoch 10 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.71it/s]Epoch 10 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.72it/s]Epoch 10 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.74it/s]Epoch 10 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.73it/s]Epoch 10 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.74it/s]Epoch 10 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.74it/s]Epoch 10 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 10 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.73it/s]Epoch 10 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.73it/s]Epoch 10 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.74it/s]Epoch 10 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.74it/s]Epoch 10 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:13<00:03,  5.74it/s]Epoch 10 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.75it/s]Epoch 10 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.74it/s]Epoch 10 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.74it/s]Epoch 10 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.75it/s]Epoch 10 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.76it/s]Epoch 10 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:14<00:02,  5.76it/s]Epoch 10 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.75it/s]Epoch 10 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.75it/s]Epoch 10 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.75it/s]Epoch 10 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.75it/s]Epoch 10 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.75it/s]Epoch 10 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.75it/s]Epoch 10 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.75it/s]Epoch 10 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.74it/s]Epoch 10 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.75it/s]Epoch 10 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.74it/s]Epoch 10 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.74it/s]Epoch 10 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.75it/s]Epoch 10 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.74it/s]Epoch 10 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.74it/s][2025-06-03 01:36:27,885][__main__][INFO] - Train Epoch: 10 [100/101 (99%)]	Loss: 0.475414
Epoch 10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.12it/s]                                                                   Epoch 10 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 10 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.38it/s]Epoch 10 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.39it/s]Epoch 10 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.36it/s]Epoch 10 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.33it/s]Epoch 10 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.33it/s]Epoch 10 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.33it/s]Epoch 10 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.29it/s]Epoch 10 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.30it/s]Epoch 10 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.34it/s]Epoch 10 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.28it/s]Epoch 10 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.29it/s]Epoch 10 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.30it/s]                                                               [2025-06-03 01:36:29,367][__main__][INFO] - Epoch 10: Val Loss: 0.9974, Accuracy: 0.5842, Metrics: {'accuracy': 0.5841584158415841, 'f1_macro': 0.5701016040129963, 'f1_weighted': 0.6007166955156262, 'precision_macro': 0.5892827733860342, 'recall_macro': 0.5914869029275809}
Epoch 11 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:36:29,555][__main__][INFO] - Train Epoch: 11 [0/101 (0%)]	Loss: 0.569528
Epoch 11 [Train]:   1%|          | 1/101 [00:00<00:18,  5.38it/s]Epoch 11 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.60it/s]Epoch 11 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.65it/s]Epoch 11 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.70it/s]Epoch 11 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.71it/s]Epoch 11 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.72it/s]Epoch 11 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.73it/s]Epoch 11 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.72it/s]Epoch 11 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.73it/s]Epoch 11 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.73it/s]Epoch 11 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.73it/s]Epoch 11 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.74it/s]Epoch 11 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.73it/s]Epoch 11 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.75it/s]Epoch 11 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:14,  5.74it/s]Epoch 11 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.73it/s]Epoch 11 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.74it/s]Epoch 11 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.74it/s]Epoch 11 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.74it/s]Epoch 11 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.72it/s]Epoch 11 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 11 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 11 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.70it/s]Epoch 11 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.71it/s]Epoch 11 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.71it/s]Epoch 11 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.70it/s]Epoch 11 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.71it/s]Epoch 11 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.72it/s]Epoch 11 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.73it/s]Epoch 11 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.73it/s]Epoch 11 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.71it/s]Epoch 11 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.72it/s]Epoch 11 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.72it/s]Epoch 11 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.71it/s]Epoch 11 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.55it/s]Epoch 11 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.52it/s]Epoch 11 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.59it/s]Epoch 11 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.57it/s]Epoch 11 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.64it/s]Epoch 11 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.67it/s]Epoch 11 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.69it/s]Epoch 11 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.71it/s]Epoch 11 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 11 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.73it/s]Epoch 11 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.73it/s]Epoch 11 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.74it/s]Epoch 11 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.74it/s]Epoch 11 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.74it/s]Epoch 11 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.74it/s]Epoch 11 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.73it/s]Epoch 11 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.74it/s]Epoch 11 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.74it/s]Epoch 11 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.74it/s]Epoch 11 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.75it/s]Epoch 11 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.74it/s]Epoch 11 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.75it/s]Epoch 11 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.74it/s]Epoch 11 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.74it/s]Epoch 11 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.74it/s]Epoch 11 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.74it/s]Epoch 11 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.74it/s]Epoch 11 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.73it/s]Epoch 11 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.73it/s]Epoch 11 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.74it/s]Epoch 11 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.72it/s]Epoch 11 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.70it/s]Epoch 11 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.71it/s]Epoch 11 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.71it/s]Epoch 11 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.73it/s]Epoch 11 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.72it/s]Epoch 11 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.72it/s]Epoch 11 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.73it/s]Epoch 11 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.72it/s]Epoch 11 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.71it/s]Epoch 11 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 11 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.72it/s]Epoch 11 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.74it/s]Epoch 11 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.72it/s]Epoch 11 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 11 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.72it/s]Epoch 11 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.70it/s]Epoch 11 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.71it/s]Epoch 11 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.72it/s]Epoch 11 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.70it/s]Epoch 11 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.71it/s]Epoch 11 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.71it/s]Epoch 11 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.72it/s]Epoch 11 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.73it/s]Epoch 11 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.72it/s]Epoch 11 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.73it/s]Epoch 11 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.72it/s]Epoch 11 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.72it/s]Epoch 11 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.73it/s]Epoch 11 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.72it/s]Epoch 11 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.73it/s]Epoch 11 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.73it/s]Epoch 11 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.71it/s]Epoch 11 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.72it/s]Epoch 11 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.71it/s]Epoch 11 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.71it/s][2025-06-03 01:36:47,009][__main__][INFO] - Train Epoch: 11 [100/101 (99%)]	Loss: 0.627084
Epoch 11 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.09it/s]                                                                   Epoch 11 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 11 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.24it/s]Epoch 11 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.21it/s]Epoch 11 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.25it/s]Epoch 11 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.26it/s]Epoch 11 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.23it/s]Epoch 11 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.24it/s]Epoch 11 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.25it/s]Epoch 11 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.25it/s]Epoch 11 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.25it/s]Epoch 11 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.24it/s]Epoch 11 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.24it/s]Epoch 11 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.22it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:36:48,498][__main__][INFO] - Epoch 11: Val Loss: 0.8867, Accuracy: 0.6337, Metrics: {'accuracy': 0.6336633663366337, 'f1_macro': 0.4565749525616698, 'f1_weighted': 0.6019445018505646, 'precision_macro': 0.48910256410256414, 'recall_macro': 0.462788906009245}
Epoch 12 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:36:48,676][__main__][INFO] - Train Epoch: 12 [0/101 (0%)]	Loss: 0.452414
Epoch 12 [Train]:   1%|          | 1/101 [00:00<00:17,  5.70it/s]Epoch 12 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.72it/s]Epoch 12 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.73it/s]Epoch 12 [Train]:   4%|‚ñç         | 4/101 [00:00<00:16,  5.73it/s]Epoch 12 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.74it/s]Epoch 12 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.73it/s]Epoch 12 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.74it/s]Epoch 12 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.73it/s]Epoch 12 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.73it/s]Epoch 12 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.74it/s]Epoch 12 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.73it/s]Epoch 12 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.73it/s]Epoch 12 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.73it/s]Epoch 12 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.73it/s]Epoch 12 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.73it/s]Epoch 12 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.72it/s]Epoch 12 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.71it/s]Epoch 12 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.72it/s]Epoch 12 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.70it/s]Epoch 12 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.71it/s]Epoch 12 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:14,  5.70it/s]Epoch 12 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.65it/s]Epoch 12 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.66it/s]Epoch 12 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.69it/s]Epoch 12 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.70it/s]Epoch 12 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.71it/s]Epoch 12 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.70it/s]Epoch 12 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.68it/s]Epoch 12 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.65it/s]Epoch 12 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.66it/s]Epoch 12 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.68it/s]Epoch 12 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.69it/s]Epoch 12 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:12,  5.65it/s]Epoch 12 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.66it/s]Epoch 12 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.68it/s]Epoch 12 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.70it/s]Epoch 12 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.71it/s]Epoch 12 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.71it/s]Epoch 12 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.70it/s]Epoch 12 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.71it/s]Epoch 12 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.71it/s]Epoch 12 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.72it/s]Epoch 12 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 12 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.74it/s]Epoch 12 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.73it/s]Epoch 12 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.73it/s]Epoch 12 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.74it/s]Epoch 12 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.73it/s]Epoch 12 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.73it/s]Epoch 12 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.73it/s]Epoch 12 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.73it/s]Epoch 12 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.74it/s]Epoch 12 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.72it/s]Epoch 12 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.72it/s]Epoch 12 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.73it/s]Epoch 12 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.70it/s]Epoch 12 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.71it/s]Epoch 12 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.71it/s]Epoch 12 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.71it/s]Epoch 12 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.72it/s]Epoch 12 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:06,  5.72it/s]Epoch 12 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.73it/s]Epoch 12 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.72it/s]Epoch 12 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.72it/s]Epoch 12 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.72it/s]Epoch 12 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.71it/s]Epoch 12 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.69it/s]Epoch 12 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.70it/s]Epoch 12 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.71it/s]Epoch 12 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.71it/s]Epoch 12 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.72it/s]Epoch 12 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.72it/s]Epoch 12 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.71it/s]Epoch 12 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.73it/s]Epoch 12 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.72it/s]Epoch 12 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.71it/s]Epoch 12 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.72it/s]Epoch 12 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.71it/s]Epoch 12 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 12 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.72it/s]Epoch 12 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.72it/s]Epoch 12 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.73it/s]Epoch 12 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.73it/s]Epoch 12 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:02,  5.73it/s]Epoch 12 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.71it/s]Epoch 12 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.70it/s]Epoch 12 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.69it/s]Epoch 12 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.71it/s]Epoch 12 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.70it/s]Epoch 12 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.72it/s]Epoch 12 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.72it/s]Epoch 12 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.72it/s]Epoch 12 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.72it/s]Epoch 12 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.71it/s]Epoch 12 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.71it/s]Epoch 12 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.72it/s]Epoch 12 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.72it/s]Epoch 12 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.73it/s]Epoch 12 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.72it/s]Epoch 12 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.70it/s][2025-06-03 01:37:06,147][__main__][INFO] - Train Epoch: 12 [100/101 (99%)]	Loss: 1.062153
Epoch 12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.08it/s]                                                                   Epoch 12 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 12 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.27it/s]Epoch 12 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.19it/s]Epoch 12 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.18it/s]Epoch 12 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.20it/s]Epoch 12 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.19it/s]Epoch 12 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.19it/s]Epoch 12 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.18it/s]Epoch 12 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.21it/s]Epoch 12 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.21it/s]Epoch 12 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.17it/s]Epoch 12 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.18it/s]Epoch 12 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.20it/s]                                                               /home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
[2025-06-03 01:37:07,639][__main__][INFO] - Epoch 12: Val Loss: 0.8310, Accuracy: 0.6733, Metrics: {'accuracy': 0.6732673267326733, 'f1_macro': 0.5179133858267717, 'f1_weighted': 0.6369065252981991, 'precision_macro': 0.516952614379085, 'recall_macro': 0.5269453004622496}
Epoch 13 [Train]:   0%|          | 0/101 [00:00<?, ?it/s][2025-06-03 01:37:07,817][__main__][INFO] - Train Epoch: 13 [0/101 (0%)]	Loss: 0.613716
Epoch 13 [Train]:   1%|          | 1/101 [00:00<00:17,  5.69it/s]Epoch 13 [Train]:   2%|‚ñè         | 2/101 [00:00<00:17,  5.67it/s]Epoch 13 [Train]:   3%|‚ñé         | 3/101 [00:00<00:17,  5.68it/s]Epoch 13 [Train]:   4%|‚ñç         | 4/101 [00:00<00:17,  5.69it/s]Epoch 13 [Train]:   5%|‚ñç         | 5/101 [00:00<00:16,  5.68it/s]Epoch 13 [Train]:   6%|‚ñå         | 6/101 [00:01<00:16,  5.70it/s]Epoch 13 [Train]:   7%|‚ñã         | 7/101 [00:01<00:16,  5.66it/s]Epoch 13 [Train]:   8%|‚ñä         | 8/101 [00:01<00:16,  5.68it/s]Epoch 13 [Train]:   9%|‚ñâ         | 9/101 [00:01<00:16,  5.69it/s]Epoch 13 [Train]:  10%|‚ñâ         | 10/101 [00:01<00:15,  5.70it/s]Epoch 13 [Train]:  11%|‚ñà         | 11/101 [00:01<00:15,  5.69it/s]Epoch 13 [Train]:  12%|‚ñà‚ñè        | 12/101 [00:02<00:15,  5.70it/s]Epoch 13 [Train]:  13%|‚ñà‚ñé        | 13/101 [00:02<00:15,  5.71it/s]Epoch 13 [Train]:  14%|‚ñà‚ñç        | 14/101 [00:02<00:15,  5.71it/s]Epoch 13 [Train]:  15%|‚ñà‚ñç        | 15/101 [00:02<00:15,  5.72it/s]Epoch 13 [Train]:  16%|‚ñà‚ñå        | 16/101 [00:02<00:14,  5.71it/s]Epoch 13 [Train]:  17%|‚ñà‚ñã        | 17/101 [00:02<00:14,  5.71it/s]Epoch 13 [Train]:  18%|‚ñà‚ñä        | 18/101 [00:03<00:14,  5.73it/s]Epoch 13 [Train]:  19%|‚ñà‚ñâ        | 19/101 [00:03<00:14,  5.72it/s]Epoch 13 [Train]:  20%|‚ñà‚ñâ        | 20/101 [00:03<00:14,  5.72it/s]Epoch 13 [Train]:  21%|‚ñà‚ñà        | 21/101 [00:03<00:13,  5.73it/s]Epoch 13 [Train]:  22%|‚ñà‚ñà‚ñè       | 22/101 [00:03<00:13,  5.72it/s]Epoch 13 [Train]:  23%|‚ñà‚ñà‚ñé       | 23/101 [00:04<00:13,  5.73it/s]Epoch 13 [Train]:  24%|‚ñà‚ñà‚ñç       | 24/101 [00:04<00:13,  5.71it/s]Epoch 13 [Train]:  25%|‚ñà‚ñà‚ñç       | 25/101 [00:04<00:13,  5.69it/s]Epoch 13 [Train]:  26%|‚ñà‚ñà‚ñå       | 26/101 [00:04<00:13,  5.70it/s]Epoch 13 [Train]:  27%|‚ñà‚ñà‚ñã       | 27/101 [00:04<00:12,  5.71it/s]Epoch 13 [Train]:  28%|‚ñà‚ñà‚ñä       | 28/101 [00:04<00:12,  5.70it/s]Epoch 13 [Train]:  29%|‚ñà‚ñà‚ñä       | 29/101 [00:05<00:12,  5.72it/s]Epoch 13 [Train]:  30%|‚ñà‚ñà‚ñâ       | 30/101 [00:05<00:12,  5.71it/s]Epoch 13 [Train]:  31%|‚ñà‚ñà‚ñà       | 31/101 [00:05<00:12,  5.71it/s]Epoch 13 [Train]:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/101 [00:05<00:12,  5.72it/s]Epoch 13 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/101 [00:05<00:11,  5.71it/s]Epoch 13 [Train]:  34%|‚ñà‚ñà‚ñà‚ñé      | 34/101 [00:05<00:11,  5.71it/s]Epoch 13 [Train]:  35%|‚ñà‚ñà‚ñà‚ñç      | 35/101 [00:06<00:11,  5.72it/s]Epoch 13 [Train]:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/101 [00:06<00:11,  5.70it/s]Epoch 13 [Train]:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/101 [00:06<00:11,  5.70it/s]Epoch 13 [Train]:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/101 [00:06<00:11,  5.70it/s]Epoch 13 [Train]:  39%|‚ñà‚ñà‚ñà‚ñä      | 39/101 [00:06<00:10,  5.70it/s]Epoch 13 [Train]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 40/101 [00:07<00:10,  5.70it/s]Epoch 13 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/101 [00:07<00:10,  5.71it/s]Epoch 13 [Train]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/101 [00:07<00:10,  5.70it/s]Epoch 13 [Train]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/101 [00:07<00:10,  5.72it/s]Epoch 13 [Train]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 44/101 [00:07<00:09,  5.71it/s]Epoch 13 [Train]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 45/101 [00:07<00:09,  5.71it/s]Epoch 13 [Train]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/101 [00:08<00:09,  5.72it/s]Epoch 13 [Train]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/101 [00:08<00:09,  5.72it/s]Epoch 13 [Train]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/101 [00:08<00:09,  5.72it/s]Epoch 13 [Train]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 49/101 [00:08<00:09,  5.72it/s]Epoch 13 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 50/101 [00:08<00:08,  5.71it/s]Epoch 13 [Train]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/101 [00:08<00:08,  5.72it/s]Epoch 13 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/101 [00:09<00:08,  5.71it/s]Epoch 13 [Train]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 53/101 [00:09<00:08,  5.71it/s]Epoch 13 [Train]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 54/101 [00:09<00:08,  5.73it/s]Epoch 13 [Train]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 55/101 [00:09<00:08,  5.69it/s]Epoch 13 [Train]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/101 [00:09<00:07,  5.70it/s]Epoch 13 [Train]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/101 [00:09<00:07,  5.71it/s]Epoch 13 [Train]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 58/101 [00:10<00:07,  5.70it/s]Epoch 13 [Train]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 59/101 [00:10<00:07,  5.71it/s]Epoch 13 [Train]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 60/101 [00:10<00:07,  5.72it/s]Epoch 13 [Train]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/101 [00:10<00:07,  5.71it/s]Epoch 13 [Train]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/101 [00:10<00:06,  5.71it/s]Epoch 13 [Train]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 63/101 [00:11<00:06,  5.70it/s]Epoch 13 [Train]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64/101 [00:11<00:06,  5.70it/s]Epoch 13 [Train]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 65/101 [00:11<00:06,  5.71it/s]Epoch 13 [Train]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/101 [00:11<00:06,  5.72it/s]Epoch 13 [Train]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/101 [00:11<00:05,  5.69it/s]Epoch 13 [Train]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 68/101 [00:11<00:05,  5.71it/s]Epoch 13 [Train]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69/101 [00:12<00:05,  5.71it/s]Epoch 13 [Train]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 70/101 [00:12<00:05,  5.71it/s]Epoch 13 [Train]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/101 [00:12<00:05,  5.72it/s]Epoch 13 [Train]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/101 [00:12<00:05,  5.72it/s]Epoch 13 [Train]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [00:12<00:04,  5.72it/s]Epoch 13 [Train]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74/101 [00:12<00:04,  5.72it/s]Epoch 13 [Train]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 75/101 [00:13<00:04,  5.71it/s]Epoch 13 [Train]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/101 [00:13<00:04,  5.72it/s]Epoch 13 [Train]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 77/101 [00:13<00:04,  5.71it/s]Epoch 13 [Train]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 78/101 [00:13<00:04,  5.70it/s]Epoch 13 [Train]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79/101 [00:13<00:03,  5.72it/s]Epoch 13 [Train]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 80/101 [00:14<00:03,  5.71it/s]Epoch 13 [Train]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/101 [00:14<00:03,  5.72it/s]Epoch 13 [Train]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 82/101 [00:14<00:03,  5.73it/s]Epoch 13 [Train]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 83/101 [00:14<00:03,  5.72it/s]Epoch 13 [Train]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84/101 [00:14<00:03,  5.67it/s]Epoch 13 [Train]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 85/101 [00:14<00:02,  5.69it/s]Epoch 13 [Train]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/101 [00:15<00:02,  5.67it/s]Epoch 13 [Train]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 87/101 [00:15<00:02,  5.69it/s]Epoch 13 [Train]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 88/101 [00:15<00:02,  5.69it/s]Epoch 13 [Train]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89/101 [00:15<00:02,  5.69it/s]Epoch 13 [Train]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 90/101 [00:15<00:01,  5.68it/s]Epoch 13 [Train]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/101 [00:15<00:01,  5.69it/s]Epoch 13 [Train]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 92/101 [00:16<00:01,  5.69it/s]Epoch 13 [Train]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93/101 [00:16<00:01,  5.70it/s]Epoch 13 [Train]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 94/101 [00:16<00:01,  5.72it/s]Epoch 13 [Train]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 95/101 [00:16<00:01,  5.71it/s]Epoch 13 [Train]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/101 [00:16<00:00,  5.71it/s]Epoch 13 [Train]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97/101 [00:16<00:00,  5.71it/s]Epoch 13 [Train]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98/101 [00:17<00:00,  5.71it/s]Epoch 13 [Train]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99/101 [00:17<00:00,  5.72it/s]Epoch 13 [Train]:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 100/101 [00:17<00:00,  5.72it/s][2025-06-03 01:37:25,301][__main__][INFO] - Train Epoch: 13 [100/101 (99%)]	Loss: 0.672456
Epoch 13 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:17<00:00,  6.10it/s]                                                                   Epoch 13 [Val]:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 13 [Val]:   8%|‚ñä         | 2/26 [00:00<00:01, 17.33it/s]Epoch 13 [Val]:  15%|‚ñà‚ñå        | 4/26 [00:00<00:01, 17.20it/s]Epoch 13 [Val]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:01, 17.17it/s]Epoch 13 [Val]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:00<00:01, 17.17it/s]Epoch 13 [Val]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:00<00:00, 17.19it/s]Epoch 13 [Val]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 17.18it/s]Epoch 13 [Val]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 17.16it/s]Epoch 13 [Val]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 17.18it/s]Epoch 13 [Val]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:01<00:00, 17.21it/s]Epoch 13 [Val]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 17.16it/s]Epoch 13 [Val]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 17.16it/s]Epoch 13 [Val]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 17.17it/s]                                                               [2025-06-03 01:37:26,795][__main__][INFO] - Epoch 13: Val Loss: 0.8893, Accuracy: 0.6238, Metrics: {'accuracy': 0.6237623762376238, 'f1_macro': 0.6241766800178015, 'f1_weighted': 0.6362657360529109, 'precision_macro': 0.619188596491228, 'recall_macro': 0.6536787365177196}
[2025-06-03 01:37:26,796][__main__][INFO] - Early stopping triggered after 13 epochs
[2025-06-03 01:37:26,796][__main__][INFO] - Saving best confusion matrix with accuracy: 0.6832
(0, 0, 8, 6)
<numpy.flatiter object at 0x7fee233f7c30>
<numpy.flatiter object at 0x7fee233f7c30>
(0.125, 0.10999999999999999, 0.9, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
<numpy.flatiter object at 0x7fef7590c8d0>
<numpy.flatiter object at 0x7fef7590c8d0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fef7590c8d0>
<numpy.flatiter object at 0x7fef7590c8d0>
(0.125, 0.10999999999999999, 0.7450000000000001, 0.88)
<numpy.flatiter object at 0x7fee2329edf0>
<numpy.flatiter object at 0x7fee2329edf0>
(0.125, 0.10999999999999999, 0.9, 0.88)
<numpy.flatiter object at 0x7fee2329edf0>
<numpy.flatiter object at 0x7fee2329edf0>
(0.7837500000000002, 0.10999999999999999, 0.9000000000000001, 0.88)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 800.0, 600.0)
(0, 0, 1, 1)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0.0, 0.0, 1.0, 1.0)
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0, 0, 1, 1)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(100.0, 532.1666666666666, 100.0, 532.1666666666666)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-172.0, -4.0, 172.0, 14.0)
(100.0, 536.3333333333333, 100.0, 536.3333333333333)
(596.0000000000001, 536.3333333333333, 596.0000000000001, 536.3333333333333)
(0.0, 0.0, 1.0, 1.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.4375, -14.0, 4.4375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-4.375, -14.0, 4.375, 0.0)
(-53.6875, -14.0, 53.6875, 0.0)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.437500000000001, 0.0, 4.437499999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.0, -4.375000000000001, 0.0, 4.374999999999999)
(-14.000000000000002, -35.75, 1.7763568394002505e-15, 35.75)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-8.875, -7.0, 8.875, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-8.8125, -7.0, 8.8125, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.375, -7.0, 4.375, 7.0)
(-4.4375, -7.0, 4.4375, 7.0)
(-172.0, -4.0, 172.0, 14.0)
(0.0, 0.0, 1.0, 1.0)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:              Best Accuracy ‚ñÅ‚ñá‚ñà
wandb:                 Train Loss ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÑ
wandb:        Validation Accuracy ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÑ
wandb:            Validation Loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ
wandb:        Validation accuracy ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÑ
wandb:        Validation f1_macro ‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà
wandb:     Validation f1_weighted ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà
wandb: Validation precision_macro ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà
wandb:    Validation recall_macro ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb:              Best Accuracy 0.68317
wandb:                 Train Loss 0.67246
wandb:        Validation Accuracy 0.62376
wandb:            Validation Loss 0.8893
wandb:        Validation accuracy 0.62376
wandb:        Validation f1_macro 0.62418
wandb:     Validation f1_weighted 0.63627
wandb: Validation precision_macro 0.61919
wandb:    Validation recall_macro 0.65368
wandb: 
wandb: üöÄ View run lunar-sweep-2 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nofyn6e2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013254-nofyn6e2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: gy023a8q with config:
wandb: 	Fdropout_rate: 0.2968765503900508
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.12198820743489724
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.10492509056897235
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013740-gy023a8q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/gy023a8q
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.

(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
(0.0, -8.5, 8.75, 5.5)
(0.0, -8.5, 17.625, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
(0.0, -8.5, 17.5, 5.5)
16
16
Training started...
Epoch 0 [Train]:   0%|          | 0/2 [00:00<?, ?it/s]                                                      wandb:                                                                                
wandb: üöÄ View run firm-sweep-3 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/gy023a8q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013740-gy023a8q/logs
wandb: ERROR Run gy023a8q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 99, in train
wandb: ERROR     output = model(*features)
wandb: ERROR              ^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/src/models/naim_text.py", line 478, in forward
wandb: ERROR     text_features = self.vitbi(input_ids, attention_mask)
wandb: ERROR                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/src/models/naim_text.py", line 145, in forward
wandb: ERROR     outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward
wandb: ERROR     encoder_outputs = self.encoder(
wandb: ERROR                       ^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward
wandb: ERROR     layer_outputs = layer_module(
wandb: ERROR                     ^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
wandb: ERROR     self_attention_outputs = self.attention(
wandb: ERROR                              ^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 524, in forward
wandb: ERROR     attention_output = self.output(self_outputs[0], hidden_states)
wandb: ERROR                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 466, in forward
wandb: ERROR     hidden_states = self.dense(hidden_states)
wandb: ERROR                     ^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
wandb: ERROR     return F.linear(input, self.weight, self.bias)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 94.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 46.94 GiB memory in use. Of the allocated memory 46.59 GiB is allocated by PyTorch, and 144.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: k7778824 with config:
wandb: 	Fdropout_rate: 0.391143283525024
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.22555872822552953
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 8
wandb: 	learning_rate: 0.05292460401634566
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013752-k7778824
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/k7778824
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run azure-sweep-4 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/k7778824
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013752-k7778824/logs
wandb: ERROR Run k7778824 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 4 more times]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 8.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 52.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 3k1ech94 with config:
wandb: 	Fdropout_rate: 0.20605269592425768
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.10443091192906331
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.02198971892855308
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013803-3k1ech94
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/3k1ech94
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run balmy-sweep-5 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/3k1ech94
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013803-3k1ech94/logs
wandb: ERROR Run 3k1ech94 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 8.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 51.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1rbswmmq with config:
wandb: 	Fdropout_rate: 0.33605470290014805
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.3755497688573809
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.03211388140407011
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013814-1rbswmmq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/1rbswmmq
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run woven-sweep-6 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/1rbswmmq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013814-1rbswmmq/logs
wandb: ERROR Run 1rbswmmq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 6.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 51.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sao33r32 with config:
wandb: 	Fdropout_rate: 0.27258075721308256
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.10129809005585866
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.054163522929828394
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013825-sao33r32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/sao33r32
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fresh-sweep-7 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/sao33r32
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013825-sao33r32/logs
wandb: ERROR Run sao33r32 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 6.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.02 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 51.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: awilio5i with config:
wandb: 	Fdropout_rate: 0.3532226086378448
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.15790314976478678
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.12457587543220729
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013835-awilio5i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/awilio5i
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run floral-sweep-8 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/awilio5i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013835-awilio5i/logs
wandb: ERROR Run awilio5i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 4.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 52.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: pq6nspid with config:
wandb: 	Fdropout_rate: 0.4720793035741756
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.17697707703768634
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.11548851997498516
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013846-pq6nspid
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/pq6nspid
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run sleek-sweep-9 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/pq6nspid
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013846-pq6nspid/logs
wandb: ERROR Run pq6nspid errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 4.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 52.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5igdnjd3 with config:
wandb: 	Fdropout_rate: 0.224239681689548
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.36651078513674695
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.12376588437453909
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013857-5igdnjd3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5igdnjd3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run copper-sweep-10 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5igdnjd3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013857-5igdnjd3/logs
wandb: ERROR Run 5igdnjd3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 4.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 52.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9c9fmwtf with config:
wandb: 	Fdropout_rate: 0.2214004334076647
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2031358014071863
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.07966181140257975
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013908-9c9fmwtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/9c9fmwtf
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run floral-sweep-11 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/9c9fmwtf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013908-9c9fmwtf/logs
wandb: ERROR Run 9c9fmwtf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 52.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f2suuaa0 with config:
wandb: 	Fdropout_rate: 0.3874218503865564
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3756530717338096
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.07715287656888714
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013919-f2suuaa0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/f2suuaa0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fanciful-sweep-12 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/f2suuaa0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013919-f2suuaa0/logs
wandb: ERROR Run f2suuaa0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 2 more times]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9wm9qhuh with config:
wandb: 	Fdropout_rate: 0.14583172330116773
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.31482970227458085
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.07003660251576696
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013929-9wm9qhuh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/9wm9qhuh
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run generous-sweep-13 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/9wm9qhuh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013929-9wm9qhuh/logs
wandb: ERROR Run 9wm9qhuh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 2 more times]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: o2h2k27x with config:
wandb: 	Fdropout_rate: 0.29653989448729084
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.27678298906007676
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 32
wandb: 	learning_rate: 0.060491266225494354
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013940-o2h2k27x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/o2h2k27x
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fancy-sweep-14 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/o2h2k27x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013940-o2h2k27x/logs
wandb: ERROR Run o2h2k27x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 991, in _apply
wandb: ERROR     self._buffers[key] = fn(buf)
wandb: ERROR                          ^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: d5fodm3u with config:
wandb: 	Fdropout_rate: 0.3652751679801046
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.38374412448448303
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.039163877167579055
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_013951-d5fodm3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/d5fodm3u
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run likely-sweep-15 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/d5fodm3u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_013951-d5fodm3u/logs
wandb: ERROR Run d5fodm3u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 65gf8gfd with config:
wandb: 	Fdropout_rate: 0.23499741702825108
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.18238154559917708
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06076481328207149
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014002-65gf8gfd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/65gf8gfd
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lucky-sweep-16 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/65gf8gfd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014002-65gf8gfd/logs
wandb: ERROR Run 65gf8gfd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: kzmrlrb5 with config:
wandb: 	Fdropout_rate: 0.23101339965447504
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.14527352333515586
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.06327410572663704
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014013-kzmrlrb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/kzmrlrb5
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run jumping-sweep-17 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/kzmrlrb5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014013-kzmrlrb5/logs
wandb: ERROR Run kzmrlrb5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sbim3slq with config:
wandb: 	Fdropout_rate: 0.48978814704202256
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.36290578625330705
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 4
wandb: 	d_token: 16
wandb: 	learning_rate: 0.04559843159488883
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014024-sbim3slq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/sbim3slq
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run hardy-sweep-18 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/sbim3slq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014024-sbim3slq/logs
wandb: ERROR Run sbim3slq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ldthkbzo with config:
wandb: 	Fdropout_rate: 0.2723675543781979
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.24492159559185225
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.042233410513775695
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014035-ldthkbzo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ldthkbzo
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run logical-sweep-19 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ldthkbzo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014035-ldthkbzo/logs
wandb: ERROR Run ldthkbzo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: mrogprc8 with config:
wandb: 	Fdropout_rate: 0.3061164530065879
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.1823407127314245
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 8
wandb: 	learning_rate: 0.121053529722464
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014101-mrogprc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/mrogprc8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run different-sweep-20 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/mrogprc8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014101-mrogprc8/logs
wandb: ERROR Run mrogprc8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vi8b03jv with config:
wandb: 	Fdropout_rate: 0.11969280440775364
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.13812165199695994
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.105550307788902
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014112-vi8b03jv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/vi8b03jv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run playful-sweep-21 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/vi8b03jv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014112-vi8b03jv/logs
wandb: ERROR Run vi8b03jv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cep6so23 with config:
wandb: 	Fdropout_rate: 0.2183799948623573
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.3853952356678778
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.019877305344790673
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014123-cep6so23
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cep6so23
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run iconic-sweep-22 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cep6so23
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014123-cep6so23/logs
wandb: ERROR Run cep6so23 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gg76l4kv with config:
wandb: 	Fdropout_rate: 0.16104978811823012
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2975422744363164
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.045445370928773675
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014134-gg76l4kv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/gg76l4kv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run solar-sweep-23 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/gg76l4kv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014134-gg76l4kv/logs
wandb: ERROR Run gg76l4kv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ytsxitub with config:
wandb: 	Fdropout_rate: 0.3244728436732237
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.36555392122353214
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.09609678609047911
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014211-ytsxitub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ytsxitub
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run decent-sweep-24 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ytsxitub
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014211-ytsxitub/logs
wandb: ERROR Run ytsxitub errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2nz9fmz9 with config:
wandb: 	Fdropout_rate: 0.49245939947817874
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.28499755401551774
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.029125061977873467
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014218-2nz9fmz9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/2nz9fmz9
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run colorful-sweep-25 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/2nz9fmz9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014218-2nz9fmz9/logs
wandb: ERROR Run 2nz9fmz9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ak4xww5n with config:
wandb: 	Fdropout_rate: 0.3622523725928263
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.22732055730735745
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.058800248244102654
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014229-ak4xww5n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ak4xww5n
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run peachy-sweep-26 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ak4xww5n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014229-ak4xww5n/logs
wandb: ERROR Run ak4xww5n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wbzc4z67 with config:
wandb: 	Fdropout_rate: 0.4120629047015428
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.3955386457254497
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 8
wandb: 	learning_rate: 0.02418438483231786
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014240-wbzc4z67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/wbzc4z67
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run wobbly-sweep-27 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/wbzc4z67
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014240-wbzc4z67/logs
wandb: ERROR Run wbzc4z67 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7uorxhan with config:
wandb: 	Fdropout_rate: 0.37183970792234056
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.2672808140355408
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 8
wandb: 	learning_rate: 0.02475149123133958
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014251-7uorxhan
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7uorxhan
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run visionary-sweep-28 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7uorxhan
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014251-7uorxhan/logs
wandb: ERROR Run 7uorxhan errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ktt5rn2e with config:
wandb: 	Fdropout_rate: 0.33952311967661153
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.19697854869999612
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 8
wandb: 	learning_rate: 0.10068289633323912
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014301-ktt5rn2e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ktt5rn2e
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run icy-sweep-29 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ktt5rn2e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014301-ktt5rn2e/logs
wandb: ERROR Run ktt5rn2e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: d1azu118 with config:
wandb: 	Fdropout_rate: 0.11333682203932814
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.19026345258975288
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 8
wandb: 	d_token: 8
wandb: 	learning_rate: 0.028619718491524417
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014312-d1azu118
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/d1azu118
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fanciful-sweep-30 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/d1azu118
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014312-d1azu118/logs
wandb: ERROR Run d1azu118 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: q5593tvt with config:
wandb: 	Fdropout_rate: 0.4820560178301124
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.1350890459423766
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 8
wandb: 	learning_rate: 0.12642241266308038
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014323-q5593tvt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/q5593tvt
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run stellar-sweep-31 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/q5593tvt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014323-q5593tvt/logs
wandb: ERROR Run q5593tvt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7yrua6f0 with config:
wandb: 	Fdropout_rate: 0.46611151488750535
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.2845857518350216
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.03719470170271295
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014334-7yrua6f0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7yrua6f0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dauntless-sweep-32 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7yrua6f0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014334-7yrua6f0/logs
wandb: ERROR Run 7yrua6f0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y38zxal6 with config:
wandb: 	Fdropout_rate: 0.15717415134113613
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.14255589692781198
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0519645010775243
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014345-y38zxal6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/y38zxal6
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run olive-sweep-33 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/y38zxal6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014345-y38zxal6/logs
wandb: ERROR Run y38zxal6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: tw48pavr with config:
wandb: 	Fdropout_rate: 0.3004161601274507
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.20057183763540315
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.024208923945465283
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014356-tw48pavr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/tw48pavr
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run elated-sweep-34 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/tw48pavr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014356-tw48pavr/logs
wandb: ERROR Run tw48pavr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 95a373sm with config:
wandb: 	Fdropout_rate: 0.12907046543871586
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.10398983214245644
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.12032088823749867
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014407-95a373sm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/95a373sm
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run devout-sweep-35 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/95a373sm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014407-95a373sm/logs
wandb: ERROR Run 95a373sm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nb8okc7c with config:
wandb: 	Fdropout_rate: 0.34051223807356734
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.10418161345364484
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.08495240024553811
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014418-nb8okc7c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nb8okc7c
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dry-sweep-36 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nb8okc7c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014418-nb8okc7c/logs
wandb: ERROR Run nb8okc7c errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 41aelhge with config:
wandb: 	Fdropout_rate: 0.10468134491282231
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.24499266626103391
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.06338152516631788
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014429-41aelhge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/41aelhge
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run revived-sweep-37 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/41aelhge
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014429-41aelhge/logs
wandb: ERROR Run 41aelhge errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: zj5fhd6u with config:
wandb: 	Fdropout_rate: 0.23288872315340217
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.27047927625644314
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.051149940035372016
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014439-zj5fhd6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/zj5fhd6u
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run eager-sweep-38 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/zj5fhd6u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014439-zj5fhd6u/logs
wandb: ERROR Run zj5fhd6u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: da3o2zve with config:
wandb: 	Fdropout_rate: 0.24152480282559569
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.3814985341099537
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06465086085266285
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014459-da3o2zve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/da3o2zve
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run tough-sweep-39 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/da3o2zve
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014459-da3o2zve/logs
wandb: ERROR Run da3o2zve errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ztxvpqx8 with config:
wandb: 	Fdropout_rate: 0.4781731087727109
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.25156778139239516
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.02099758592031952
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014536-ztxvpqx8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ztxvpqx8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run ruby-sweep-40 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ztxvpqx8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014536-ztxvpqx8/logs
wandb: ERROR Run ztxvpqx8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: oodym6px with config:
wandb: 	Fdropout_rate: 0.2123364642202535
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.2310810589009964
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.09678439432078
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014545-oodym6px
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/oodym6px
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run amber-sweep-41 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/oodym6px
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014545-oodym6px/logs
wandb: ERROR Run oodym6px errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: fwerdu0n with config:
wandb: 	Fdropout_rate: 0.41296344904305216
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.1954931630809033
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.05295466677339187
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014555-fwerdu0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/fwerdu0n
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run tough-sweep-42 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/fwerdu0n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014555-fwerdu0n/logs
wandb: ERROR Run fwerdu0n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: yutuadbe with config:
wandb: 	Fdropout_rate: 0.47558136685480445
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.24825306921401535
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0945742583376866
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014606-yutuadbe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/yutuadbe
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run balmy-sweep-43 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/yutuadbe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014606-yutuadbe/logs
wandb: ERROR Run yutuadbe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ddahb7ph with config:
wandb: 	Fdropout_rate: 0.46187689446679614
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.1501272969052759
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 8
wandb: 	d_token: 32
wandb: 	learning_rate: 0.01905653169660273
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014617-ddahb7ph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ddahb7ph
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run firm-sweep-44 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ddahb7ph
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014617-ddahb7ph/logs
wandb: ERROR Run ddahb7ph errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cpkdv9fc with config:
wandb: 	Fdropout_rate: 0.3299849029537075
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2892328759277135
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.09920699439845174
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014628-cpkdv9fc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cpkdv9fc
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run warm-sweep-45 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cpkdv9fc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014628-cpkdv9fc/logs
wandb: ERROR Run cpkdv9fc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 76by0nsh with config:
wandb: 	Fdropout_rate: 0.4561617868995066
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.35449595321618255
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.020144137976410563
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014639-76by0nsh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/76by0nsh
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run bumbling-sweep-46 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/76by0nsh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014639-76by0nsh/logs
wandb: ERROR Run 76by0nsh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kw6bem6y with config:
wandb: 	Fdropout_rate: 0.25351246977825526
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.35762039335485596
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 8
wandb: 	learning_rate: 0.07389972186196228
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014719-kw6bem6y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/kw6bem6y
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run upbeat-sweep-47 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/kw6bem6y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014719-kw6bem6y/logs
wandb: ERROR Run kw6bem6y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: fhrkj9su with config:
wandb: 	Fdropout_rate: 0.4197710043933812
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.1139507241126772
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.02829052056468912
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014730-fhrkj9su
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/fhrkj9su
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fresh-sweep-48 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/fhrkj9su
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014730-fhrkj9su/logs
wandb: ERROR Run fhrkj9su errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: w5vicxr9 with config:
wandb: 	Fdropout_rate: 0.3114638112589494
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.3745736589763409
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06309864392969489
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014741-w5vicxr9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/w5vicxr9
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run swift-sweep-49 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/w5vicxr9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014741-w5vicxr9/logs
wandb: ERROR Run w5vicxr9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f7d3qx2t with config:
wandb: 	Fdropout_rate: 0.3354481566948695
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.32503198832901314
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 8
wandb: 	d_token: 16
wandb: 	learning_rate: 0.12503704335340574
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014752-f7d3qx2t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/f7d3qx2t
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run glowing-sweep-50 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/f7d3qx2t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014752-f7d3qx2t/logs
wandb: ERROR Run f7d3qx2t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: e83plfrq with config:
wandb: 	Fdropout_rate: 0.2715546346157396
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.18631884925102712
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.12902754954913215
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014803-e83plfrq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/e83plfrq
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run stellar-sweep-51 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/e83plfrq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014803-e83plfrq/logs
wandb: ERROR Run e83plfrq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7e1gkfx9 with config:
wandb: 	Fdropout_rate: 0.4019726817910809
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.27959254743159867
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.01854959628538228
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014813-7e1gkfx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7e1gkfx9
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run rich-sweep-52 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7e1gkfx9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014813-7e1gkfx9/logs
wandb: ERROR Run 7e1gkfx9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: zmjnxijz with config:
wandb: 	Fdropout_rate: 0.1786314269184176
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.24097822681158337
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.02275198216897628
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014830-zmjnxijz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/zmjnxijz
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run grateful-sweep-53 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/zmjnxijz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014830-zmjnxijz/logs
wandb: ERROR Run zmjnxijz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 89stlnnt with config:
wandb: 	Fdropout_rate: 0.27689057333160677
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.39175130507176104
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 16
wandb: 	learning_rate: 0.022089785227547568
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014839-89stlnnt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/89stlnnt
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run good-sweep-54 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/89stlnnt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014839-89stlnnt/logs
wandb: ERROR Run 89stlnnt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: utqpl4gg with config:
wandb: 	Fdropout_rate: 0.14106123823539596
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.1648420771225821
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 128
wandb: 	d_token: 8
wandb: 	learning_rate: 0.1298777998645626
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014849-utqpl4gg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/utqpl4gg
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dark-sweep-55 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/utqpl4gg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014849-utqpl4gg/logs
wandb: ERROR Run utqpl4gg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: q52jaamk with config:
wandb: 	Fdropout_rate: 0.3954841886448698
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.1051856143010983
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.08627598166501659
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014900-q52jaamk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/q52jaamk
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fresh-sweep-56 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/q52jaamk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014900-q52jaamk/logs
wandb: ERROR Run q52jaamk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: esqful89 with config:
wandb: 	Fdropout_rate: 0.42329649289340265
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.2894332638348389
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 64
wandb: 	learning_rate: 0.045260103706688926
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014911-esqful89
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/esqful89
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run daily-sweep-57 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/esqful89
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014911-esqful89/logs
wandb: ERROR Run esqful89 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9jgvr238 with config:
wandb: 	Fdropout_rate: 0.12562165275546114
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2221142527161043
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.02718235692195983
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014922-9jgvr238
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/9jgvr238
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run vague-sweep-58 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/9jgvr238
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014922-9jgvr238/logs
wandb: ERROR Run 9jgvr238 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: twfyumem with config:
wandb: 	Fdropout_rate: 0.15901914254038008
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.27671589800783225
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.04244450691386317
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014933-twfyumem
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/twfyumem
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run generous-sweep-59 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/twfyumem
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014933-twfyumem/logs
wandb: ERROR Run twfyumem errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s80x4tcv with config:
wandb: 	Fdropout_rate: 0.2710601825544352
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.14175431119326024
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.0905136987181002
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014944-s80x4tcv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/s80x4tcv
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zany-sweep-60 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/s80x4tcv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014944-s80x4tcv/logs
wandb: ERROR Run s80x4tcv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cr4ntx18 with config:
wandb: 	Fdropout_rate: 0.3338616681530789
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.2287322675931756
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.06543432700463005
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_014955-cr4ntx18
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/cr4ntx18
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run blooming-sweep-61 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/cr4ntx18
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_014955-cr4ntx18/logs
wandb: ERROR Run cr4ntx18 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: hgzwgow3 with config:
wandb: 	Fdropout_rate: 0.3792802483148313
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.29015097672146145
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 128
wandb: 	d_token: 8
wandb: 	learning_rate: 0.07949589659902892
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015006-hgzwgow3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/hgzwgow3
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run different-sweep-62 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/hgzwgow3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015006-hgzwgow3/logs
wandb: ERROR Run hgzwgow3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jo18ekqs with config:
wandb: 	Fdropout_rate: 0.3240435867728802
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.32993200120529736
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 64
wandb: 	d_token: 8
wandb: 	learning_rate: 0.026153854766143795
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015022-jo18ekqs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/jo18ekqs
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run avid-sweep-63 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/jo18ekqs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015022-jo18ekqs/logs
wandb: ERROR Run jo18ekqs errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sj2bpyvo with config:
wandb: 	Fdropout_rate: 0.10088186079631534
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.2593117862242059
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.11231809060262936
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015033-sj2bpyvo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/sj2bpyvo
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run usual-sweep-64 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/sj2bpyvo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015033-sj2bpyvo/logs
wandb: ERROR Run sj2bpyvo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2i21guho with config:
wandb: 	Fdropout_rate: 0.4451235719792752
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.1356105021333287
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 16
wandb: 	learning_rate: 0.046268311849337586
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015044-2i21guho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/2i21guho
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run generous-sweep-65 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/2i21guho
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015044-2i21guho/logs
wandb: ERROR Run 2i21guho errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: j7vn3cx6 with config:
wandb: 	Fdropout_rate: 0.3179036939010361
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.21363413023830025
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 16
wandb: 	learning_rate: 0.036870952597217926
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015131-j7vn3cx6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/j7vn3cx6
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run gentle-sweep-66 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/j7vn3cx6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015131-j7vn3cx6/logs
wandb: ERROR Run j7vn3cx6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: kh8beqob with config:
wandb: 	Fdropout_rate: 0.3418953736514101
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.17109092550588512
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 16
wandb: 	learning_rate: 0.029654953227783793
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015141-kh8beqob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/kh8beqob
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run pious-sweep-67 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/kh8beqob
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015141-kh8beqob/logs
wandb: ERROR Run kh8beqob errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 045046ot with config:
wandb: 	Fdropout_rate: 0.11604062150812117
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.1485879363762293
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.02086547152368921
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015152-045046ot
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/045046ot
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run quiet-sweep-68 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/045046ot
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015152-045046ot/logs
wandb: ERROR Run 045046ot errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1uvhcwod with config:
wandb: 	Fdropout_rate: 0.3049205433120036
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.18820124596504836
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.022070215435132625
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015203-1uvhcwod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/1uvhcwod
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run dark-sweep-69 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/1uvhcwod
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015203-1uvhcwod/logs
wandb: ERROR Run 1uvhcwod errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: hptkcn2d with config:
wandb: 	Fdropout_rate: 0.1656661382228354
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.20207911392645572
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 16
wandb: 	learning_rate: 0.021558801013487675
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015213-hptkcn2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/hptkcn2d
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run hearty-sweep-70 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/hptkcn2d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015213-hptkcn2d/logs
wandb: ERROR Run hptkcn2d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: heibeeey with config:
wandb: 	Fdropout_rate: 0.3301114049320474
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.10778046003937183
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.09599179843904738
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015224-heibeeey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/heibeeey
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run tough-sweep-71 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/heibeeey
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015224-heibeeey/logs
wandb: ERROR Run heibeeey errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qewbcyjx with config:
wandb: 	Fdropout_rate: 0.22568714726830336
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.18745942719955988
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 64
wandb: 	learning_rate: 0.021497059483284925
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015235-qewbcyjx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-72
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/qewbcyjx
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run unique-sweep-72 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/qewbcyjx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015235-qewbcyjx/logs
wandb: ERROR Run qewbcyjx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ud47pdc6 with config:
wandb: 	Fdropout_rate: 0.44970954789481976
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.11000344776954088
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.04102182982181089
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015312-ud47pdc6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ud47pdc6
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run kind-sweep-73 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ud47pdc6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015312-ud47pdc6/logs
wandb: ERROR Run ud47pdc6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bo9vybkn with config:
wandb: 	Fdropout_rate: 0.433799410781177
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.1367467934715902
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.02982201745956055
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015323-bo9vybkn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/bo9vybkn
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run olive-sweep-74 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/bo9vybkn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015323-bo9vybkn/logs
wandb: ERROR Run bo9vybkn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: b3zh1eeu with config:
wandb: 	Fdropout_rate: 0.11217736349578215
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.3010439680479726
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 16
wandb: 	d_token: 64
wandb: 	learning_rate: 0.08724487000381366
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015334-b3zh1eeu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/b3zh1eeu
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fallen-sweep-75 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/b3zh1eeu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015334-b3zh1eeu/logs
wandb: ERROR Run b3zh1eeu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5ohg67zl with config:
wandb: 	Fdropout_rate: 0.1759603819922142
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.13196659298550267
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 128
wandb: 	d_token: 32
wandb: 	learning_rate: 0.08573757209613096
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015345-5ohg67zl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-76
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/5ohg67zl
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run glad-sweep-76 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/5ohg67zl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015345-5ohg67zl/logs
wandb: ERROR Run 5ohg67zl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s5s2hb7r with config:
wandb: 	Fdropout_rate: 0.1320359437702131
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.19384280292979209
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 4
wandb: 	d_token: 16
wandb: 	learning_rate: 0.05180532919780022
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015356-s5s2hb7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/s5s2hb7r
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run solar-sweep-77 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/s5s2hb7r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015356-s5s2hb7r/logs
wandb: ERROR Run s5s2hb7r errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ofdrieoa with config:
wandb: 	Fdropout_rate: 0.11513691602019574
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.37920982571951223
wandb: 	Mnum_layers: 2
wandb: 	batch_size: 4
wandb: 	d_token: 32
wandb: 	learning_rate: 0.028898690640297937
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015407-ofdrieoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ofdrieoa
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run sweet-sweep-78 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ofdrieoa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015407-ofdrieoa/logs
wandb: ERROR Run ofdrieoa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rcwfuea8 with config:
wandb: 	Fdropout_rate: 0.27675801812235623
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.15913237926970636
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03343133423379956
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015418-rcwfuea8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/rcwfuea8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run polished-sweep-79 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/rcwfuea8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015418-rcwfuea8/logs
wandb: ERROR Run rcwfuea8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: xres6cdx with config:
wandb: 	Fdropout_rate: 0.11616022278740834
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.13406954961726772
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 256
wandb: 	d_token: 16
wandb: 	learning_rate: 0.04153746563912475
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015455-xres6cdx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/xres6cdx
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run driven-sweep-80 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/xres6cdx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015455-xres6cdx/logs
wandb: ERROR Run xres6cdx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8pl6tmyt with config:
wandb: 	Fdropout_rate: 0.4722281987150919
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.16195523090343356
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 64
wandb: 	learning_rate: 0.01985633878885358
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015506-8pl6tmyt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/8pl6tmyt
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run youthful-sweep-81 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/8pl6tmyt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015506-8pl6tmyt/logs
wandb: ERROR Run 8pl6tmyt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: q7qghofw with config:
wandb: 	Fdropout_rate: 0.4878346753617666
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.3234807086154077
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 16
wandb: 	learning_rate: 0.12225264846375078
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015522-q7qghofw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-82
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/q7qghofw
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lunar-sweep-82 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/q7qghofw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015522-q7qghofw/logs
wandb: ERROR Run q7qghofw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: la1jhyh0 with config:
wandb: 	Fdropout_rate: 0.35309888094971387
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.3983420989425873
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 16
wandb: 	d_token: 32
wandb: 	learning_rate: 0.04924759587549848
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015533-la1jhyh0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/la1jhyh0
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fine-sweep-83 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/la1jhyh0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015533-la1jhyh0/logs
wandb: ERROR Run la1jhyh0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ay7bs6op with config:
wandb: 	Fdropout_rate: 0.3424991031278338
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.3839480965660496
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.03136301305619237
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015544-ay7bs6op
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ay7bs6op
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run clean-sweep-84 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ay7bs6op
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015544-ay7bs6op/logs
wandb: ERROR Run ay7bs6op errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: kho839bb with config:
wandb: 	Fdropout_rate: 0.19194551586578976
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.31504717036940666
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 16
wandb: 	d_token: 16
wandb: 	learning_rate: 0.07994203427215701
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015555-kho839bb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/kho839bb
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run charmed-sweep-85 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/kho839bb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015555-kho839bb/logs
wandb: ERROR Run kho839bb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: eh6p2oar with config:
wandb: 	Fdropout_rate: 0.38072142013832766
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.22070889432249088
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.04805663756632177
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015611-eh6p2oar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/eh6p2oar
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run cerulean-sweep-86 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/eh6p2oar
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015611-eh6p2oar/logs
wandb: ERROR Run eh6p2oar errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n4ew5jxg with config:
wandb: 	Fdropout_rate: 0.3832582996889784
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.2822710834999681
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.13033667591128054
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015622-n4ew5jxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/n4ew5jxg
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run exalted-sweep-87 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/n4ew5jxg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015622-n4ew5jxg/logs
wandb: ERROR Run n4ew5jxg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6v0buo22 with config:
wandb: 	Fdropout_rate: 0.244159429097024
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.19449710100894765
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 256
wandb: 	d_token: 8
wandb: 	learning_rate: 0.05691525303635096
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015643-6v0buo22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-88
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/6v0buo22
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run trim-sweep-88 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/6v0buo22
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015643-6v0buo22/logs
wandb: ERROR Run 6v0buo22 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: pac33lb2 with config:
wandb: 	Fdropout_rate: 0.2694896537517064
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 3
wandb: 	Mdropout_rate: 0.12251790032111128
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 64
wandb: 	learning_rate: 0.023703373941318433
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015654-pac33lb2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/pac33lb2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run fragrant-sweep-89 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/pac33lb2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015654-pac33lb2/logs
wandb: ERROR Run pac33lb2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nquo412y with config:
wandb: 	Fdropout_rate: 0.26621769977618914
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.3161718287877184
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.02450313869116646
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015705-nquo412y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nquo412y
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run trim-sweep-90 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nquo412y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015705-nquo412y/logs
wandb: ERROR Run nquo412y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ps36m23o with config:
wandb: 	Fdropout_rate: 0.46907611868271026
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.29847692424562144
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.1245710256519892
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015716-ps36m23o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/ps36m23o
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run rare-sweep-91 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/ps36m23o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015716-ps36m23o/logs
wandb: ERROR Run ps36m23o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xiedf794 with config:
wandb: 	Fdropout_rate: 0.3946673516232164
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 1
wandb: 	Mdropout_rate: 0.25783690851374474
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 16
wandb: 	learning_rate: 0.03465628101600093
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015727-xiedf794
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-92
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/xiedf794
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run playful-sweep-92 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/xiedf794
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015727-xiedf794/logs
wandb: ERROR Run xiedf794 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rct6rr7k with config:
wandb: 	Fdropout_rate: 0.4153358648386759
wandb: 	Fnum_heads: 8
wandb: 	Fnum_layers: 2
wandb: 	Mdropout_rate: 0.2395879824450936
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 8
wandb: 	d_token: 64
wandb: 	learning_rate: 0.05175898364517302
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015738-rct6rr7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/rct6rr7k
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run icy-sweep-93 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/rct6rr7k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015738-rct6rr7k/logs
wandb: ERROR Run rct6rr7k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: woyg06c2 with config:
wandb: 	Fdropout_rate: 0.1185166886569926
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 7
wandb: 	Mdropout_rate: 0.1012122517436106
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 32
wandb: 	d_token: 32
wandb: 	learning_rate: 0.050953749554268075
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015754-woyg06c2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/woyg06c2
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run olive-sweep-94 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/woyg06c2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015754-woyg06c2/logs
wandb: ERROR Run woyg06c2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kiu5ktmm with config:
wandb: 	Fdropout_rate: 0.2953371030959008
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 6
wandb: 	Mdropout_rate: 0.165874289776888
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 256
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06974152235151074
wandb: 	pretrained_model_name: emilyalsentzer/Bio_ClinicalBERT
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015831-kiu5ktmm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/kiu5ktmm
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zany-sweep-95 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/kiu5ktmm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015831-kiu5ktmm/logs
wandb: ERROR Run kiu5ktmm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6o2y6env with config:
wandb: 	Fdropout_rate: 0.3612482000401157
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.2456461350173954
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 16
wandb: 	d_token: 8
wandb: 	learning_rate: 0.1034056342450033
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015858-6o2y6env
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/6o2y6env
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run polished-sweep-96 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/6o2y6env
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015858-6o2y6env/logs
wandb: ERROR Run 6o2y6env errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nadsewoa with config:
wandb: 	Fdropout_rate: 0.47723992267385207
wandb: 	Fnum_heads: 2
wandb: 	Fnum_layers: 5
wandb: 	Mdropout_rate: 0.18626187934622357
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.1272299909865934
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015909-nadsewoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/nadsewoa
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run lilac-sweep-97 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/nadsewoa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015909-nadsewoa/logs
wandb: ERROR Run nadsewoa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: fnt7r6kr with config:
wandb: 	Fdropout_rate: 0.3667385138811789
wandb: 	Fnum_heads: 1
wandb: 	Fnum_layers: 4
wandb: 	Mdropout_rate: 0.250205971597321
wandb: 	Mnum_layers: 3
wandb: 	batch_size: 32
wandb: 	d_token: 8
wandb: 	learning_rate: 0.08078975817723771
wandb: 	pretrained_model_name: dmis-lab/biobert-base-cased-v1.1
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015920-fnt7r6kr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/fnt7r6kr
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run rich-sweep-98 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/fnt7r6kr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015920-fnt7r6kr/logs
wandb: ERROR Run fnt7r6kr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7aadpw7f with config:
wandb: 	Fdropout_rate: 0.1209670666793913
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.15958808570686045
wandb: 	Mnum_layers: 4
wandb: 	batch_size: 64
wandb: 	d_token: 32
wandb: 	learning_rate: 0.07490997889816291
wandb: 	pretrained_model_name: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015931-7aadpw7f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-99
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/7aadpw7f
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run zesty-sweep-99 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/7aadpw7f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015931-7aadpw7f/logs
wandb: ERROR Run 7aadpw7f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gfgovpw8 with config:
wandb: 	Fdropout_rate: 0.4585557481652706
wandb: 	Fnum_heads: 4
wandb: 	Fnum_layers: 8
wandb: 	Mdropout_rate: 0.14998213932457205
wandb: 	Mnum_layers: 1
wandb: 	batch_size: 64
wandb: 	d_token: 64
wandb: 	learning_rate: 0.06854877658262364
wandb: 	pretrained_model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/outputs/2025-06-03/01-27-18/wandb/run-20250603_015942-gfgovpw8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vinakhiem120/naim_tbi
wandb: üßπ View sweep at https://wandb.ai/vinakhiem120/naim_tbi/sweeps/xefe9qf1
wandb: üöÄ View run at https://wandb.ai/vinakhiem120/naim_tbi/runs/gfgovpw8
wandb: WARNING Ignoring project 'naim_tbi' when running a sweep.
wandb:                                                                                
wandb: üöÄ View run glowing-sweep-100 at: https://wandb.ai/vinakhiem120/naim_tbi/runs/gfgovpw8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vinakhiem120/naim_tbi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250603_015942-gfgovpw8/logs
wandb: ERROR Run gfgovpw8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 298, in sweep_train
wandb: ERROR     train(config)
wandb: ERROR   File "/home/khanhhiep/Code/Khanh/Khiem/MyBachelorThesis/train.py", line 69, in train
wandb: ERROR     model = model.to(config.device)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1343, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 903, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/home/khanhhiep/anaconda3/envs/khiem/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1329, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 2.19 MiB is free. Process 151104 has 226.00 MiB memory in use. Including non-PyTorch memory, this process has 47.03 GiB memory in use. Of the allocated memory 46.78 GiB is allocated by PyTorch, and 51.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
Training started...
